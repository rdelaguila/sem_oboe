{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de Explicación de Tópicos\n",
    "Este notebook implementa un pipeline completo para:\n",
    "1. Procesamiento de tripletas de conocimiento\n",
    "2. Expansión de vocabulario con similitud semántica\n",
    "3. Clustering jerárquico de términos\n",
    "4. Generación de explicaciones con LLMs\n",
    "5. Evaluación automática de las explicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración y Preparación\n",
    "Configuración inicial y carga de recursos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import spacy\n",
    "import torch\n",
    "from utils.types import *\n",
    "from utils.triplet_manager_lib import Tripleta\n",
    "from operator import itemgetter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, T5Tokenizer, T5ForConditionalGeneration\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Configuración\n",
    "TRIPLES_PATH = 'data/triples_ft/processed/dataset_final_triplet_bbc_pykeen'\n",
    "TOPIC_ID = 3\n",
    "VISITAR_OBJETO = True\n",
    "TERMINOS_A_INCLUIR = set(['dvd','google','electronic','tv','sony','screen','nintendo',\n",
    "                         'player','mobile','phone','software','video','network','apple',\n",
    "                         'program','linux'])\n",
    "DBPEDIA_PATH = 'data/corpus_ft/bbc/diccionario_topic_entidades_dbpedia'\n",
    "NER_PATH = 'data/corpus_ft/bbc/diccionario_ner'\n",
    "N_SINONIMOS = 1\n",
    "OUTPUT_DIR = 'output'\n",
    "SPACY_MODEL = 'en_core_web_lg'\n",
    "GEN_MODEL = 'Qwen/Qwen2-7B-Instruct'\n",
    "EVAL_MODEL = 'google/mt5-small'\n",
    "TOP_K_TERMS = 10\n",
    "\n",
    "# Preparación\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "df_tr = joblib.load(TRIPLES_PATH)\n",
    "topics_dbp = joblib.load(DBPEDIA_PATH)\n",
    "dictdbp = topics_dbp.get(TOPIC_ID, {})\n",
    "dictner = joblib.load(NER_PATH)\n",
    "nlp = spacy.load(SPACY_MODEL)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Funciones auxiliares\n",
    "def remove_numbers(text): return re.sub(r\"\\d+\", \"\", text)\n",
    "def remove_dbpedia_categories(s): return s.split('/')[-1]\n",
    "def return_url_element(s):\n",
    "    for sep in ['#','/']:\n",
    "        if sep in s:\n",
    "            s = s.split(sep)[-1]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtrado de Tripletas y Extracción de Términos\n",
    "Procesamiento de tripletas para extraer términos relevantes usando información de DBpedia y NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listado_tripletas = []\n",
    "palabrasdbpedia = set(k.lower() for k in dictdbp.keys())\n",
    "anterior = None\n",
    "\n",
    "for i, row in df_tr.iterrows():\n",
    "    tripleta = Tripleta({'subject': str(row['subject']),\n",
    "                     'relation': row['relation'],\n",
    "                     'object': str(row['object'])})\n",
    "\n",
    "    sujeto = set(tripleta.sujeto.split())\n",
    "    objeto = set(tripleta.objeto.split()) if VISITAR_OBJETO else set()\n",
    "\n",
    "    # Lógica de comparación entre tripletas\n",
    "    if anterior is None:\n",
    "        anterior = tripleta\n",
    "    \n",
    "    misma_super = (tripleta.esTripletaSuper(anterior) == anterior.esTripletaSuper(tripleta))\n",
    "    dif = tripleta.dondeSonDiferentes(anterior)\n",
    "\n",
    "    if (misma_super and (dif == ('sujeto', 'relacion', 'objeto') or dif == ('sujeto', None, 'objeto'))):\n",
    "        anterior = tripleta\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Filtro por términos relevantes\n",
    "    if (TERMINOS_A_INCLUIR is None\n",
    "            or not TERMINOS_A_INCLUIR.isdisjoint(sujeto)\n",
    "            or (VISITAR_OBJETO and not TERMINOS_A_INCLUIR.isdisjoint(objeto))):\n",
    "\n",
    "        visitados = set()\n",
    "        encontradas = sujeto.intersection(palabrasdbpedia)\n",
    "        no_encontradas = sujeto.difference(palabrasdbpedia)\n",
    "\n",
    "        if VISITAR_OBJETO:\n",
    "            encontradas.update(objeto.intersection(palabrasdbpedia))\n",
    "            no_encontradas.update(objeto.difference(palabrasdbpedia))\n",
    "\n",
    "        for termino in encontradas:\n",
    "            termino_lower = termino.lower()\n",
    "\n",
    "            if termino in visitados:\n",
    "                continue\n",
    "\n",
    "            if termino[0].isdigit():\n",
    "                no_encontradas.add(termino)\n",
    "                continue\n",
    "\n",
    "            info_list = dictdbp.get(termino_lower, [])\n",
    "            if not info_list:\n",
    "                no_encontradas.add(termino)\n",
    "                continue\n",
    "\n",
    "            info_termino = info_list[0]\n",
    "            uri_db = info_termino.get('URI', '')\n",
    "            tipos_db = info_termino.get('tipos', [])\n",
    "\n",
    "            # Extracción de sinónimos y hypernyms de WordNet\n",
    "            sinonimos = []\n",
    "            lwordnet = []\n",
    "            for syn in wn.synsets(termino):\n",
    "                sinonimos.extend(syn.lemma_names())\n",
    "                for h in syn.hypernyms():\n",
    "                    lwordnet.extend(h.lemma_names())\n",
    "\n",
    "            # Información NER\n",
    "            sujeto_en_ner = dictner.get(termino_lower, '')\n",
    "            ner = []\n",
    "            if sujeto_en_ner:\n",
    "                ner.append(sujeto_en_ner)\n",
    "\n",
    "            diccionario_termino = {\n",
    "                'termino': termino,\n",
    "                'sinonimos': list(set(sinonimos)),\n",
    "                'resource': uri_db,\n",
    "                'dbpedia': tipos_db,\n",
    "                'ner': ner,\n",
    "                'wordnet': lwordnet\n",
    "            }\n",
    "\n",
    "            listado_tripletas.append(diccionario_termino)\n",
    "            visitados.add(termino)\n",
    "\n",
    "print(f\"Términos extraídos: {len(listado_tripletas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expansión de Vocabulario con Similitud Semántica\n",
    "Uso de spaCy para encontrar términos similares y expandir el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(listado_tripletas)\n",
    "vocab_aux, lista_tipos = [], []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    termino = row['termino']\n",
    "    tipos = []\n",
    "    tipos.extend(row['dbpedia'] if isinstance(row['dbpedia'], list) else row['dbpedia'].split(','))\n",
    "    tipos.extend(row['wordnet'])\n",
    "    \n",
    "    # Limpieza de tipos\n",
    "    tipos_clean = []\n",
    "    for t in tipos:\n",
    "        el = return_url_element(remove_dbpedia_categories(remove_numbers(str(t))))\n",
    "        if el and el != 'Q': tipos_clean.append(el)\n",
    "    \n",
    "    # Cálculo de similitudes\n",
    "    sims = [nlp(termino).similarity(nlp(t2)) for t2 in tipos_clean]\n",
    "    if not sims: continue\n",
    "    \n",
    "    # Selección de los N_SINONIMOS más similares\n",
    "    idx = list(np.argpartition(sims, -N_SINONIMOS)[-N_SINONIMOS:])\n",
    "    sel = itemgetter(*idx)(tipos_clean)\n",
    "    puntuaciones = itemgetter(*idx)(sims)\n",
    "    \n",
    "    lista_tipos.append({'termino': termino, 'tipos': sel, 'similitudes': puntuaciones})\n",
    "    vocab_aux.append(termino)\n",
    "    if isinstance(sel, str): vocab_aux.append(sel)\n",
    "    else: vocab_aux.extend(sel)\n",
    "\n",
    "# Lemmatización del vocabulario\n",
    "vocab = set()\n",
    "for doc in nlp.pipe(vocab_aux):\n",
    "    lemmatized = \" \".join([token.lemma_.lower() for token in doc])\n",
    "    vocab.add(lemmatized)\n",
    "\n",
    "terms = list(vocab)\n",
    "print(f\"Términos finales después de expansión: {len(terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Jerárquico\n",
    "Agrupamiento de términos basado en similitud semántica y evaluación con silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de similitud\n",
    "M = np.array([[nlp(t1).similarity(nlp(t2)) for t2 in terms] for t1 in terms])\n",
    "\n",
    "# Clustering\n",
    "labels = AgglomerativeClustering(n_clusters=min(len(terms)-1, TOP_K_TERMS)).fit_predict(M)\n",
    "sample_sil = silhouette_samples(M, labels)\n",
    "global_sil = silhouette_score(M, labels)\n",
    "\n",
    "# Selección de términos más representativos por cluster\n",
    "clusters = {}\n",
    "for cl in set(labels):\n",
    "    idxs = np.where(labels==cl)[0]\n",
    "    term_sils = [(terms[i], sample_sil[i]) for i in idxs]\n",
    "    top_terms = [t for t,_ in sorted(term_sils, key=lambda x: -x[1])[:TOP_K_TERMS]]\n",
    "    clusters[cl] = top_terms\n",
    "\n",
    "# Guardado de resultados\n",
    "clusters_str = {str(k): v for k, v in clusters.items()}\n",
    "with open(os.path.join(OUTPUT_DIR,'clusters.json'),'w', encoding='utf-8') as f:\n",
    "    json.dump({'best_k': len(clusters_str), 'global_sil': global_sil, 'clusters': clusters_str}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Clusters generados: {len(clusters)} con silhouette score: {global_sil:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generación de Explicaciones con Qwen-2.5\n",
    "Uso de LLM para generar explicaciones comprensibles de cada cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo generativo\n",
    "torch_dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tok_g = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
    "mod_g = AutoModelForCausalLM.from_pretrained(GEN_MODEL).to(torch_dev)\n",
    "gen = pipeline('text-generation', model=mod_g, tokenizer=tok_g,\n",
    "               device=0 if torch.cuda.is_available() else -1,\n",
    "               return_full_text=False)\n",
    "\n",
    "# Ejemplo few-shot\n",
    "ejemplo = (\n",
    "    \"Ejemplo:\\n\"\n",
    "    \"{'explicación':'Este cluster agrupa términos relacionados con la tecnología móvil y las comunicaciones.',\"\n",
    "    \"'coherencia':4,'relevancia':5,'cobertura':4}\\n\\n\"\n",
    ")\n",
    "\n",
    "explanations = {}\n",
    "for cid, terms_c in clusters.items():\n",
    "    prompt = (\n",
    "        ejemplo +\n",
    "        f\"Cluster {TOPIC_ID}-{cid}: términos {', '.join(terms_c)}. \"\n",
    "        f\"Silhouette global={global_sil:.3f}.\\n\"\n",
    "        \"Genera solo un JSON con claves 'explicación','coherencia','relevancia','cobertura'.\"\n",
    "    )\n",
    "    out = gen(prompt, max_new_tokens=150, do_sample=False)\n",
    "    txt = out[0].get('generated_text','')\n",
    "    try:\n",
    "        jf = txt[txt.find('{'):txt.rfind('}')+1]\n",
    "        parsed = json.loads(jf)\n",
    "    except:\n",
    "        parsed = {'explicación': txt.strip()}\n",
    "    explanations[cid] = parsed\n",
    "\n",
    "# Guardado de explicaciones\n",
    "explanations_str = {str(k): v for k, v in explanations.items()}\n",
    "with open(os.path.join(OUTPUT_DIR,'explanations.json'),'w', encoding='utf-8') as f:\n",
    "    json.dump(explanations_str, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Explicaciones generadas para todos los clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluación con mT5\n",
    "Evaluación automática de la calidad de las explicaciones generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo de evaluación\n",
    "tok_e = T5Tokenizer.from_pretrained(EVAL_MODEL)\n",
    "mod_e = T5ForConditionalGeneration.from_pretrained(EVAL_MODEL).to(torch_dev)\n",
    "evalp = pipeline(\n",
    "    'text2text-generation', model=mod_e, tokenizer=tok_e,\n",
    "    device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Ejemplo few-shot para evaluación\n",
    "ejemplo_eval = (\n",
    "    \"Ejemplo de evaluación con justificación:\"\n",
    "    \"{'coherencia':4,'relevancia':5,'cobertura':4}\"\n",
    "    \"Justificación: La explicación agrupa bien los términos (coherencia alta), cubre aspectos clave del tópico (relevancia máxima)\"\n",
    "    \"y describe adecuadamente la amplitud temática (cobertura alta).\"\n",
    ")\n",
    "\n",
    "evaluations = {}\n",
    "for cid, exp in explanations.items():\n",
    "    terms_c = clusters[cid]\n",
    "    prompt = (\n",
    "        ejemplo_eval +\n",
    "        f\"Evalúa esta explicación para el cluster {TOPIC_ID}-{cid}.\"\n",
    "        f\"Términos clave: {', '.join(terms_c)}.\"\n",
    "        f\"Explicación: {exp.get('explicación', exp)}.\"\n",
    "        \"Devuelve un JSON con claves 'coherencia','relevancia','cobertura' y añade un campo 'justificación' con unas 1-2 frases.\"\n",
    "    )\n",
    "    out = evalp(prompt, max_new_tokens=150, do_sample=False)\n",
    "    txt = out[0].get('generated_text','')\n",
    "    try:\n",
    "        jf = txt[txt.find('{'):txt.rfind('}')+1]\n",
    "        parsed = json.loads(jf)\n",
    "    except:\n",
    "        parsed = {'error': txt.strip()}\n",
    "    evaluations[cid] = parsed\n",
    "\n",
    "# Guardado de evaluaciones\n",
    "evaluations_str = {str(k): v for k, v in evaluations.items()}\n",
    "with open(os.path.join(OUTPUT_DIR,'evaluations.json'),'w', encoding='utf-8') as f:\n",
    "    json.dump(evaluations_str, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Evaluaciones completadas para todas las explicaciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resumen Final\n",
    "Generación de un resumen ejecutivo con los resultados principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR,'summary.txt'),'w', encoding='utf-8') as f:\n",
    "    f.write(f\"Tópico {TOPIC_ID}: {len(clusters)} clusters, silhouette global {global_sil:.3f}\\n\")\n",
    "    for cid, terms_c in clusters.items():\n",
    "        exp = explanations[cid].get('explicación','')\n",
    "        eva = evaluations[cid]\n",
    "        coh = eva.get('coherencia','N/A')\n",
    "        rel = eva.get('relevancia','N/A')\n",
    "        cov = eva.get('cobertura','N/A')\n",
    "        f.write(f\"Cluster {cid}: explicación='{exp}' coherencia={coh} relevancia={rel} cobertura={cov}\\n\")\n",
    "\n",
    "print('Pipeline completo. Resultados guardados en:', OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
