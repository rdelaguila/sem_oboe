{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa438ff-f0fb-45c3-b401-fd2093b359d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "df_topic = joblib.load('data/lda_eval/bbc/df_topic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01d14ff-b395-466d-8727-023a0415da55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>type</th>\n",
       "      <th>text_coref</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>entidades_raw</th>\n",
       "      <th>entidades_dbpedia</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>topic_dist</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "      <td>business</td>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "      <td>china had role in yukos splitup\\n \\n china len...</td>\n",
       "      <td>{'yukos': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'yukos': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>china had role in yukos split-up\\n \\n china le...</td>\n",
       "      <td>[china, role, yukos, split, china, lend, russi...</td>\n",
       "      <td>[(2, 0.1521518), (4, 0.026116), (5, 0.10821417...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "      <td>business</td>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "      <td>oil rebounds from weather effect\\n \\n oil pric...</td>\n",
       "      <td>{'asian': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'asian': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>oil rebounds from weather effect\\n \\n oil pric...</td>\n",
       "      <td>[oil, rebound, weather, effect, oil, price, re...</td>\n",
       "      <td>[(1, 0.33736482), (2, 0.14471534), (5, 0.24712...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "      <td>business</td>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "      <td>indonesia declines debt freeze\\n \\n indonesia ...</td>\n",
       "      <td>{'aceh province': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>{'aceh province': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>indonesia 'declines debt freeze'\\n \\n indonesi...</td>\n",
       "      <td>[indonesia, decline, debt, freeze, indonesia, ...</td>\n",
       "      <td>[(2, 0.33151984), (3, 0.070257254), (7, 0.3591...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "      <td>business</td>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "      <td>$1m payoff for former shell boss\\n \\n shell is...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>$1m payoff for former shell boss\\n \\n shell is...</td>\n",
       "      <td>[payoff, shell, boss, shell, pay, ex, finance,...</td>\n",
       "      <td>[(0, 0.076440915), (1, 0.08356439), (3, 0.1388...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
       "      <td>business</td>\n",
       "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
       "      <td>us bank in $515m sec settlement\\n \\n five bank...</td>\n",
       "      <td>{'mutual fund': {'URI': 'http://dbpedia.org/re...</td>\n",
       "      <td>{'mutual fund': {'URI': 'http://dbpedia.org/re...</td>\n",
       "      <td>us bank in $515m sec settlement\\n \\n five bank...</td>\n",
       "      <td>[bank, sec, settlement, bank, america, subsidi...</td>\n",
       "      <td>[(1, 0.08517497), (4, 0.19468576), (10, 0.4840...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Microsoft launches its own search\\n \\n Microso...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft launches its own search\\n \\n Microso...</td>\n",
       "      <td>microsoft launches its own search\\n \\n microso...</td>\n",
       "      <td>{'search engine': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>{'search engine': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>microsoft launches its own search\\n \\n microso...</td>\n",
       "      <td>[microsoft, launch, search, microsoft, unveil,...</td>\n",
       "      <td>[(5, 0.06285988), (10, 0.93661404)]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Warnings about junk mail deluge\\n \\n The amoun...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Warnings about junk mail deluge\\n \\n The amoun...</td>\n",
       "      <td>warnings about junk mail deluge\\n \\n the amoun...</td>\n",
       "      <td>{'virus': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'virus': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>warnings about junk mail deluge\\n \\n the amoun...</td>\n",
       "      <td>[warning, junk, mail, deluge, spam, circulate,...</td>\n",
       "      <td>[(4, 0.8922938), (6, 0.048490256), (10, 0.0587...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Microsoft gets the blogging bug\\n \\n Software ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft gets the blogging bug\\n \\n Software ...</td>\n",
       "      <td>microsoft gets the blogging bug\\n \\n software ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>microsoft gets the blogging bug\\n \\n software ...</td>\n",
       "      <td>[microsoft, blogging, bug, software, giant, mi...</td>\n",
       "      <td>[(5, 0.9989484)]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Gamers snap up new Sony PSP\\n \\n Gamers have b...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Gamers snap up new Sony PSP\\n \\n Gamers have b...</td>\n",
       "      <td>gamers snap up new sony psp\\n \\n gamers have b...</td>\n",
       "      <td>{'sony': {'URI': 'http://dbpedia.org/resource/...</td>\n",
       "      <td>{'sony': {'URI': 'http://dbpedia.org/resource/...</td>\n",
       "      <td>gamers snap up new sony psp\\n \\n gamers have b...</td>\n",
       "      <td>[gamer, snap, new, sony, psp, gamer, buy, batc...</td>\n",
       "      <td>[(0, 0.8536542), (10, 0.14511149)]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n \\n The App...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n \\n The App...</td>\n",
       "      <td>apple laptop is greatest gadget\\n \\n the apple...</td>\n",
       "      <td>{'apple': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'apple': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>apple laptop is 'greatest gadget'\\n \\n the app...</td>\n",
       "      <td>[apple, laptop, great, gadget, apple, powerboo...</td>\n",
       "      <td>[(0, 0.5367707), (5, 0.27873483), (6, 0.184266...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news      type  \\\n",
       "0     China had role in Yukos split-up\\n \\n China le...  business   \n",
       "1     Oil rebounds from weather effect\\n \\n Oil pric...  business   \n",
       "2     Indonesia 'declines debt freeze'\\n \\n Indonesi...  business   \n",
       "3     $1m payoff for former Shell boss\\n \\n Shell is...  business   \n",
       "4     US bank in $515m SEC settlement\\n \\n Five Bank...  business   \n",
       "...                                                 ...       ...   \n",
       "2220  Microsoft launches its own search\\n \\n Microso...      tech   \n",
       "2221  Warnings about junk mail deluge\\n \\n The amoun...      tech   \n",
       "2222  Microsoft gets the blogging bug\\n \\n Software ...      tech   \n",
       "2223  Gamers snap up new Sony PSP\\n \\n Gamers have b...      tech   \n",
       "2224  Apple laptop is 'greatest gadget'\\n \\n The App...      tech   \n",
       "\n",
       "                                             text_coref  \\\n",
       "0     China had role in Yukos split-up\\n \\n China le...   \n",
       "1     Oil rebounds from weather effect\\n \\n Oil pric...   \n",
       "2     Indonesia 'declines debt freeze'\\n \\n Indonesi...   \n",
       "3     $1m payoff for former Shell boss\\n \\n Shell is...   \n",
       "4     US bank in $515m SEC settlement\\n \\n Five Bank...   \n",
       "...                                                 ...   \n",
       "2220  Microsoft launches its own search\\n \\n Microso...   \n",
       "2221  Warnings about junk mail deluge\\n \\n The amoun...   \n",
       "2222  Microsoft gets the blogging bug\\n \\n Software ...   \n",
       "2223  Gamers snap up new Sony PSP\\n \\n Gamers have b...   \n",
       "2224  Apple laptop is 'greatest gadget'\\n \\n The App...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "0     china had role in yukos splitup\\n \\n china len...   \n",
       "1     oil rebounds from weather effect\\n \\n oil pric...   \n",
       "2     indonesia declines debt freeze\\n \\n indonesia ...   \n",
       "3     $1m payoff for former shell boss\\n \\n shell is...   \n",
       "4     us bank in $515m sec settlement\\n \\n five bank...   \n",
       "...                                                 ...   \n",
       "2220  microsoft launches its own search\\n \\n microso...   \n",
       "2221  warnings about junk mail deluge\\n \\n the amoun...   \n",
       "2222  microsoft gets the blogging bug\\n \\n software ...   \n",
       "2223  gamers snap up new sony psp\\n \\n gamers have b...   \n",
       "2224  apple laptop is greatest gadget\\n \\n the apple...   \n",
       "\n",
       "                                          entidades_raw  \\\n",
       "0     {'yukos': {'URI': 'http://dbpedia.org/resource...   \n",
       "1     {'asian': {'URI': 'http://dbpedia.org/resource...   \n",
       "2     {'aceh province': {'URI': 'http://dbpedia.org/...   \n",
       "3                                                    {}   \n",
       "4     {'mutual fund': {'URI': 'http://dbpedia.org/re...   \n",
       "...                                                 ...   \n",
       "2220  {'search engine': {'URI': 'http://dbpedia.org/...   \n",
       "2221  {'virus': {'URI': 'http://dbpedia.org/resource...   \n",
       "2222                                                 {}   \n",
       "2223  {'sony': {'URI': 'http://dbpedia.org/resource/...   \n",
       "2224  {'apple': {'URI': 'http://dbpedia.org/resource...   \n",
       "\n",
       "                                      entidades_dbpedia  \\\n",
       "0     {'yukos': {'URI': 'http://dbpedia.org/resource...   \n",
       "1     {'asian': {'URI': 'http://dbpedia.org/resource...   \n",
       "2     {'aceh province': {'URI': 'http://dbpedia.org/...   \n",
       "3                                                    {}   \n",
       "4     {'mutual fund': {'URI': 'http://dbpedia.org/re...   \n",
       "...                                                 ...   \n",
       "2220  {'search engine': {'URI': 'http://dbpedia.org/...   \n",
       "2221  {'virus': {'URI': 'http://dbpedia.org/resource...   \n",
       "2222                                                 {}   \n",
       "2223  {'sony': {'URI': 'http://dbpedia.org/resource/...   \n",
       "2224  {'apple': {'URI': 'http://dbpedia.org/resource...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0     china had role in yukos split-up\\n \\n china le...   \n",
       "1     oil rebounds from weather effect\\n \\n oil pric...   \n",
       "2     indonesia 'declines debt freeze'\\n \\n indonesi...   \n",
       "3     $1m payoff for former shell boss\\n \\n shell is...   \n",
       "4     us bank in $515m sec settlement\\n \\n five bank...   \n",
       "...                                                 ...   \n",
       "2220  microsoft launches its own search\\n \\n microso...   \n",
       "2221  warnings about junk mail deluge\\n \\n the amoun...   \n",
       "2222  microsoft gets the blogging bug\\n \\n software ...   \n",
       "2223  gamers snap up new sony psp\\n \\n gamers have b...   \n",
       "2224  apple laptop is 'greatest gadget'\\n \\n the app...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [china, role, yukos, split, china, lend, russi...   \n",
       "1     [oil, rebound, weather, effect, oil, price, re...   \n",
       "2     [indonesia, decline, debt, freeze, indonesia, ...   \n",
       "3     [payoff, shell, boss, shell, pay, ex, finance,...   \n",
       "4     [bank, sec, settlement, bank, america, subsidi...   \n",
       "...                                                 ...   \n",
       "2220  [microsoft, launch, search, microsoft, unveil,...   \n",
       "2221  [warning, junk, mail, deluge, spam, circulate,...   \n",
       "2222  [microsoft, blogging, bug, software, giant, mi...   \n",
       "2223  [gamer, snap, new, sony, psp, gamer, buy, batc...   \n",
       "2224  [apple, laptop, great, gadget, apple, powerboo...   \n",
       "\n",
       "                                             topic_dist  topic  \n",
       "0     [(2, 0.1521518), (4, 0.026116), (5, 0.10821417...     10  \n",
       "1     [(1, 0.33736482), (2, 0.14471534), (5, 0.24712...      1  \n",
       "2     [(2, 0.33151984), (3, 0.070257254), (7, 0.3591...      7  \n",
       "3     [(0, 0.076440915), (1, 0.08356439), (3, 0.1388...     10  \n",
       "4     [(1, 0.08517497), (4, 0.19468576), (10, 0.4840...     10  \n",
       "...                                                 ...    ...  \n",
       "2220                [(5, 0.06285988), (10, 0.93661404)]     10  \n",
       "2221  [(4, 0.8922938), (6, 0.048490256), (10, 0.0587...      4  \n",
       "2222                                   [(5, 0.9989484)]      5  \n",
       "2223                 [(0, 0.8536542), (10, 0.14511149)]      0  \n",
       "2224  [(0, 0.5367707), (5, 0.27873483), (6, 0.184266...      0  \n",
       "\n",
       "[2225 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85309fb-6023-455e-b22d-46f98933ece0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_coref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Microsoft launches its own search\\n \\n Microso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Warnings about junk mail deluge\\n \\n The amoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Microsoft gets the blogging bug\\n \\n Software ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Gamers snap up new Sony PSP\\n \\n Gamers have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n \\n The App...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_coref\n",
       "0     China had role in Yukos split-up\\n \\n China le...\n",
       "1     Oil rebounds from weather effect\\n \\n Oil pric...\n",
       "2     Indonesia 'declines debt freeze'\\n \\n Indonesi...\n",
       "3     $1m payoff for former Shell boss\\n \\n Shell is...\n",
       "4     US bank in $515m SEC settlement\\n \\n Five Bank...\n",
       "...                                                 ...\n",
       "2220  Microsoft launches its own search\\n \\n Microso...\n",
       "2221  Warnings about junk mail deluge\\n \\n The amoun...\n",
       "2222  Microsoft gets the blogging bug\\n \\n Software ...\n",
       "2223  Gamers snap up new Sony PSP\\n \\n Gamers have b...\n",
       "2224  Apple laptop is 'greatest gadget'\\n \\n The App...\n",
       "\n",
       "[2225 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic[['text_coref']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60879159-2c7c-45dd-b93f-062daa97c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "df2 = joblib.load('data/processed/bbc/bbc_processed_semantic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0f687d-4316-4125-948a-0931e4485173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>type</th>\n",
       "      <th>text_coref</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>entidades_raw</th>\n",
       "      <th>entidades_dbpedia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "      <td>business</td>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "      <td>china had role in yukos splitup\\n \\n china len...</td>\n",
       "      <td>{'yukos': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'yukos': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "      <td>business</td>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "      <td>oil rebounds from weather effect\\n \\n oil pric...</td>\n",
       "      <td>{'asian': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'asian': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "      <td>business</td>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "      <td>indonesia declines debt freeze\\n \\n indonesia ...</td>\n",
       "      <td>{'aceh province': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>{'aceh province': {'URI': 'http://dbpedia.org/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "      <td>business</td>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "      <td>$1m payoff for former shell boss\\n \\n shell is...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
       "      <td>business</td>\n",
       "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
       "      <td>us bank in $515m sec settlement\\n \\n five bank...</td>\n",
       "      <td>{'mutual fund': {'URI': 'http://dbpedia.org/re...</td>\n",
       "      <td>{'mutual fund': {'URI': 'http://dbpedia.org/re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Microsoft launches its own search\\n \\n Microso...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft launches its own search\\n \\n Microso...</td>\n",
       "      <td>microsoft launches its own search\\n \\n microso...</td>\n",
       "      <td>{'search engine': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>{'search engine': {'URI': 'http://dbpedia.org/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Warnings about junk mail deluge\\n \\n The amoun...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Warnings about junk mail deluge\\n \\n The amoun...</td>\n",
       "      <td>warnings about junk mail deluge\\n \\n the amoun...</td>\n",
       "      <td>{'virus': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'virus': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Microsoft gets the blogging bug\\n \\n Software ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft gets the blogging bug\\n \\n Software ...</td>\n",
       "      <td>microsoft gets the blogging bug\\n \\n software ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Gamers snap up new Sony PSP\\n \\n Gamers have b...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Gamers snap up new Sony PSP\\n \\n Gamers have b...</td>\n",
       "      <td>gamers snap up new sony psp\\n \\n gamers have b...</td>\n",
       "      <td>{'sony': {'URI': 'http://dbpedia.org/resource/...</td>\n",
       "      <td>{'sony': {'URI': 'http://dbpedia.org/resource/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n \\n The App...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n \\n The App...</td>\n",
       "      <td>apple laptop is greatest gadget\\n \\n the apple...</td>\n",
       "      <td>{'apple': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'apple': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news      type  \\\n",
       "0     China had role in Yukos split-up\\n \\n China le...  business   \n",
       "1     Oil rebounds from weather effect\\n \\n Oil pric...  business   \n",
       "2     Indonesia 'declines debt freeze'\\n \\n Indonesi...  business   \n",
       "3     $1m payoff for former Shell boss\\n \\n Shell is...  business   \n",
       "4     US bank in $515m SEC settlement\\n \\n Five Bank...  business   \n",
       "...                                                 ...       ...   \n",
       "2220  Microsoft launches its own search\\n \\n Microso...      tech   \n",
       "2221  Warnings about junk mail deluge\\n \\n The amoun...      tech   \n",
       "2222  Microsoft gets the blogging bug\\n \\n Software ...      tech   \n",
       "2223  Gamers snap up new Sony PSP\\n \\n Gamers have b...      tech   \n",
       "2224  Apple laptop is 'greatest gadget'\\n \\n The App...      tech   \n",
       "\n",
       "                                             text_coref  \\\n",
       "0     China had role in Yukos split-up\\n \\n China le...   \n",
       "1     Oil rebounds from weather effect\\n \\n Oil pric...   \n",
       "2     Indonesia 'declines debt freeze'\\n \\n Indonesi...   \n",
       "3     $1m payoff for former Shell boss\\n \\n Shell is...   \n",
       "4     US bank in $515m SEC settlement\\n \\n Five Bank...   \n",
       "...                                                 ...   \n",
       "2220  Microsoft launches its own search\\n \\n Microso...   \n",
       "2221  Warnings about junk mail deluge\\n \\n The amoun...   \n",
       "2222  Microsoft gets the blogging bug\\n \\n Software ...   \n",
       "2223  Gamers snap up new Sony PSP\\n \\n Gamers have b...   \n",
       "2224  Apple laptop is 'greatest gadget'\\n \\n The App...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "0     china had role in yukos splitup\\n \\n china len...   \n",
       "1     oil rebounds from weather effect\\n \\n oil pric...   \n",
       "2     indonesia declines debt freeze\\n \\n indonesia ...   \n",
       "3     $1m payoff for former shell boss\\n \\n shell is...   \n",
       "4     us bank in $515m sec settlement\\n \\n five bank...   \n",
       "...                                                 ...   \n",
       "2220  microsoft launches its own search\\n \\n microso...   \n",
       "2221  warnings about junk mail deluge\\n \\n the amoun...   \n",
       "2222  microsoft gets the blogging bug\\n \\n software ...   \n",
       "2223  gamers snap up new sony psp\\n \\n gamers have b...   \n",
       "2224  apple laptop is greatest gadget\\n \\n the apple...   \n",
       "\n",
       "                                          entidades_raw  \\\n",
       "0     {'yukos': {'URI': 'http://dbpedia.org/resource...   \n",
       "1     {'asian': {'URI': 'http://dbpedia.org/resource...   \n",
       "2     {'aceh province': {'URI': 'http://dbpedia.org/...   \n",
       "3                                                    {}   \n",
       "4     {'mutual fund': {'URI': 'http://dbpedia.org/re...   \n",
       "...                                                 ...   \n",
       "2220  {'search engine': {'URI': 'http://dbpedia.org/...   \n",
       "2221  {'virus': {'URI': 'http://dbpedia.org/resource...   \n",
       "2222                                                 {}   \n",
       "2223  {'sony': {'URI': 'http://dbpedia.org/resource/...   \n",
       "2224  {'apple': {'URI': 'http://dbpedia.org/resource...   \n",
       "\n",
       "                                      entidades_dbpedia  \n",
       "0     {'yukos': {'URI': 'http://dbpedia.org/resource...  \n",
       "1     {'asian': {'URI': 'http://dbpedia.org/resource...  \n",
       "2     {'aceh province': {'URI': 'http://dbpedia.org/...  \n",
       "3                                                    {}  \n",
       "4     {'mutual fund': {'URI': 'http://dbpedia.org/re...  \n",
       "...                                                 ...  \n",
       "2220  {'search engine': {'URI': 'http://dbpedia.org/...  \n",
       "2221  {'virus': {'URI': 'http://dbpedia.org/resource...  \n",
       "2222                                                 {}  \n",
       "2223  {'sony': {'URI': 'http://dbpedia.org/resource/...  \n",
       "2224  {'apple': {'URI': 'http://dbpedia.org/resource...  \n",
       "\n",
       "[2225 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95fcec1f-f8f2-434a-b7f5-2840a44fd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = joblib.load('data/corpus_raw/bbc/bbc_processed_final_semantic_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18466f58-5e58-4532-afd4-8943ed4559e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>coref_text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>entidades</th>\n",
       "      <th>new_target</th>\n",
       "      <th>entidades_dbpedia</th>\n",
       "      <th>entidades_dbpedia_simplificadas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "      <td>{Starcom MediaVest, CES, Windows, Older, BBC, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Televisi...</td>\n",
       "      <td>{'TV': [{'URI': 'http://dbpedia.org/resource/T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss leave book worldcom boss bernie ...</td>\n",
       "      <td>{20,000, David Myers, Worldcom, 11bn, Reid Wei...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/MCI_Inc....</td>\n",
       "      <td>{'Worldcom': [{'URI': 'http://dbpedia.org/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tiger wary farrell gamble leicester tiger wary...</td>\n",
       "      <td>{Farrell, Tigers, Saracens, John Wells, five w...</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Colin_Fa...</td>\n",
       "      <td>{'Farrell': [{'URI': 'http://dbpedia.org/resou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeade face newcastle fa cup premiership newcas...</td>\n",
       "      <td>{Brentford, West Brom, third, the weekend of 8...</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Yeading_...</td>\n",
       "      <td>{'Yeading': [{'URI': 'http://dbpedia.org/resou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s raid box office ocean s crime caper se...</td>\n",
       "      <td>{Oscar, Dan Fellman, $40.8m, third, Ocean, $38...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Crime_fi...</td>\n",
       "      <td>{'crime': [{'URI': 'http://dbpedia.org/resourc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                          coref_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0  tv future hand viewer home theatre system plas...   \n",
       "1  worldcom boss leave book worldcom boss bernie ...   \n",
       "2  tiger wary farrell gamble leicester tiger wary...   \n",
       "3  yeade face newcastle fa cup premiership newcas...   \n",
       "4  ocean s raid box office ocean s crime caper se...   \n",
       "\n",
       "                                           entidades  new_target  \\\n",
       "0  {Starcom MediaVest, CES, Windows, Older, BBC, ...           3   \n",
       "1  {20,000, David Myers, Worldcom, 11bn, Reid Wei...           1   \n",
       "2  {Farrell, Tigers, Saracens, John Wells, five w...           5   \n",
       "3  {Brentford, West Brom, third, the weekend of 8...           2   \n",
       "4  {Oscar, Dan Fellman, $40.8m, third, Ocean, $38...           0   \n",
       "\n",
       "                                   entidades_dbpedia  \\\n",
       "0  [{'URI': 'http://dbpedia.org/resource/Televisi...   \n",
       "1  [{'URI': 'http://dbpedia.org/resource/MCI_Inc....   \n",
       "2  [{'URI': 'http://dbpedia.org/resource/Colin_Fa...   \n",
       "3  [{'URI': 'http://dbpedia.org/resource/Yeading_...   \n",
       "4  [{'URI': 'http://dbpedia.org/resource/Crime_fi...   \n",
       "\n",
       "                     entidades_dbpedia_simplificadas  \n",
       "0  {'TV': [{'URI': 'http://dbpedia.org/resource/T...  \n",
       "1  {'Worldcom': [{'URI': 'http://dbpedia.org/reso...  \n",
       "2  {'Farrell': [{'URI': 'http://dbpedia.org/resou...  \n",
       "3  {'Yeading': [{'URI': 'http://dbpedia.org/resou...  \n",
       "4  {'crime': [{'URI': 'http://dbpedia.org/resourc...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb550e04-d818-4b9d-9ab5-5e8397b7c569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>coref_text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>entidades</th>\n",
       "      <th>new_target</th>\n",
       "      <th>entidades_dbpedia</th>\n",
       "      <th>entidades_dbpedia_simplificadas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>car pull retail figure retail sale fall 0.3 ja...</td>\n",
       "      <td>{Nomura Securities International, Popular Secu...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Car', 's...</td>\n",
       "      <td>{'Cars': [{'URI': 'http://dbpedia.org/resource...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "      <td>kilroy unveil immigration policy ex chatshow h...</td>\n",
       "      <td>{Robert Kilroy-Silk, the United Nations Conven...</td>\n",
       "      <td>6</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Robert_K...</td>\n",
       "      <td>{'Kilroy': [{'URI': 'http://dbpedia.org/resour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>rem announce new glasgow concert band rem anno...</td>\n",
       "      <td>{Balloch Castle Country Park, Glasgow, SECC, B...</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Glasgow'...</td>\n",
       "      <td>{'Glasgow': [{'URI': 'http://dbpedia.org/resou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>political squabble snowball s commonplace argu...</td>\n",
       "      <td>{two, Brown, Google, Mr Brown, Blair and Brown...</td>\n",
       "      <td>6</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Snowball...</td>\n",
       "      <td>{'snowball': [{'URI': 'http://dbpedia.org/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>souness delight euro progress boss graeme soun...</td>\n",
       "      <td>{200, 4, the first half, Laurent Robert, the U...</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Graeme_S...</td>\n",
       "      <td>{'Graeme Souness': [{'URI': 'http://dbpedia.or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text  \\\n",
       "2220       business  cars pull down us retail figures us retail sal...   \n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...   \n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...   \n",
       "2223       politics  how political squabbles snowball it s become c...   \n",
       "2224          sport  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "                                             coref_text  \\\n",
       "2220  cars pull down us retail figures us retail sal...   \n",
       "2221  kilroy unveils immigration policy ex-chatshow ...   \n",
       "2222  rem announce new glasgow concert us band rem h...   \n",
       "2223  how political squabbles snowball it s become c...   \n",
       "2224  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "2220  car pull retail figure retail sale fall 0.3 ja...   \n",
       "2221  kilroy unveil immigration policy ex chatshow h...   \n",
       "2222  rem announce new glasgow concert band rem anno...   \n",
       "2223  political squabble snowball s commonplace argu...   \n",
       "2224  souness delight euro progress boss graeme soun...   \n",
       "\n",
       "                                              entidades  new_target  \\\n",
       "2220  {Nomura Securities International, Popular Secu...           1   \n",
       "2221  {Robert Kilroy-Silk, the United Nations Conven...           6   \n",
       "2222  {Balloch Castle Country Park, Glasgow, SECC, B...           4   \n",
       "2223  {two, Brown, Google, Mr Brown, Blair and Brown...           6   \n",
       "2224  {200, 4, the first half, Laurent Robert, the U...           2   \n",
       "\n",
       "                                      entidades_dbpedia  \\\n",
       "2220  [{'URI': 'http://dbpedia.org/resource/Car', 's...   \n",
       "2221  [{'URI': 'http://dbpedia.org/resource/Robert_K...   \n",
       "2222  [{'URI': 'http://dbpedia.org/resource/Glasgow'...   \n",
       "2223  [{'URI': 'http://dbpedia.org/resource/Snowball...   \n",
       "2224  [{'URI': 'http://dbpedia.org/resource/Graeme_S...   \n",
       "\n",
       "                        entidades_dbpedia_simplificadas  \n",
       "2220  {'Cars': [{'URI': 'http://dbpedia.org/resource...  \n",
       "2221  {'Kilroy': [{'URI': 'http://dbpedia.org/resour...  \n",
       "2222  {'Glasgow': [{'URI': 'http://dbpedia.org/resou...  \n",
       "2223  {'snowball': [{'URI': 'http://dbpedia.org/reso...  \n",
       "2224  {'Graeme Souness': [{'URI': 'http://dbpedia.or...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdaaa7f-2439-48cd-aa5f-bd3f36e0f6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Raul/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-31 19:09:19 WARNING: Setting 'start_server' to a boolean value when constructing CoreNLPClient is deprecated and will stop to function in a future version of stanza. Please consider switching to using a value from stanza.server.StartServer.\n",
      "2025-07-31 19:09:19 INFO: Writing properties to tmp file: corenlp_server-e78b2ae654a84eea.props\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Please install CoreNLP by running `stanza.install_corenlp()`. If you have installed it, please define $CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter.  $CORENLP_HOME=None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstanza\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoreNLPClient\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m client = \u001b[43mCoreNLPClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mannotators\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopenie\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp://localhost:9000\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstart_server\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbe_quiet\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/stanza/server/client.py:305\u001b[39m, in \u001b[36mCoreNLPClient.__init__\u001b[39m\u001b[34m(self, start_server, endpoint, timeout, threads, annotators, pretokenized, output_format, properties, stdout, stderr, memory, be_quiet, max_char_length, preload, classpath, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m port = \u001b[38;5;28mint\u001b[39m(port)\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m host == \u001b[33m\"\u001b[39m\u001b[33mlocalhost\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIf starting a server, endpoint must be localhost\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m classpath = \u001b[43mresolve_classpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasspath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m start_cmd = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mjava -Xmx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -cp \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasspath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m  edu.stanford.nlp.pipeline.StanfordCoreNLPServer \u001b[39m\u001b[33m\"\u001b[39m \\\n\u001b[32m    307\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m-port \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -timeout \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -threads \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreads\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -maxCharLength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_char_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m \\\n\u001b[32m    308\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m-quiet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbe_quiet\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28mself\u001b[39m.server_classpath = classpath\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/stanza/server/client.py:240\u001b[39m, in \u001b[36mresolve_classpath\u001b[39m\u001b[34m(classpath)\u001b[39m\n\u001b[32m    237\u001b[39m     classpath = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mCORENLP_HOME\u001b[39m\u001b[33m\"\u001b[39m, os.path.join(\u001b[38;5;28mstr\u001b[39m(Path.home()), \u001b[33m'\u001b[39m\u001b[33mstanza_corenlp\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(classpath):\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPlease install CoreNLP by running `stanza.install_corenlp()`. If you have installed it, please define \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    241\u001b[39m                                 \u001b[33m\"\u001b[39m\u001b[33m$CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter.  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m                                 \u001b[33m\"\u001b[39m\u001b[33m$CORENLP_HOME=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mCORENLP_HOME\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m    243\u001b[39m     classpath = os.path.join(classpath, \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m classpath\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Please install CoreNLP by running `stanza.install_corenlp()`. If you have installed it, please define $CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter.  $CORENLP_HOME=None"
     ]
    }
   ],
   "source": [
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "client = CoreNLPClient(\n",
    "            annotators=['openie'],\n",
    "            endpoint='http://localhost:9000',\n",
    "            start_server=True,\n",
    "            be_quiet=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdc64d6-b5d1-48c5-a69e-904d16617bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = joblib.load('data/triples_raw/bbc/bbc_semantic_tripletas_simplificado-contopic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255b7f3e-ff8a-467a-8989-e252b655a95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>type</th>\n",
       "      <th>text_coref</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>entidades_raw</th>\n",
       "      <th>entidades_dbpedia</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>topic_dist</th>\n",
       "      <th>topic</th>\n",
       "      <th>tripletas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Microsoft launches its own search\n",
       " \n",
       " Microsoft...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft launches its own search\n",
       " \n",
       " Microsoft...</td>\n",
       "      <td>microsoft launches its own search\n",
       " \n",
       " microsoft...</td>\n",
       "      <td>{'search engine': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>{'search engine': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>microsoft launches its own search\n",
       " \n",
       " microsoft...</td>\n",
       "      <td>['microsoft', 'launch', 'search', 'microsoft',...</td>\n",
       "      <td>[(5, 0.06285988), (10, 0.93661404)]</td>\n",
       "      <td>10</td>\n",
       "      <td>[{'subject': 'Microsoft', 'relation': 'launche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Warnings about junk mail deluge\n",
       " \n",
       " The amount ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Warnings about junk mail deluge\n",
       " \n",
       " The amount ...</td>\n",
       "      <td>warnings about junk mail deluge\n",
       " \n",
       " the amount ...</td>\n",
       "      <td>{'virus': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'virus': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>warnings about junk mail deluge\n",
       " \n",
       " the amount ...</td>\n",
       "      <td>['warning', 'junk', 'mail', 'deluge', 'spam', ...</td>\n",
       "      <td>[(4, 0.8922938), (6, 0.048490256), (10, 0.0587...</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'subject': 'massive increase', 'relation': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Microsoft gets the blogging bug\n",
       " \n",
       " Software gi...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft gets the blogging bug\n",
       " \n",
       " Software gi...</td>\n",
       "      <td>microsoft gets the blogging bug\n",
       " \n",
       " software gi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>microsoft gets the blogging bug\n",
       " \n",
       " software gi...</td>\n",
       "      <td>['microsoft', 'blogging', 'bug', 'software', '...</td>\n",
       "      <td>[(5, 0.9989484)]</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'subject': 'blogging bug Software', 'relatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Gamers snap up new Sony PSP\n",
       " \n",
       " Gamers have bou...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Gamers snap up new Sony PSP\n",
       " \n",
       " Gamers have bou...</td>\n",
       "      <td>gamers snap up new sony psp\n",
       " \n",
       " gamers have bou...</td>\n",
       "      <td>{'sony': {'URI': 'http://dbpedia.org/resource/...</td>\n",
       "      <td>{'sony': {'URI': 'http://dbpedia.org/resource/...</td>\n",
       "      <td>gamers snap up new sony psp\n",
       " \n",
       " gamers have bou...</td>\n",
       "      <td>['gamer', 'snap', 'new', 'sony', 'psp', 'gamer...</td>\n",
       "      <td>[(0, 0.8536542), (10, 0.14511149)]</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'subject': 'sale', 'relation': 'is in', 'obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Apple laptop is 'greatest gadget'\n",
       " \n",
       " The Apple...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Apple laptop is 'greatest gadget'\n",
       " \n",
       " The Apple...</td>\n",
       "      <td>apple laptop is greatest gadget\n",
       " \n",
       " the apple p...</td>\n",
       "      <td>{'apple': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'apple': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>apple laptop is 'greatest gadget'\n",
       " \n",
       " the apple...</td>\n",
       "      <td>['apple', 'laptop', 'great', 'gadget', 'apple'...</td>\n",
       "      <td>[(0, 0.5367707), (5, 0.27873483), (6, 0.184266...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'subject': 'Apple Powerbook 100', 'relation'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news  type  \\\n",
       "2220  Microsoft launches its own search\n",
       " \n",
       " Microsoft...  tech   \n",
       "2221  Warnings about junk mail deluge\n",
       " \n",
       " The amount ...  tech   \n",
       "2222  Microsoft gets the blogging bug\n",
       " \n",
       " Software gi...  tech   \n",
       "2223  Gamers snap up new Sony PSP\n",
       " \n",
       " Gamers have bou...  tech   \n",
       "2224  Apple laptop is 'greatest gadget'\n",
       " \n",
       " The Apple...  tech   \n",
       "\n",
       "                                             text_coref  \\\n",
       "2220  Microsoft launches its own search\n",
       " \n",
       " Microsoft...   \n",
       "2221  Warnings about junk mail deluge\n",
       " \n",
       " The amount ...   \n",
       "2222  Microsoft gets the blogging bug\n",
       " \n",
       " Software gi...   \n",
       "2223  Gamers snap up new Sony PSP\n",
       " \n",
       " Gamers have bou...   \n",
       "2224  Apple laptop is 'greatest gadget'\n",
       " \n",
       " The Apple...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "2220  microsoft launches its own search\n",
       " \n",
       " microsoft...   \n",
       "2221  warnings about junk mail deluge\n",
       " \n",
       " the amount ...   \n",
       "2222  microsoft gets the blogging bug\n",
       " \n",
       " software gi...   \n",
       "2223  gamers snap up new sony psp\n",
       " \n",
       " gamers have bou...   \n",
       "2224  apple laptop is greatest gadget\n",
       " \n",
       " the apple p...   \n",
       "\n",
       "                                          entidades_raw  \\\n",
       "2220  {'search engine': {'URI': 'http://dbpedia.org/...   \n",
       "2221  {'virus': {'URI': 'http://dbpedia.org/resource...   \n",
       "2222                                                 {}   \n",
       "2223  {'sony': {'URI': 'http://dbpedia.org/resource/...   \n",
       "2224  {'apple': {'URI': 'http://dbpedia.org/resource...   \n",
       "\n",
       "                                      entidades_dbpedia  \\\n",
       "2220  {'search engine': {'URI': 'http://dbpedia.org/...   \n",
       "2221  {'virus': {'URI': 'http://dbpedia.org/resource...   \n",
       "2222                                                 {}   \n",
       "2223  {'sony': {'URI': 'http://dbpedia.org/resource/...   \n",
       "2224  {'apple': {'URI': 'http://dbpedia.org/resource...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "2220  microsoft launches its own search\n",
       " \n",
       " microsoft...   \n",
       "2221  warnings about junk mail deluge\n",
       " \n",
       " the amount ...   \n",
       "2222  microsoft gets the blogging bug\n",
       " \n",
       " software gi...   \n",
       "2223  gamers snap up new sony psp\n",
       " \n",
       " gamers have bou...   \n",
       "2224  apple laptop is 'greatest gadget'\n",
       " \n",
       " the apple...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "2220  ['microsoft', 'launch', 'search', 'microsoft',...   \n",
       "2221  ['warning', 'junk', 'mail', 'deluge', 'spam', ...   \n",
       "2222  ['microsoft', 'blogging', 'bug', 'software', '...   \n",
       "2223  ['gamer', 'snap', 'new', 'sony', 'psp', 'gamer...   \n",
       "2224  ['apple', 'laptop', 'great', 'gadget', 'apple'...   \n",
       "\n",
       "                                             topic_dist  topic  \\\n",
       "2220                [(5, 0.06285988), (10, 0.93661404)]     10   \n",
       "2221  [(4, 0.8922938), (6, 0.048490256), (10, 0.0587...      4   \n",
       "2222                                   [(5, 0.9989484)]      5   \n",
       "2223                 [(0, 0.8536542), (10, 0.14511149)]      0   \n",
       "2224  [(0, 0.5367707), (5, 0.27873483), (6, 0.184266...      0   \n",
       "\n",
       "                                              tripletas  \n",
       "2220  [{'subject': 'Microsoft', 'relation': 'launche...  \n",
       "2221  [{'subject': 'massive increase', 'relation': '...  \n",
       "2222  [{'subject': 'blogging bug Software', 'relatio...  \n",
       "2223  [{'subject': 'sale', 'relation': 'is in', 'obj...  \n",
       "2224  [{'subject': 'Apple Powerbook 100', 'relation'...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba033005-2bde-41a6-9516-775f3f2716bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfold = joblib.load('data/triples_ft/processed/dataset_final_triplet_bbc_pykeen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafc38b8-f28e-42ee-a788-ea77ac82a2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>new_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands of viewers with home theatre systems ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands will radically different in five year...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>way people</td>\n",
       "      <td>watch</td>\n",
       "      <td>tv</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands of viewers will different in five yea...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands of viewers with home theatre systems ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159557</th>\n",
       "      <td>boss graeme souness</td>\n",
       "      <td>paid</td>\n",
       "      <td>tribute</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159558</th>\n",
       "      <td>boss graeme souness</td>\n",
       "      <td>paid</td>\n",
       "      <td>tribute</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159559</th>\n",
       "      <td>tribute</td>\n",
       "      <td>laurent</td>\n",
       "      <td>united</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159560</th>\n",
       "      <td>please</td>\n",
       "      <td>said</td>\n",
       "      <td>graeme</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159561</th>\n",
       "      <td>please</td>\n",
       "      <td>said</td>\n",
       "      <td>souness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159562 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    subject relation  \\\n",
       "0                 tv future     will   \n",
       "1                 tv future     will   \n",
       "2                way people    watch   \n",
       "3                 tv future     will   \n",
       "4                 tv future     will   \n",
       "...                     ...      ...   \n",
       "159557  boss graeme souness     paid   \n",
       "159558  boss graeme souness     paid   \n",
       "159559              tribute  laurent   \n",
       "159560               please     said   \n",
       "159561               please     said   \n",
       "\n",
       "                                                   object  new_topic  \n",
       "0       in hands of viewers with home theatre systems ...          3  \n",
       "1       in hands will radically different in five year...          3  \n",
       "2                                                      tv          3  \n",
       "3       in hands of viewers will different in five yea...          3  \n",
       "4       in hands of viewers with home theatre systems ...          3  \n",
       "...                                                   ...        ...  \n",
       "159557                                            tribute          2  \n",
       "159558                                            tribute          2  \n",
       "159559                                             united          2  \n",
       "159560                                             graeme          2  \n",
       "159561                                            souness          2  \n",
       "\n",
       "[159562 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2df2e2f-4c0e-4c10-b55b-41ab06e7ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas filtradas: 2225 -> 2225 (0 sin tripletas)\n",
      "ERROR durante la transformación: 'index'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Raul/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'index'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/zy/9n6jmtx94qb6l_vcz_gzs41r0000gn/T/ipykernel_94222/1057270377.py\", line 59, in <module>\n",
      "    triplet_df_kge['old_index'] = triplet_df_def['index']\n",
      "                                  ~~~~~~~~~~~~~~^^^^^^^^^\n",
      "  File \"/Users/Raul/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Raul/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'index'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import traceback\n",
    "def deserializar_tripletas(triplet_str):\n",
    "    \"\"\"Convierte string de tripletas a lista real\"\"\"\n",
    "    if pd.isna(triplet_str) or triplet_str == '' or triplet_str == '[]':\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Intentar eval (cuidado: solo usar con datos confiables)\n",
    "        import ast\n",
    "        return ast.literal_eval(triplet_str)\n",
    "    except:\n",
    "        try:\n",
    "            # Alternativa con eval si ast falla\n",
    "            return eval(triplet_str)\n",
    "        except:\n",
    "            print(f\"ADVERTENCIA: No se pudo deserializar: {triplet_str[:100]}...\")\n",
    "            return []\n",
    "\n",
    "# Aplicar deserialización\n",
    "\n",
    "include_topics = True\n",
    "\n",
    "try:\n",
    "    # Eliminar filas sin tripletas\n",
    "    original_len = len(df)\n",
    "    df_filtered = df[df['tripletas'].map(len) > 0].copy()\n",
    "    filtered_len = len(df_filtered)\n",
    "    print(f\"Filas filtradas: {original_len} -> {filtered_len} ({original_len - filtered_len} sin tripletas)\")\n",
    "\n",
    "    if filtered_len == 0:\n",
    "        print(\"ADVERTENCIA: No hay tripletas para procesar\")\n",
    "\n",
    "        \n",
    "\n",
    "    # Explotar tripletas\n",
    "\n",
    "    #triplet_df_def = df_filtered.explode('tripletas')\n",
    "\n",
    "    #triplet_df_def = triplet_df_def.reset_index()\n",
    "    df_filtered['tripletas'] = df_filtered['tripletas'].apply(deserializar_tripletas)\n",
    "\n",
    "    # Explotar cuidadosamente\n",
    "    triplet_df_def = df_filtered.explode('tripletas', ignore_index=True)\n",
    "\n",
    "    # Filtrar posibles None resultantes de la explosión\n",
    "    triplet_df_def = triplet_df_def[triplet_df_def['tripletas'].notna()]\n",
    "    triplet_df_kge = pd.DataFrame(triplet_df_def['tripletas'].tolist(), index=triplet_df_def.index)\n",
    "\n",
    "    # Añadir metadatos - buscar diferentes nombres de columna para tópicos\n",
    "    topic_col_found = None\n",
    "    if include_topics:\n",
    "        for topic_col in ['topic', 'new_target', 'target']:\n",
    "            if topic_col in triplet_df_def.columns:\n",
    "                triplet_df_kge['new_topic'] = triplet_df_def[topic_col]\n",
    "                topic_col_found = topic_col\n",
    "                break\n",
    "\n",
    "    triplet_df_kge['old_index'] = triplet_df_def['index']\n",
    "\n",
    "    # Configurar nombres de columnas\n",
    "    if include_topics and topic_col_found:\n",
    "        triplet_df_kge.columns = ['subject', 'relation', 'object', 'new_topic', 'old_index']\n",
    "    else:\n",
    "        triplet_df_kge.columns = ['subject', 'relation', 'object', 'old_index']\n",
    "\n",
    "    # Guardar CSV\n",
    "    csv_path = os.path.join(output_dir, csv_filename)\n",
    "    triplet_df_kge.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"Transformación completada!\")\n",
    "    print(f\"Total de tripletas generadas: {len(triplet_df_kge)}\")\n",
    "    print(f\"Archivo CSV guardado en: {csv_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR durante la transformación: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "855339e5-cc74-4df2-ba11-00cfc406399e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_filtered['tripletas'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8bd7ed0-d886-4795-9b34-860fc2399783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval(df_filtered['tripletas'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "052c6042-29aa-46e0-8dc9-e3e948dee598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43aabb6c-f52e-4b76-9f14-9551edb26ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfantiguo1 = joblib.load('data/triples_ft/processed/dataset_final_triplet_bbc_pykeen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4be67b72-64a6-43b8-a33b-97e3715dfa1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>new_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands of viewers with home theatre systems ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands will radically different in five year...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>way people</td>\n",
       "      <td>watch</td>\n",
       "      <td>tv</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands of viewers will different in five yea...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands of viewers with home theatre systems ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159557</th>\n",
       "      <td>boss graeme souness</td>\n",
       "      <td>paid</td>\n",
       "      <td>tribute</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159558</th>\n",
       "      <td>boss graeme souness</td>\n",
       "      <td>paid</td>\n",
       "      <td>tribute</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159559</th>\n",
       "      <td>tribute</td>\n",
       "      <td>laurent</td>\n",
       "      <td>united</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159560</th>\n",
       "      <td>please</td>\n",
       "      <td>said</td>\n",
       "      <td>graeme</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159561</th>\n",
       "      <td>please</td>\n",
       "      <td>said</td>\n",
       "      <td>souness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159562 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    subject relation  \\\n",
       "0                 tv future     will   \n",
       "1                 tv future     will   \n",
       "2                way people    watch   \n",
       "3                 tv future     will   \n",
       "4                 tv future     will   \n",
       "...                     ...      ...   \n",
       "159557  boss graeme souness     paid   \n",
       "159558  boss graeme souness     paid   \n",
       "159559              tribute  laurent   \n",
       "159560               please     said   \n",
       "159561               please     said   \n",
       "\n",
       "                                                   object  new_topic  \n",
       "0       in hands of viewers with home theatre systems ...          3  \n",
       "1       in hands will radically different in five year...          3  \n",
       "2                                                      tv          3  \n",
       "3       in hands of viewers will different in five yea...          3  \n",
       "4       in hands of viewers with home theatre systems ...          3  \n",
       "...                                                   ...        ...  \n",
       "159557                                            tribute          2  \n",
       "159558                                            tribute          2  \n",
       "159559                                             united          2  \n",
       "159560                                             graeme          2  \n",
       "159561                                            souness          2  \n",
       "\n",
       "[159562 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfantiguo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc293d-5308-4ab5-8ad4-c6a6defb2122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08dc5775-c276-43d0-84ba-ecf92462e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fc667-756a-4437-b4ae-16e517a33ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90abe20f-08fc-4d01-8004-67b92dda82c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dfantiguocsv = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/triples_raw/bbc/bbc_semantic_tripletas_simplificado-contopic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "dfantiguocsv = pd.read_csv('data/triples_raw/bbc/bbc_semantic_tripletas_simplificado-contopic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "493560fe-a92c-46e4-82d7-010de3024cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>new_topic</th>\n",
       "      <th>old_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$ 6bn</td>\n",
       "      <td>role in</td>\n",
       "      <td>Yukos China</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>role</td>\n",
       "      <td>is in</td>\n",
       "      <td>Yukos up China</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>role</td>\n",
       "      <td>lent</td>\n",
       "      <td>$ 6bn</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$ 6bn</td>\n",
       "      <td>role in</td>\n",
       "      <td>Yukos up China</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>role</td>\n",
       "      <td>lent</td>\n",
       "      <td>Russia</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266240</th>\n",
       "      <td>They</td>\n",
       "      <td>'re</td>\n",
       "      <td>invariably inaccurate</td>\n",
       "      <td>0</td>\n",
       "      <td>266240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266241</th>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>They</td>\n",
       "      <td>0</td>\n",
       "      <td>266241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266242</th>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>inaccurate</td>\n",
       "      <td>0</td>\n",
       "      <td>266242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266243</th>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>uncomfortable</td>\n",
       "      <td>0</td>\n",
       "      <td>266243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266244</th>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>cumbersome</td>\n",
       "      <td>0</td>\n",
       "      <td>266244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266245 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject relation                 object  new_topic  old_index\n",
       "0        $ 6bn  role in            Yukos China         10          0\n",
       "1         role    is in         Yukos up China         10          1\n",
       "2         role     lent                  $ 6bn         10          2\n",
       "3        $ 6bn  role in         Yukos up China         10          3\n",
       "4         role     lent                 Russia         10          4\n",
       "...        ...      ...                    ...        ...        ...\n",
       "266240    They      're  invariably inaccurate          0     266240\n",
       "266241     use      use                   They          0     266241\n",
       "266242     use      use             inaccurate          0     266242\n",
       "266243     use      use          uncomfortable          0     266243\n",
       "266244     use      use             cumbersome          0     266244\n",
       "\n",
       "[266245 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfantiguocsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59822a03-af54-4552-8dfd-a8fd944f342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfantiguo1 = joblib.load('data/triples_ft/processed/dataset_final_triplet_bbc_pykeen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f25e878-5605-4b96-ac47-2ce19fc076a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnuevo = joblib.load('data/triples_raw/bbc/bbc_semantic_tripletas_simplificado-contopic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed9626d2-1ef4-4be5-970b-aafc3572a8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>new_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands of viewers with home theatre systems ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands will radically different in five year...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>way people</td>\n",
       "      <td>watch</td>\n",
       "      <td>tv</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands of viewers will different in five yea...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tv future</td>\n",
       "      <td>will</td>\n",
       "      <td>in hands of viewers with home theatre systems ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159557</th>\n",
       "      <td>boss graeme souness</td>\n",
       "      <td>paid</td>\n",
       "      <td>tribute</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159558</th>\n",
       "      <td>boss graeme souness</td>\n",
       "      <td>paid</td>\n",
       "      <td>tribute</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159559</th>\n",
       "      <td>tribute</td>\n",
       "      <td>laurent</td>\n",
       "      <td>united</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159560</th>\n",
       "      <td>please</td>\n",
       "      <td>said</td>\n",
       "      <td>graeme</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159561</th>\n",
       "      <td>please</td>\n",
       "      <td>said</td>\n",
       "      <td>souness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159562 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    subject relation  \\\n",
       "0                 tv future     will   \n",
       "1                 tv future     will   \n",
       "2                way people    watch   \n",
       "3                 tv future     will   \n",
       "4                 tv future     will   \n",
       "...                     ...      ...   \n",
       "159557  boss graeme souness     paid   \n",
       "159558  boss graeme souness     paid   \n",
       "159559              tribute  laurent   \n",
       "159560               please     said   \n",
       "159561               please     said   \n",
       "\n",
       "                                                   object  new_topic  \n",
       "0       in hands of viewers with home theatre systems ...          3  \n",
       "1       in hands will radically different in five year...          3  \n",
       "2                                                      tv          3  \n",
       "3       in hands of viewers will different in five yea...          3  \n",
       "4       in hands of viewers with home theatre systems ...          3  \n",
       "...                                                   ...        ...  \n",
       "159557                                            tribute          2  \n",
       "159558                                            tribute          2  \n",
       "159559                                             united          2  \n",
       "159560                                             graeme          2  \n",
       "159561                                            souness          2  \n",
       "\n",
       "[159562 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfantiguo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "037a38da-e260-4479-81d7-ce80feb2eba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>type</th>\n",
       "      <th>text_coref</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>entidades_raw</th>\n",
       "      <th>entidades_dbpedia</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>topic_dist</th>\n",
       "      <th>topic</th>\n",
       "      <th>tripletas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China had role in Yukos split-up\n",
       " \n",
       " China lent...</td>\n",
       "      <td>business</td>\n",
       "      <td>China had role in Yukos split-up\n",
       " \n",
       " China lent...</td>\n",
       "      <td>china had role in yukos splitup\n",
       " \n",
       " china lent ...</td>\n",
       "      <td>{'yukos': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'yukos': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>china had role in yukos split-up\n",
       " \n",
       " china lent...</td>\n",
       "      <td>['china', 'role', 'yukos', 'split', 'china', '...</td>\n",
       "      <td>[(2, 0.1521518), (4, 0.026116), (5, 0.10821417...</td>\n",
       "      <td>10</td>\n",
       "      <td>[{'subject': '$ 6bn', 'relation': 'role in', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oil rebounds from weather effect\n",
       " \n",
       " Oil prices...</td>\n",
       "      <td>business</td>\n",
       "      <td>Oil rebounds from weather effect\n",
       " \n",
       " Oil prices...</td>\n",
       "      <td>oil rebounds from weather effect\n",
       " \n",
       " oil prices...</td>\n",
       "      <td>{'asian': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'asian': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>oil rebounds from weather effect\n",
       " \n",
       " oil prices...</td>\n",
       "      <td>['oil', 'rebound', 'weather', 'effect', 'oil',...</td>\n",
       "      <td>[(1, 0.33736482), (2, 0.14471534), (5, 0.24712...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'subject': 'Oil', 'relation': 'rebounds from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indonesia 'declines debt freeze'\n",
       " \n",
       " Indonesia ...</td>\n",
       "      <td>business</td>\n",
       "      <td>Indonesia 'declines debt freeze'\n",
       " \n",
       " Indonesia ...</td>\n",
       "      <td>indonesia declines debt freeze\n",
       " \n",
       " indonesia no...</td>\n",
       "      <td>{'aceh province': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>{'aceh province': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>indonesia 'declines debt freeze'\n",
       " \n",
       " indonesia ...</td>\n",
       "      <td>['indonesia', 'decline', 'debt', 'freeze', 'in...</td>\n",
       "      <td>[(2, 0.33151984), (3, 0.070257254), (7, 0.3591...</td>\n",
       "      <td>7</td>\n",
       "      <td>[{'subject': 'Indonesia', 'relation': 'longer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1m payoff for former Shell boss\n",
       " \n",
       " Shell is t...</td>\n",
       "      <td>business</td>\n",
       "      <td>$1m payoff for former Shell boss\n",
       " \n",
       " Shell is t...</td>\n",
       "      <td>$1m payoff for former shell boss\n",
       " \n",
       " shell is t...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>$1m payoff for former shell boss\n",
       " \n",
       " shell is t...</td>\n",
       "      <td>['payoff', 'shell', 'boss', 'shell', 'pay', 'e...</td>\n",
       "      <td>[(0, 0.076440915), (1, 0.08356439), (3, 0.1388...</td>\n",
       "      <td>10</td>\n",
       "      <td>[{'subject': 'her post', 'relation': 'is in', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US bank in $515m SEC settlement\n",
       " \n",
       " Five Bank o...</td>\n",
       "      <td>business</td>\n",
       "      <td>US bank in $515m SEC settlement\n",
       " \n",
       " Five Bank o...</td>\n",
       "      <td>us bank in $515m sec settlement\n",
       " \n",
       " five bank o...</td>\n",
       "      <td>{'mutual fund': {'URI': 'http://dbpedia.org/re...</td>\n",
       "      <td>{'mutual fund': {'URI': 'http://dbpedia.org/re...</td>\n",
       "      <td>us bank in $515m sec settlement\n",
       " \n",
       " five bank o...</td>\n",
       "      <td>['bank', 'sec', 'settlement', 'bank', 'america...</td>\n",
       "      <td>[(1, 0.08517497), (4, 0.19468576), (10, 0.4840...</td>\n",
       "      <td>10</td>\n",
       "      <td>[{'subject': 'US bank', 'relation': 'is in', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Microsoft launches its own search\n",
       " \n",
       " Microsoft...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft launches its own search\n",
       " \n",
       " Microsoft...</td>\n",
       "      <td>microsoft launches its own search\n",
       " \n",
       " microsoft...</td>\n",
       "      <td>{'search engine': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>{'search engine': {'URI': 'http://dbpedia.org/...</td>\n",
       "      <td>microsoft launches its own search\n",
       " \n",
       " microsoft...</td>\n",
       "      <td>['microsoft', 'launch', 'search', 'microsoft',...</td>\n",
       "      <td>[(5, 0.06285988), (10, 0.93661404)]</td>\n",
       "      <td>10</td>\n",
       "      <td>[{'subject': 'Microsoft', 'relation': 'launche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Warnings about junk mail deluge\n",
       " \n",
       " The amount ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Warnings about junk mail deluge\n",
       " \n",
       " The amount ...</td>\n",
       "      <td>warnings about junk mail deluge\n",
       " \n",
       " the amount ...</td>\n",
       "      <td>{'virus': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'virus': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>warnings about junk mail deluge\n",
       " \n",
       " the amount ...</td>\n",
       "      <td>['warning', 'junk', 'mail', 'deluge', 'spam', ...</td>\n",
       "      <td>[(4, 0.8922938), (6, 0.048490256), (10, 0.0587...</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'subject': 'Anti-spam group Spamhaus', 'rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Microsoft gets the blogging bug\n",
       " \n",
       " Software gi...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft gets the blogging bug\n",
       " \n",
       " Software gi...</td>\n",
       "      <td>microsoft gets the blogging bug\n",
       " \n",
       " software gi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>microsoft gets the blogging bug\n",
       " \n",
       " software gi...</td>\n",
       "      <td>['microsoft', 'blogging', 'bug', 'software', '...</td>\n",
       "      <td>[(5, 0.9989484)]</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'subject': 'Microsoft', 'relation': 'gets', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Gamers snap up new Sony PSP\n",
       " \n",
       " Gamers have bou...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Gamers snap up new Sony PSP\n",
       " \n",
       " Gamers have bou...</td>\n",
       "      <td>gamers snap up new sony psp\n",
       " \n",
       " gamers have bou...</td>\n",
       "      <td>{'sony': {'URI': 'http://dbpedia.org/resource/...</td>\n",
       "      <td>{'sony': {'URI': 'http://dbpedia.org/resource/...</td>\n",
       "      <td>gamers snap up new sony psp\n",
       " \n",
       " gamers have bou...</td>\n",
       "      <td>['gamer', 'snap', 'new', 'sony', 'psp', 'gamer...</td>\n",
       "      <td>[(0, 0.8536542), (10, 0.14511149)]</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'subject': 'sale', 'relation': 'is in', 'obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Apple laptop is 'greatest gadget'\n",
       " \n",
       " The Apple...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Apple laptop is 'greatest gadget'\n",
       " \n",
       " The Apple...</td>\n",
       "      <td>apple laptop is greatest gadget\n",
       " \n",
       " the apple p...</td>\n",
       "      <td>{'apple': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>{'apple': {'URI': 'http://dbpedia.org/resource...</td>\n",
       "      <td>apple laptop is 'greatest gadget'\n",
       " \n",
       " the apple...</td>\n",
       "      <td>['apple', 'laptop', 'great', 'gadget', 'apple'...</td>\n",
       "      <td>[(0, 0.5367707), (5, 0.27873483), (6, 0.184266...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'subject': 'Apple Powerbook 100', 'relation'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news      type  \\\n",
       "0     China had role in Yukos split-up\n",
       " \n",
       " China lent...  business   \n",
       "1     Oil rebounds from weather effect\n",
       " \n",
       " Oil prices...  business   \n",
       "2     Indonesia 'declines debt freeze'\n",
       " \n",
       " Indonesia ...  business   \n",
       "3     $1m payoff for former Shell boss\n",
       " \n",
       " Shell is t...  business   \n",
       "4     US bank in $515m SEC settlement\n",
       " \n",
       " Five Bank o...  business   \n",
       "...                                                 ...       ...   \n",
       "2220  Microsoft launches its own search\n",
       " \n",
       " Microsoft...      tech   \n",
       "2221  Warnings about junk mail deluge\n",
       " \n",
       " The amount ...      tech   \n",
       "2222  Microsoft gets the blogging bug\n",
       " \n",
       " Software gi...      tech   \n",
       "2223  Gamers snap up new Sony PSP\n",
       " \n",
       " Gamers have bou...      tech   \n",
       "2224  Apple laptop is 'greatest gadget'\n",
       " \n",
       " The Apple...      tech   \n",
       "\n",
       "                                             text_coref  \\\n",
       "0     China had role in Yukos split-up\n",
       " \n",
       " China lent...   \n",
       "1     Oil rebounds from weather effect\n",
       " \n",
       " Oil prices...   \n",
       "2     Indonesia 'declines debt freeze'\n",
       " \n",
       " Indonesia ...   \n",
       "3     $1m payoff for former Shell boss\n",
       " \n",
       " Shell is t...   \n",
       "4     US bank in $515m SEC settlement\n",
       " \n",
       " Five Bank o...   \n",
       "...                                                 ...   \n",
       "2220  Microsoft launches its own search\n",
       " \n",
       " Microsoft...   \n",
       "2221  Warnings about junk mail deluge\n",
       " \n",
       " The amount ...   \n",
       "2222  Microsoft gets the blogging bug\n",
       " \n",
       " Software gi...   \n",
       "2223  Gamers snap up new Sony PSP\n",
       " \n",
       " Gamers have bou...   \n",
       "2224  Apple laptop is 'greatest gadget'\n",
       " \n",
       " The Apple...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "0     china had role in yukos splitup\n",
       " \n",
       " china lent ...   \n",
       "1     oil rebounds from weather effect\n",
       " \n",
       " oil prices...   \n",
       "2     indonesia declines debt freeze\n",
       " \n",
       " indonesia no...   \n",
       "3     $1m payoff for former shell boss\n",
       " \n",
       " shell is t...   \n",
       "4     us bank in $515m sec settlement\n",
       " \n",
       " five bank o...   \n",
       "...                                                 ...   \n",
       "2220  microsoft launches its own search\n",
       " \n",
       " microsoft...   \n",
       "2221  warnings about junk mail deluge\n",
       " \n",
       " the amount ...   \n",
       "2222  microsoft gets the blogging bug\n",
       " \n",
       " software gi...   \n",
       "2223  gamers snap up new sony psp\n",
       " \n",
       " gamers have bou...   \n",
       "2224  apple laptop is greatest gadget\n",
       " \n",
       " the apple p...   \n",
       "\n",
       "                                          entidades_raw  \\\n",
       "0     {'yukos': {'URI': 'http://dbpedia.org/resource...   \n",
       "1     {'asian': {'URI': 'http://dbpedia.org/resource...   \n",
       "2     {'aceh province': {'URI': 'http://dbpedia.org/...   \n",
       "3                                                    {}   \n",
       "4     {'mutual fund': {'URI': 'http://dbpedia.org/re...   \n",
       "...                                                 ...   \n",
       "2220  {'search engine': {'URI': 'http://dbpedia.org/...   \n",
       "2221  {'virus': {'URI': 'http://dbpedia.org/resource...   \n",
       "2222                                                 {}   \n",
       "2223  {'sony': {'URI': 'http://dbpedia.org/resource/...   \n",
       "2224  {'apple': {'URI': 'http://dbpedia.org/resource...   \n",
       "\n",
       "                                      entidades_dbpedia  \\\n",
       "0     {'yukos': {'URI': 'http://dbpedia.org/resource...   \n",
       "1     {'asian': {'URI': 'http://dbpedia.org/resource...   \n",
       "2     {'aceh province': {'URI': 'http://dbpedia.org/...   \n",
       "3                                                    {}   \n",
       "4     {'mutual fund': {'URI': 'http://dbpedia.org/re...   \n",
       "...                                                 ...   \n",
       "2220  {'search engine': {'URI': 'http://dbpedia.org/...   \n",
       "2221  {'virus': {'URI': 'http://dbpedia.org/resource...   \n",
       "2222                                                 {}   \n",
       "2223  {'sony': {'URI': 'http://dbpedia.org/resource/...   \n",
       "2224  {'apple': {'URI': 'http://dbpedia.org/resource...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0     china had role in yukos split-up\n",
       " \n",
       " china lent...   \n",
       "1     oil rebounds from weather effect\n",
       " \n",
       " oil prices...   \n",
       "2     indonesia 'declines debt freeze'\n",
       " \n",
       " indonesia ...   \n",
       "3     $1m payoff for former shell boss\n",
       " \n",
       " shell is t...   \n",
       "4     us bank in $515m sec settlement\n",
       " \n",
       " five bank o...   \n",
       "...                                                 ...   \n",
       "2220  microsoft launches its own search\n",
       " \n",
       " microsoft...   \n",
       "2221  warnings about junk mail deluge\n",
       " \n",
       " the amount ...   \n",
       "2222  microsoft gets the blogging bug\n",
       " \n",
       " software gi...   \n",
       "2223  gamers snap up new sony psp\n",
       " \n",
       " gamers have bou...   \n",
       "2224  apple laptop is 'greatest gadget'\n",
       " \n",
       " the apple...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     ['china', 'role', 'yukos', 'split', 'china', '...   \n",
       "1     ['oil', 'rebound', 'weather', 'effect', 'oil',...   \n",
       "2     ['indonesia', 'decline', 'debt', 'freeze', 'in...   \n",
       "3     ['payoff', 'shell', 'boss', 'shell', 'pay', 'e...   \n",
       "4     ['bank', 'sec', 'settlement', 'bank', 'america...   \n",
       "...                                                 ...   \n",
       "2220  ['microsoft', 'launch', 'search', 'microsoft',...   \n",
       "2221  ['warning', 'junk', 'mail', 'deluge', 'spam', ...   \n",
       "2222  ['microsoft', 'blogging', 'bug', 'software', '...   \n",
       "2223  ['gamer', 'snap', 'new', 'sony', 'psp', 'gamer...   \n",
       "2224  ['apple', 'laptop', 'great', 'gadget', 'apple'...   \n",
       "\n",
       "                                             topic_dist  topic  \\\n",
       "0     [(2, 0.1521518), (4, 0.026116), (5, 0.10821417...     10   \n",
       "1     [(1, 0.33736482), (2, 0.14471534), (5, 0.24712...      1   \n",
       "2     [(2, 0.33151984), (3, 0.070257254), (7, 0.3591...      7   \n",
       "3     [(0, 0.076440915), (1, 0.08356439), (3, 0.1388...     10   \n",
       "4     [(1, 0.08517497), (4, 0.19468576), (10, 0.4840...     10   \n",
       "...                                                 ...    ...   \n",
       "2220                [(5, 0.06285988), (10, 0.93661404)]     10   \n",
       "2221  [(4, 0.8922938), (6, 0.048490256), (10, 0.0587...      4   \n",
       "2222                                   [(5, 0.9989484)]      5   \n",
       "2223                 [(0, 0.8536542), (10, 0.14511149)]      0   \n",
       "2224  [(0, 0.5367707), (5, 0.27873483), (6, 0.184266...      0   \n",
       "\n",
       "                                              tripletas  \n",
       "0     [{'subject': '$ 6bn', 'relation': 'role in', '...  \n",
       "1     [{'subject': 'Oil', 'relation': 'rebounds from...  \n",
       "2     [{'subject': 'Indonesia', 'relation': 'longer ...  \n",
       "3     [{'subject': 'her post', 'relation': 'is in', ...  \n",
       "4     [{'subject': 'US bank', 'relation': 'is in', '...  \n",
       "...                                                 ...  \n",
       "2220  [{'subject': 'Microsoft', 'relation': 'launche...  \n",
       "2221  [{'subject': 'Anti-spam group Spamhaus', 'rela...  \n",
       "2222  [{'subject': 'Microsoft', 'relation': 'gets', ...  \n",
       "2223  [{'subject': 'sale', 'relation': 'is in', 'obj...  \n",
       "2224  [{'subject': 'Apple Powerbook 100', 'relation'...  \n",
       "\n",
       "[2225 rows x 11 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37d79132-6735-4e6c-83d8-968f9a2e3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/triples_raw/bbc/dataset_triplet_bbc_new_simplificado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62121e6a-85f9-45f7-b85e-b3ecdb31e76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>new_topic</th>\n",
       "      <th>old_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$ 6bn</td>\n",
       "      <td>role in</td>\n",
       "      <td>Yukos China</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>role</td>\n",
       "      <td>is in</td>\n",
       "      <td>Yukos up China</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>role</td>\n",
       "      <td>lent</td>\n",
       "      <td>$ 6bn</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$ 6bn</td>\n",
       "      <td>role in</td>\n",
       "      <td>Yukos up China</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>role</td>\n",
       "      <td>lent</td>\n",
       "      <td>Russia</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266240</th>\n",
       "      <td>They</td>\n",
       "      <td>'re</td>\n",
       "      <td>invariably inaccurate</td>\n",
       "      <td>0</td>\n",
       "      <td>266240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266241</th>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>They</td>\n",
       "      <td>0</td>\n",
       "      <td>266241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266242</th>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>inaccurate</td>\n",
       "      <td>0</td>\n",
       "      <td>266242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266243</th>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>uncomfortable</td>\n",
       "      <td>0</td>\n",
       "      <td>266243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266244</th>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>cumbersome</td>\n",
       "      <td>0</td>\n",
       "      <td>266244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266245 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject relation                 object  new_topic  old_index\n",
       "0        $ 6bn  role in            Yukos China         10          0\n",
       "1         role    is in         Yukos up China         10          1\n",
       "2         role     lent                  $ 6bn         10          2\n",
       "3        $ 6bn  role in         Yukos up China         10          3\n",
       "4         role     lent                 Russia         10          4\n",
       "...        ...      ...                    ...        ...        ...\n",
       "266240    They      're  invariably inaccurate          0     266240\n",
       "266241     use      use                   They          0     266241\n",
       "266242     use      use             inaccurate          0     266242\n",
       "266243     use      use          uncomfortable          0     266243\n",
       "266244     use      use             cumbersome          0     266244\n",
       "\n",
       "[266245 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "526c04a2-2855-4916-97b6-e7a29274e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df23 = joblib.load('data/processed/bbc/bbc_processed_semantic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75b44728-f8e0-4119-b476-01ab86645fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>type</th>\n",
       "      <th>text_coref</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>entidades</th>\n",
       "      <th>entidades_raw</th>\n",
       "      <th>entidades_dbpedia_simplificadas</th>\n",
       "      <th>entidades_dbpedia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "      <td>business</td>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "      <td>china had role in yukos splitup\\n \\n china len...</td>\n",
       "      <td>{yuganskneftegas, sergei oganesyan, 6bn, five ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "      <td>business</td>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "      <td>oil rebounds from weather effect\\n \\n oil pric...</td>\n",
       "      <td>{asian, almost 30%, monday, riyadh, the far ea...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "      <td>business</td>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "      <td>indonesia declines debt freeze\\n \\n indonesia ...</td>\n",
       "      <td>{ap, 3.4bn, $500m, 1.7bn, indonesian, alwi shi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "      <td>business</td>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "      <td>$1m payoff for former shell boss\\n \\n shell is...</td>\n",
       "      <td>{$1m, phil watts, â£82.7m, 31 december, april ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
       "      <td>business</td>\n",
       "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
       "      <td>us bank in $515m sec settlement\\n \\n five bank...</td>\n",
       "      <td>{robert hussey, james tambone, joseph palombo,...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Microsoft launches its own search\\n \\n Microso...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft launches its own search\\n \\n Microso...</td>\n",
       "      <td>microsoft launches its own search\\n \\n microso...</td>\n",
       "      <td>{amazon, april 2003, blinkx, yahoo, encarta, n...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Warnings about junk mail deluge\\n \\n The amoun...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Warnings about junk mail deluge\\n \\n The amoun...</td>\n",
       "      <td>warnings about junk mail deluge\\n \\n the amoun...</td>\n",
       "      <td>{uk, kevin hogan, symantec, david stanley, bri...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Microsoft gets the blogging bug\\n \\n Software ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft gets the blogging bug\\n \\n Software ...</td>\n",
       "      <td>microsoft gets the blogging bug\\n \\n software ...</td>\n",
       "      <td>{the last 18 months, google, msn, microsoft, t...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Gamers snap up new Sony PSP\\n \\n Gamers have b...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Gamers snap up new Sony PSP\\n \\n Gamers have b...</td>\n",
       "      <td>gamers snap up new sony psp\\n \\n gamers have b...</td>\n",
       "      <td>{europe, us, next year, playstation portable, ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n \\n The App...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n \\n The App...</td>\n",
       "      <td>apple laptop is greatest gadget\\n \\n the apple...</td>\n",
       "      <td>{1900, today, karl elsener's, the motorola sta...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news      type  \\\n",
       "0     China had role in Yukos split-up\\n \\n China le...  business   \n",
       "1     Oil rebounds from weather effect\\n \\n Oil pric...  business   \n",
       "2     Indonesia 'declines debt freeze'\\n \\n Indonesi...  business   \n",
       "3     $1m payoff for former Shell boss\\n \\n Shell is...  business   \n",
       "4     US bank in $515m SEC settlement\\n \\n Five Bank...  business   \n",
       "...                                                 ...       ...   \n",
       "2220  Microsoft launches its own search\\n \\n Microso...      tech   \n",
       "2221  Warnings about junk mail deluge\\n \\n The amoun...      tech   \n",
       "2222  Microsoft gets the blogging bug\\n \\n Software ...      tech   \n",
       "2223  Gamers snap up new Sony PSP\\n \\n Gamers have b...      tech   \n",
       "2224  Apple laptop is 'greatest gadget'\\n \\n The App...      tech   \n",
       "\n",
       "                                             text_coref  \\\n",
       "0     China had role in Yukos split-up\\n \\n China le...   \n",
       "1     Oil rebounds from weather effect\\n \\n Oil pric...   \n",
       "2     Indonesia 'declines debt freeze'\\n \\n Indonesi...   \n",
       "3     $1m payoff for former Shell boss\\n \\n Shell is...   \n",
       "4     US bank in $515m SEC settlement\\n \\n Five Bank...   \n",
       "...                                                 ...   \n",
       "2220  Microsoft launches its own search\\n \\n Microso...   \n",
       "2221  Warnings about junk mail deluge\\n \\n The amoun...   \n",
       "2222  Microsoft gets the blogging bug\\n \\n Software ...   \n",
       "2223  Gamers snap up new Sony PSP\\n \\n Gamers have b...   \n",
       "2224  Apple laptop is 'greatest gadget'\\n \\n The App...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "0     china had role in yukos splitup\\n \\n china len...   \n",
       "1     oil rebounds from weather effect\\n \\n oil pric...   \n",
       "2     indonesia declines debt freeze\\n \\n indonesia ...   \n",
       "3     $1m payoff for former shell boss\\n \\n shell is...   \n",
       "4     us bank in $515m sec settlement\\n \\n five bank...   \n",
       "...                                                 ...   \n",
       "2220  microsoft launches its own search\\n \\n microso...   \n",
       "2221  warnings about junk mail deluge\\n \\n the amoun...   \n",
       "2222  microsoft gets the blogging bug\\n \\n software ...   \n",
       "2223  gamers snap up new sony psp\\n \\n gamers have b...   \n",
       "2224  apple laptop is greatest gadget\\n \\n the apple...   \n",
       "\n",
       "                                              entidades entidades_raw  \\\n",
       "0     {yuganskneftegas, sergei oganesyan, 6bn, five ...            {}   \n",
       "1     {asian, almost 30%, monday, riyadh, the far ea...            {}   \n",
       "2     {ap, 3.4bn, $500m, 1.7bn, indonesian, alwi shi...            {}   \n",
       "3     {$1m, phil watts, â£82.7m, 31 december, april ...            {}   \n",
       "4     {robert hussey, james tambone, joseph palombo,...            {}   \n",
       "...                                                 ...           ...   \n",
       "2220  {amazon, april 2003, blinkx, yahoo, encarta, n...            {}   \n",
       "2221  {uk, kevin hogan, symantec, david stanley, bri...            {}   \n",
       "2222  {the last 18 months, google, msn, microsoft, t...            {}   \n",
       "2223  {europe, us, next year, playstation portable, ...            {}   \n",
       "2224  {1900, today, karl elsener's, the motorola sta...            {}   \n",
       "\n",
       "     entidades_dbpedia_simplificadas entidades_dbpedia  \n",
       "0                                 {}                {}  \n",
       "1                                 {}                {}  \n",
       "2                                 {}                {}  \n",
       "3                                 {}                {}  \n",
       "4                                 {}                {}  \n",
       "...                              ...               ...  \n",
       "2220                              {}                {}  \n",
       "2221                              {}                {}  \n",
       "2222                              {}                {}  \n",
       "2223                              {}                {}  \n",
       "2224                              {}                {}  \n",
       "\n",
       "[2225 rows x 8 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e57acb4-763a-4429-9a7e-b0940fe7ffbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2634983504.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m03_triplet_gen genera las ternas para que sean clasificadas por C_classification. Identifica qué cambios en las rutas y en la carga de datos tienes que hacer en  C_classification para que importe ls rutas generadas por el archivo 03_triplet_Gen. No generes de nuevo el archivo, sólo dime dónde tengo que cambiar\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "03_triplet_gen genera las ternas para que sean clasificadas por C_classification. Identifica qué cambios en las rutas y en la carga de datos tienes que hacer en  C_classification para que importe ls rutas generadas por el archivo 03_triplet_Gen. No generes de nuevo el archivo, sólo dime dónde tengo que cambiar\n",
    "Editar\n",
    "Analizando ambos archivos, veo que necesitas hacer varios cambios en las rutas y carga de datos en C_classification_2.py para que sea compatible con los archivos generados por 03_triplet_gen.py.\n",
    "Cambios necesarios en C_classification_2.py:\n",
    "1. Rutas de archivos de entrada\n",
    "En las funciones ejemplo al final del archivo, necesitas cambiar:\n",
    "Cambio principal:\n",
    "python\n",
    "# ANTES (líneas 682-683, 691-692, 704-705):\n",
    "triples_path=\"data/triples_ft/processed/dataset_final_triplet_bbc_pykeen\"\n",
    "\n",
    "# DESPUÉS:\n",
    "triples_path=\"data/triples_raw/bbc/bbc_semantic_tripletas_simplificado-contopic\"\n",
    "2. Ruta del archivo TSV en run_complete_pipeline\n",
    "python\n",
    "# ANTES (línea 706):\n",
    "kg_path=\"data/triples_ft/processed/triples_bbc.tsv\"\n",
    "\n",
    "# DESPUÉS:\n",
    "kg_path=\"data/triples_raw/bbc/dataset_triplet_bbc_new_simplificado.csv\"\n",
    "3. Método de carga de datos\n",
    "En el método load_data() de la clase KGEModelTrainer (línea 90), el archivo que genera 03_triplet_gen.py es un DataFrame con tripletas, no un archivo ya procesado para PyKEEN. Necesitas modificar:\n",
    "python\n",
    "def load_data(self) -> pd.DataFrame:\n",
    "    \"\"\"Load and prepare the dataset for KGE training\"\"\"\n",
    "    print(\"\\nLoading dataset for KGE training...\")\n",
    "    # CAMBIO: Cargar el DataFrame generado por 03_triplet_gen\n",
    "    df = joblib.load(self.triples_path)\n",
    "    \n",
    "    # AÑADIR: Si el DataFrame tiene la columna 'tripletas', necesitas procesarla\n",
    "    if 'tripletas' in df.columns:\n",
    "        # Explotar las tripletas y crear el formato necesario\n",
    "        df_exploded = df.explode('tripletas').reset_index(drop=True)\n",
    "        df_exploded = df_exploded[df_exploded['tripletas'].notna()]\n",
    "        \n",
    "        # Extraer subject, relation, object de las tripletas\n",
    "        tripletas = pd.DataFrame(df_exploded['tripletas'].tolist())\n",
    "        tripletas.columns = ['subject', 'relation', 'object']\n",
    "        \n",
    "        # Añadir la columna de tópicos\n",
    "        if 'topic' in df_exploded.columns:\n",
    "            tripletas['new_topic'] = df_exploded['topic'].values\n",
    "        \n",
    "        df = tripletas\n",
    "4. Método de carga en TopicClassifier\n",
    "Similar cambio en el método load_classification_data() de la clase TopicClassifier (línea 299).\n",
    "5. Rutas de directorios de salida\n",
    "Actualizar las rutas de directorios para que sean consistentes:\n",
    "python\n",
    "# ANTES:\n",
    "emb_dir=\"data/triples_emb/\"\n",
    "output_dir=\"data/output/repo\"\n",
    "\n",
    "# DESPUÉS:\n",
    "emb_dir=\"data/triples_emb/bbc/\"\n",
    "output_dir=\"data/output/bbc\"\n",
    "Archivos específicos que genera 03_triplet_gen.py:\n",
    "DataFrame principal: data/triples_raw/bbc/bbc_semantic_tripletas_simplificado-contopic\n",
    "CSV de tripletas: data/triples_raw/bbc/dataset_triplet_bbc_new_simplificado.csv\n",
    "Resumen de ubicaciones a cambiar:\n",
    "Líneas 682, 691, 704: Cambiar triples_path\n",
    "Línea 706: Cambiar kg_path\n",
    "Líneas 90-100: Modificar método load_data() para procesar el DataFrame de tripletas\n",
    "Líneas 299-310: Modificar método load_classification_data() de manera similar\n",
    "Líneas 680, 689, 702: Actualizar rutas de directorios de salida\n",
    "El cambio más crítico es en los métodos de carga de datos, ya que 03_triplet_gen.py genera un DataFrame con una columna tripletas que contiene listas de tuplas, mientras que C_classification_2.py espera un DataFrame ya procesado con columnas separadas para subject, relation, object y new_topic.\n",
    "\n",
    "\n",
    "\n",
    "KGEModelTrainer load_data teóricamente no debería ser necesario en un Pipeline porque ya tienes el csv no?\n",
    "Editar\n",
    "¡Exacto! Tienes razón. En un pipeline bien diseñado, KGEModelTrainer.load_data() no debería ser necesario porque 03_triplet_gen.py ya genera el CSV con las tripletas procesadas.\n",
    "El flujo lógico debería ser:\n",
    "03_triplet_gen.py → Genera dataset_triplet_bbc_new_simplificado.csv\n",
    "C_classification_2.py → Lee directamente el CSV\n",
    "Cambios mínimos necesarios:\n",
    "1. En prepare_triples_factory() (línea ~105)\n",
    "En lugar de llamar a self.load_data(), cargar directamente el CSV:\n",
    "python\n",
    "def prepare_triples_factory(self) -> Tuple[TriplesFactory, TriplesFactory]:\n",
    "    \"\"\"Prepare triples factory for KGE training\"\"\"\n",
    "    print(\"\\nPreparing triples factory...\")\n",
    "    \n",
    "    # CAMBIO: Cargar directamente el CSV generado por 03_triplet_gen\n",
    "    df = pd.read_csv(self.kg_path)  # Este es el CSV ya procesado\n",
    "    \n",
    "    # El CSV ya tiene las columnas: subject, relation, object, new_topic\n",
    "    triples = df[['subject', 'relation', 'object']].values\n",
    "    \n",
    "    # Resto del código igual...\n",
    "2. En run_kge_training() (línea ~240)\n",
    "Eliminar la llamada a load_data():\n",
    "python\n",
    "def run_kge_training(self) -> str:\n",
    "    \"\"\"Run complete KGE training phase\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STARTING PHASE 1: KGE MODEL TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ELIMINAR: df = self.load_data()\n",
    "    \n",
    "    # Preparar triples (que ahora carga internamente el CSV)\n",
    "    training_tf, testing_tf = self.prepare_triples_factory()\n",
    "    \n",
    "    # Resto igual...\n",
    "3. Cambiar solo el parámetro kg_path\n",
    "python\n",
    "# En las funciones ejemplo:\n",
    "run_complete_pipeline(\n",
    "    triples_path=\"data/triples_raw/bbc/bbc_semantic_tripletas_simplificado-contopic\",  # Para TopicClassifier\n",
    "    kg_path=\"data/triples_raw/bbc/dataset_triplet_bbc_new_simplificado.csv\",  # Para KGEModelTrainer\n",
    "    # ...\n",
    ")\n",
    "Ventajas de este enfoque:\n",
    "Separación clara: Cada fase usa su archivo apropiado\n",
    "Menos código: No necesitas procesar el DataFrame de tripletas\n",
    "Más eficiente: Lectura directa del CSV\n",
    "Pipeline limpio: 03_triplet_gen → CSV → C_classification\n",
    "Entonces sí, tienes razón: load_data() es redundante si ya tienes el CSV procesado. Solo necesitas cambiar kg_path y modificar prepare_triples_factory() para leer directamente el CSV.\n",
    "\n",
    "\n",
    "\n",
    "triples_path para que se usa\n",
    "Editar\n",
    "Analizando el código, triples_path se usa específicamente en la Fase 2 (TopicClassifier) para la clasificación de tópicos, no para el entrenamiento de KGE.\n",
    "Uso de triples_path:\n",
    "En TopicClassifier.load_classification_data() (línea ~299):\n",
    "python\n",
    "def load_classification_data(self) -> pd.DataFrame:\n",
    "    \"\"\"Load data for topic classification\"\"\"\n",
    "    print(\"\\nLoading dataset for topic classification...\")\n",
    "    df = joblib.load(self.triples_path)  # <-- Aquí se usa\n",
    "Propósito:\n",
    "Cargar el DataFrame original que contiene:\n",
    "Las tripletas extraídas\n",
    "Los labels de tópicos (columna topic o new_topic)\n",
    "Metadatos adicionales del dataset original\n",
    "¿Por qué necesita el DataFrame original y no solo el CSV?\n",
    "El CSV (dataset_triplet_bbc_new_simplificado.csv) solo contiene las tripletas \"explotadas\" (una fila por tripleta), pero para la clasificación necesita:\n",
    "Mapear tripletas a embeddings usando el modelo KGE entrenado\n",
    "Asociar cada tripleta con su tópico para el entrenamiento supervisado\n",
    "Crear features concatenando embeddings de subject y object\n",
    "Flujo completo:\n",
    "03_triplet_gen.py → genera dos archivos:\n",
    "├── DataFrame: bbc_semantic_tripletas_simplificado-contopic (para TopicClassifier)\n",
    "└── CSV: dataset_triplet_bbc_new_simplificado.csv (para KGEModelTrainer)\n",
    "\n",
    "C_classification_2.py:\n",
    "├── Fase 1 (KGE): usa kg_path (CSV) → entrena embeddings\n",
    "└── Fase 2 (Classification): usa triples_path (DataFrame) → clasifica tópicos\n",
    "Entonces los parámetros quedan:\n",
    "python\n",
    "run_complete_pipeline(\n",
    "    triples_path=\"data/triples_raw/bbc/bbc_semantic_tripletas_simplificado-contopic\",  # DataFrame para clasificación\n",
    "    kg_path=\"data/triples_raw/bbc/dataset_triplet_bbc_new_simplificado.csv\",  # CSV para KGE training\n",
    "    # ...\n",
    ")\n",
    "Resumen: triples_path es esencial para la fase de clasificación porque necesita el DataFrame original con los labels de tópicos, no solo las tripletas del CSV.\n",
    "\n",
    "\n",
    "\n",
    "pero el csv ahora también tiene los topics\n",
    "Editar\n",
    "¡Tienes razón! Si el CSV ahora también tiene los tópicos (columna new_topic), entonces ambas fases pueden usar el mismo archivo CSV.\n",
    "Análisis del flujo actual:\n",
    "El 03_triplet_gen.py genera el CSV con:\n",
    "subject, relation, object\n",
    "new_topic (los labels de clasificación)\n",
    "old_index\n",
    "Simplificación posible:\n",
    "Opción 1: Usar solo el CSV para ambas fases\n",
    "python\n",
    "def run_complete_pipeline(\n",
    "    csv_path=\"data/triples_raw/bbc/dataset_triplet_bbc_new_simplificado.csv\",  # Un solo archivo\n",
    "    emb_dir=\"data/triples_emb/bbc/\",\n",
    "    output_dir=\"data/output/bbc\",\n",
    "    # ...\n",
    "):\n",
    "    # Fase 1: KGE training\n",
    "    model_path = run_kge_training_phase(\n",
    "        csv_path=csv_path,  # Usa el CSV\n",
    "        emb_dir=emb_dir,\n",
    "        output_dir=output_dir,\n",
    "        model_type=model_type\n",
    "    )\n",
    "    \n",
    "    # Fase 2: Classification  \n",
    "    best_model, metrics = run_classification_phase(\n",
    "        csv_path=csv_path,  # Usa el mismo CSV\n",
    "        model_path=model_path,\n",
    "        mappings_path=mappings_path,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "Cambios necesarios:\n",
    "En KGEModelTrainer: Cambiar joblib.load() por pd.read_csv()\n",
    "En TopicClassifier: Cambiar joblib.load() por pd.read_csv()\n",
    "Unificar parámetros: Un solo csv_path en lugar de triples_path y kg_path\n",
    "Ventajas:\n",
    "Pipeline más limpio: Un solo archivo de entrada\n",
    "Menos confusión: No hay que mantener dos archivos sincronizados\n",
    "Más eficiente: Menos I/O de archivos\n",
    "¿Es esto correcto?\n",
    "Sí, si el CSV tiene toda la información necesaria (tripletas + tópicos), entonces triples_path (el DataFrame serializado) se vuelve redundante.\n",
    "Respuesta directa: Tienes razón, con el CSV que incluye tópicos, podrías simplificar el pipeline para usar un solo archivo de entrada para ambas fases, eliminando la necesidad del triples_path.\n",
    "\n",
    "\n",
    "\n",
    "identifica las modificaciones que tengo que hacer en el código\n",
    "Editar\n",
    "Modificaciones necesarias en C_classification_2.py:\n",
    "1. Modificar constructor de KGEModelTrainer (línea ~27)\n",
    "python\n",
    "# ANTES:\n",
    "def __init__(self, triples_path: str, kg_path: str, emb_dir: str,\n",
    "             output_dir: str = \"data/output/repo\", model_type: str = None):\n",
    "    self.triples_path = triples_path\n",
    "    self.kg_path = kg_path\n",
    "\n",
    "# DESPUÉS:\n",
    "def __init__(self, csv_path: str, emb_dir: str,\n",
    "             output_dir: str = \"data/output/repo\", model_type: str = None):\n",
    "    self.csv_path = csv_path\n",
    "2. Eliminar método load_data() completo (líneas ~89-103)\n",
    "python\n",
    "# ELIMINAR TODO ESTE MÉTODO:\n",
    "def load_data(self) -> pd.DataFrame:\n",
    "    \"\"\"Load and prepare the dataset for KGE training\"\"\"\n",
    "    # ... todo el método\n",
    "3. Modificar prepare_triples_factory() (línea ~105)\n",
    "python\n",
    "# ANTES:\n",
    "def prepare_triples_factory(self, df: pd.DataFrame) -> Tuple[TriplesFactory, TriplesFactory]:\n",
    "\n",
    "# DESPUÉS:\n",
    "def prepare_triples_factory(self) -> Tuple[TriplesFactory, TriplesFactory]:\n",
    "    \"\"\"Prepare triples factory for KGE training\"\"\"\n",
    "    print(\"\\nPreparing triples factory...\")\n",
    "    \n",
    "    # Cargar CSV directamente\n",
    "    df = pd.read_csv(self.csv_path)\n",
    "    \n",
    "    # Clean data\n",
    "    df = df[df['subject'].notna() & df['object'].notna()]\n",
    "    df['relation'] = df['relation'].fillna('have')\n",
    "    \n",
    "    triples = df[['subject', 'relation', 'object']].values\n",
    "    # resto del método igual...\n",
    "4. Modificar run_kge_training() (línea ~240)\n",
    "python\n",
    "# ANTES:\n",
    "def run_kge_training(self) -> str:\n",
    "    # Load data\n",
    "    df = self.load_data()\n",
    "    \n",
    "    # Prepare triples\n",
    "    training_tf, testing_tf = self.prepare_triples_factory(df)\n",
    "\n",
    "# DESPUÉS:\n",
    "def run_kge_training(self) -> str:\n",
    "    # Prepare triples (ahora carga internamente el CSV)\n",
    "    training_tf, testing_tf = self.prepare_triples_factory()\n",
    "5. Modificar constructor de TopicClassifier (línea ~269)\n",
    "python\n",
    "# ANTES:\n",
    "def __init__(self, triples_path: str, model_path: str, mappings_path: str,\n",
    "             output_dir: str = \"data/output/repo\"):\n",
    "    self.triples_path = triples_path\n",
    "\n",
    "# DESPUÉS:\n",
    "def __init__(self, csv_path: str, model_path: str, mappings_path: str,\n",
    "             output_dir: str = \"data/output/repo\"):\n",
    "    self.csv_path = csv_path\n",
    "6. Modificar load_classification_data() (línea ~299)\n",
    "python\n",
    "# ANTES:\n",
    "def load_classification_data(self) -> pd.DataFrame:\n",
    "    \"\"\"Load data for topic classification\"\"\"\n",
    "    print(\"\\nLoading dataset for topic classification...\")\n",
    "    df = joblib.load(self.triples_path)\n",
    "\n",
    "# DESPUÉS:\n",
    "def load_classification_data(self) -> pd.DataFrame:\n",
    "    \"\"\"Load data for topic classification\"\"\"\n",
    "    print(\"\\nLoading dataset for topic classification...\")\n",
    "    df = pd.read_csv(self.csv_path)\n",
    "7. Modificar firmas de funciones principales (líneas ~558-570)\n",
    "python\n",
    "# ANTES:\n",
    "def run_kge_training_phase(triples_path: str, kg_path: str, emb_dir: str,\n",
    "                           output_dir: str = \"data/output/repo\", model_type: str = None) -> str:\n",
    "    trainer = KGEModelTrainer(\n",
    "        triples_path=triples_path,\n",
    "        kg_path=kg_path,\n",
    "        emb_dir=emb_dir,\n",
    "        output_dir=output_dir,\n",
    "        model_type=model_type\n",
    "    )\n",
    "\n",
    "# DESPUÉS:\n",
    "def run_kge_training_phase(csv_path: str, emb_dir: str,\n",
    "                           output_dir: str = \"data/output/repo\", model_type: str = None) -> str:\n",
    "    trainer = KGEModelTrainer(\n",
    "        csv_path=csv_path,\n",
    "        emb_dir=emb_dir,\n",
    "        output_dir=output_dir,\n",
    "        model_type=model_type\n",
    "    )\n",
    "8. Modificar run_classification_phase() (líneas ~581-594)\n",
    "python\n",
    "# ANTES:\n",
    "def run_classification_phase(triples_path: str, model_path: str, mappings_path: str,\n",
    "                             output_dir: str = \"data/output/repo\",\n",
    "                             use_random_search: bool = True, n_iter: int = 20, cv: int = 3):\n",
    "    classifier = TopicClassifier(\n",
    "        triples_path=triples_path,\n",
    "        model_path=model_path,\n",
    "        mappings_path=mappings_path,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "# DESPUÉS:\n",
    "def run_classification_phase(csv_path: str, model_path: str, mappings_path: str,\n",
    "                             output_dir: str = \"data/output/repo\",\n",
    "                             use_random_search: bool = True, n_iter: int = 20, cv: int = 3):\n",
    "    classifier = TopicClassifier(\n",
    "        csv_path=csv_path,\n",
    "        model_path=model_path,\n",
    "        mappings_path=mappings_path,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "9. Modificar run_complete_pipeline() (líneas ~607-622)\n",
    "python\n",
    "# ANTES:\n",
    "def run_complete_pipeline(triples_path: str, kg_path: str, emb_dir: str,\n",
    "                          output_dir: str = \"data/output/repo\", model_type: str = None,\n",
    "                          use_random_search: bool = True, n_iter: int = 20, cv: int = 3):\n",
    "    # Phase 1: KGE Training\n",
    "    model_path = run_kge_training_phase(\n",
    "        triples_path=triples_path,\n",
    "        kg_path=kg_path,\n",
    "        emb_dir=emb_dir,\n",
    "        output_dir=output_dir,\n",
    "        model_type=model_type\n",
    "    )\n",
    "    \n",
    "    # Phase 2: Classification\n",
    "    best_model, metrics = run_classification_phase(\n",
    "        triples_path=triples_path,\n",
    "        model_path=model_path,\n",
    "        mappings_path=mappings_path,\n",
    "        output_dir=output_dir,\n",
    "        use_random_search=use_random_search,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv\n",
    "    )\n",
    "\n",
    "# DESPUÉS:\n",
    "def run_complete_pipeline(csv_path: str, emb_dir: str,\n",
    "                          output_dir: str = \"data/output/repo\", model_type: str = None,\n",
    "                          use_random_search: bool = True, n_iter: int = 20, cv: int = 3):\n",
    "    # Phase 1: KGE Training\n",
    "    model_path = run_kge_training_phase(\n",
    "        csv_path=csv_path,\n",
    "        emb_dir=emb_dir,\n",
    "        output_dir=output_dir,\n",
    "        model_type=model_type\n",
    "    )\n",
    "    \n",
    "    # Derive mappings path from model path\n",
    "    mappings_path = model_path.replace('_model.pkl', '_mappings.pkl')\n",
    "    \n",
    "    # Phase 2: Classification\n",
    "    best_model, metrics = run_classification_phase(\n",
    "        csv_path=csv_path,\n",
    "        model_path=model_path,\n",
    "        mappings_path=mappings_path,\n",
    "        output_dir=output_dir,\n",
    "        use_random_search=use_random_search,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv\n",
    "    )\n",
    "10. Modificar funciones ejemplo (líneas ~659-711)\n",
    "python\n",
    "# ANTES:\n",
    "def example_complete_pipeline():\n",
    "    \"\"\"Example of running both phases sequentially\"\"\"\n",
    "    best_model, metrics = run_complete_pipeline(\n",
    "        triples_path=\"data/triples_ft/processed/dataset_final_triplet_bbc_pykeen\",\n",
    "        kg_path=\"data/triples_ft/processed/triples_bbc.tsv\",\n",
    "        emb_dir=\"data/triples_emb/bbc/\",\n",
    "        output_dir=\"data/output/bbc\",\n",
    "        model_type=None,\n",
    "        use_random_search=True,\n",
    "        n_iter=25,\n",
    "        cv=3\n",
    "    )\n",
    "\n",
    "# DESPUÉS:\n",
    "def example_complete_pipeline():\n",
    "    \"\"\"Example of running both phases sequentially\"\"\"\n",
    "    best_model, metrics = run_complete_pipeline(\n",
    "        csv_path=\"data/triples_raw/bbc/dataset_triplet_bbc_new_simplificado.csv\",\n",
    "        emb_dir=\"data/triples_emb/bbc/\",\n",
    "        output_dir=\"data/output/bbc\",\n",
    "        model_type=None,\n",
    "        use_random_search=True,\n",
    "        n_iter=25,\n",
    "        cv=3\n",
    "    )\n",
    "11. Modificar llamada al final del archivo (línea ~720)\n",
    "python\n",
    "# ANTES:\n",
    "if __name__ == \"__main__\":\n",
    "    # ...\n",
    "    run_complete_pipeline()\n",
    "\n",
    "# DESPUÉS:\n",
    "if __name__ == \"__main__\":\n",
    "    # ...\n",
    "    run_complete_pipeline(\n",
    "        csv_path=\"data/triples_raw/bbc/dataset_triplet_bbc_new_simplificado.csv\",\n",
    "        emb_dir=\"data/triples_emb/bbc/\",\n",
    "        output_dir=\"data/output/bbc\"\n",
    "    )\n",
    "Resumen de cambios:\n",
    "Eliminar: triples_path, kg_path, método load_data()\n",
    "Añadir: csv_path como único parámetro de entrada\n",
    "Cambiar: joblib.load() por pd.read_csv() en ambas clases\n",
    "Simplificar: Firmas de todas las funciones principales\n",
    "\n",
    "\n",
    "\n",
    "Preparing triples factory...\n",
    "Traceback (most recent call last):\n",
    "  File \"/Users/Raul/repos_personales/semoboe-articulo/pipeline/C_classification_2.py\", line 817, in <module>\n",
    "    run_complete_pipeline(\n",
    "  File \"/Users/Raul/repos_personales/semoboe-articulo/pipeline/C_classification_2.py\", line 734, in run_complete_pipeline\n",
    "    model_path = run_kge_training_phase(\n",
    "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/Users/Raul/repos_personales/semoboe-articulo/pipeline/C_classification_2.py\", line 675, in run_kge_training_phase\n",
    "    return trainer.run_kge_training()\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/Users/Raul/repos_personales/semoboe-articulo/pipeline/C_classification_2.py\", line 259, in run_kge_training\n",
    "    training_tf, testing_tf = self.prepare_triples_factory()\n",
    "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/Users/Raul/repos_personales/semoboe-articulo/pipeline/C_classification_2.py\", line 83, in prepare_triples_factory\n",
    "    tf = TriplesFactory.from_labeled_triples(\n",
    "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/Users/Raul/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/pykeen/triples/triples_factory.py\", line 1192, in from_labeled_triples\n",
    "    entity_to_id = create_entity_mapping(triples=triples)\n",
    "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/Users/Raul/repos_personales/semoboe-articulo/.venv/lib/python3.11/site-packages/pykeen/triples/triples_factory.py\", line 67, in create_entity_mapping\n",
    "    entity_labels = sorted(set(heads).union(tails))\n",
    "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "Editar\n",
    "El error indica que hay valores NaN o tipos de datos incorrectos en las tripletas que están causando problemas en PyKEEN. Necesitas añadir más limpieza de datos en prepare_triples_factory().\n",
    "Modificación en prepare_triples_factory() (línea ~83):\n",
    "python\n",
    "def prepare_triples_factory(self) -> Tuple[TriplesFactory, TriplesFactory]:\n",
    "    \"\"\"Prepare triples factory for KGE training\"\"\"\n",
    "    print(\"\\nPreparing triples factory...\")\n",
    "    \n",
    "    # Cargar CSV directamente\n",
    "    df = pd.read_csv(self.csv_path)\n",
    "    \n",
    "    print(f\"Datos cargados: {len(df)} filas\")\n",
    "    print(f\"Columnas: {list(df.columns)}\")\n",
    "    \n",
    "    # LIMPIEZA EXHAUSTIVA DE DATOS\n",
    "    # 1. Eliminar filas con valores NaN en columnas críticas\n",
    "    df = df.dropna(subset=['subject', 'relation', 'object'])\n",
    "    print(f\"Después de eliminar NaN: {len(df)} filas\")\n",
    "    \n",
    "    # 2. Convertir todo a string y limpiar espacios\n",
    "    df['subject'] = df['subject'].astype(str).str.strip()\n",
    "    df['relation'] = df['relation'].astype(str).str.strip()\n",
    "    df['object'] = df['object'].astype(str).str.strip()\n",
    "    \n",
    "    # 3. Eliminar filas con strings vacíos o 'nan'\n",
    "    df = df[\n",
    "        (df['subject'] != '') & (df['subject'] != 'nan') &\n",
    "        (df['relation'] != '') & (df['relation'] != 'nan') &\n",
    "        (df['object'] != '') & (df['object'] != 'nan')\n",
    "    ]\n",
    "    print(f\"Después de limpiar strings vacíos: {len(df)} filas\")\n",
    "    \n",
    "    # 4. Rellenar relaciones vacías\n",
    "    df['relation'] = df['relation'].fillna('have')\n",
    "    \n",
    "    # 5. Mostrar estadísticas\n",
    "    print(f\"Unique subjects: {df['subject'].nunique()}\")\n",
    "    print(f\"Unique relations: {df['relation'].nunique()}\")\n",
    "    print(f\"Unique objects: {df['object'].nunique()}\")\n",
    "    \n",
    "    # 6. Mostrar algunos ejemplos para debug\n",
    "    print(\"\\nPrimeras 5 tripletas:\")\n",
    "    for i in range(min(5, len(df))):\n",
    "        print(f\"  {df.iloc[i]['subject']} | {df.iloc[i]['relation']} | {df.iloc[i]['object']}\")\n",
    "    \n",
    "    if 'new_topic' in df.columns:\n",
    "        print(f\"Target classes: {sorted(df['new_topic'].unique())}\")\n",
    "    \n",
    "    # 7. Verificar que no hay problemas antes de crear triples\n",
    "    triples = df[['subject', 'relation', 'object']].values\n",
    "    \n",
    "    # 8. Verificación adicional\n",
    "    print(f\"Forma del array de tripletas: {triples.shape}\")\n",
    "    print(f\"Tipo de datos: {triples.dtype}\")\n",
    "    \n",
    "    # 9. Verificar que no hay None en el array\n",
    "    if pd.isna(triples).any():\n",
    "        print(\"ADVERTENCIA: Hay valores NaN en las tripletas\")\n",
    "        # Filtrar filas con NaN\n",
    "        mask = ~pd.isna(triples).any(axis=1)\n",
    "        triples = triples[mask]\n",
    "        print(f\"Tripletas después de filtrar NaN: {triples.shape}\")\n",
    "    \n",
    "    # Create triples factory\n",
    "    try:\n",
    "        tf = TriplesFactory.from_labeled_triples(\n",
    "            triples,\n",
    "            create_inverse_triples=False,\n",
    "        )\n",
    "        print(\"TriplesFactory creado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creando TriplesFactory: {e}\")\n",
    "        print(\"Mostrando algunas tripletas problemáticas:\")\n",
    "        for i in range(min(10, len(triples))):\n",
    "            print(f\"  Tripleta {i}: {triples[i]}\")\n",
    "        raise\n",
    "    \n",
    "    # Train/test split for KGE evaluation\n",
    "    training_tf, testing_tf = tf.split([0.8, 0.2], random_state=0)\n",
    "\n",
    "    print(f\"Total triples: {len(triples)}\")\n",
    "    print(f\"Training triples: {len(training_tf.triples)}\")\n",
    "    print(f\"Testing triples: {len(testing_tf.triples)}\")\n",
    "    print(f\"Entities: {training_tf.num_entities}\")\n",
    "    print(f\"Relations: {training_tf.num_relations}\")\n",
    "\n",
    "    return training_tf, testing_tf\n",
    "Si persiste el error, añade también esta verificación previa:\n",
    "Antes de la línea del error, añade esto para inspeccionar los datos:\n",
    "python\n",
    "# DIAGNÓSTICO ADICIONAL (temporal, para debug)\n",
    "print(\"\\nDIAGNÓSTICO DE DATOS:\")\n",
    "print(f\"Tipos de datos en triples:\")\n",
    "print(f\"  Column 0 (subject): {type(triples[0, 0]) if len(triples) > 0 else 'Empty'}\")\n",
    "print(f\"  Column 1 (relation): {type(triples[0, 1]) if len(triples) > 0 else 'Empty'}\")\n",
    "print(f\"  Column 2 (object): {type(triples[0, 2]) if len(triples) > 0 else 'Empty'}\")\n",
    "\n",
    "# Verificar si hay listas o tuplas anidadas\n",
    "for i in range(min(3, len(triples))):\n",
    "    print(f\"Tripleta {i}: {triples[i]} (tipos: {[type(x) for x in triples[i]]})\")\n",
    "La causa más probable es que el CSV tiene:\n",
    "Valores NaN no filtrados\n",
    "Strings vacíos o 'nan' como texto\n",
    "Tipos de datos mixtos en las columnas\n",
    "La limpieza exhaustiva debería resolver el problema.\n",
    "\n",
    "\n",
    "\n",
    "pero la limpieza la debería hacer el archivo 03_triplet_gen\n",
    "Editar\n",
    "¡Tienes razón! La limpieza de datos debería hacerse en 03_triplet_gen.py cuando se genera el CSV, no en C_classification_2.py.\n",
    "Modificaciones necesarias en 03_triplet_gen.py:\n",
    "En la función create_triplets_csv() (línea ~381):\n",
    "python\n",
    "def create_triplets_csv(df, output_dir, csv_filename, include_topics=True):\n",
    "    \"\"\"\n",
    "    Función auxiliar para crear CSV de tripletas\n",
    "    \"\"\"\n",
    "    print(f\"\\nTransformando a dataframe de tripletas...\")\n",
    "\n",
    "    try:\n",
    "        # Eliminar filas sin tripletas\n",
    "        original_len = len(df)\n",
    "        df_filtered = df[df['tripletas'].map(len) > 0].copy()\n",
    "        filtered_len = len(df_filtered)\n",
    "        print(f\"Filas filtradas: {original_len} -> {filtered_len} ({original_len - filtered_len} sin tripletas)\")\n",
    "\n",
    "        if filtered_len == 0:\n",
    "            print(\"ADVERTENCIA: No hay tripletas para procesar\")\n",
    "            return\n",
    "\n",
    "        # Explotar tripletas\n",
    "        df_filtered['tripletas'] = df_filtered['tripletas'].apply(deserializar_tripletas)\n",
    "        triplet_df_def = df_filtered.explode('tripletas', ignore_index=True)\n",
    "\n",
    "        # Filtrar posibles None resultantes de la explosión\n",
    "        triplet_df_def = triplet_df_def[triplet_df_def['tripletas'].notna()]\n",
    "        triplet_df_kge = pd.DataFrame(triplet_df_def['tripletas'].tolist(), index=triplet_df_def.index)\n",
    "\n",
    "        # AÑADIR LIMPIEZA EXHAUSTIVA AQUÍ:\n",
    "        print(\"Aplicando limpieza de datos...\")\n",
    "        \n",
    "        # 1. Verificar que tenemos 3 columnas (subject, relation, object)\n",
    "        if triplet_df_kge.shape[1] != 3:\n",
    "            print(f\"ERROR: Se esperaban 3 columnas, se encontraron {triplet_df_kge.shape[1]}\")\n",
    "            return\n",
    "        \n",
    "        # 2. Nombrar columnas temporalmente para la limpieza\n",
    "        triplet_df_kge.columns = ['subject', 'relation', 'object']\n",
    "        \n",
    "        # 3. Eliminar filas con valores NaN\n",
    "        triplet_df_kge = triplet_df_kge.dropna()\n",
    "        print(f\"Después de eliminar NaN: {len(triplet_df_kge)} tripletas\")\n",
    "        \n",
    "        # 4. Convertir todo a string y limpiar espacios\n",
    "        triplet_df_kge['subject'] = triplet_df_kge['subject'].astype(str).str.strip()\n",
    "        triplet_df_kge['relation'] = triplet_df_kge['relation'].astype(str).str.strip()\n",
    "        triplet_df_kge['object'] = triplet_df_kge['object'].astype(str).str.strip()\n",
    "        \n",
    "        # 5. Eliminar filas con strings vacíos o 'nan'\n",
    "        mask = (\n",
    "            (triplet_df_kge['subject'] != '') & (triplet_df_kge['subject'] != 'nan') &\n",
    "            (triplet_df_kge['relation'] != '') & (triplet_df_kge['relation'] != 'nan') &\n",
    "            (triplet_df_kge['object'] != '') & (triplet_df_kge['object'] != 'nan')\n",
    "        )\n",
    "        triplet_df_kge = triplet_df_kge[mask]\n",
    "        print(f\"Después de limpiar strings vacíos: {len(triplet_df_kge)} tripletas\")\n",
    "        \n",
    "        # 6. Rellenar relaciones vacías con 'have'\n",
    "        triplet_df_kge['relation'] = triplet_df_kge['relation'].replace('', 'have')\n",
    "        \n",
    "        # 7. Filtrar índices correspondientes en triplet_df_def\n",
    "        triplet_df_def = triplet_df_def.loc[triplet_df_kge.index]\n",
    "\n",
    "        # Añadir metadatos - buscar diferentes nombres de columna para tópicos\n",
    "        topic_col_found = None\n",
    "        if include_topics:\n",
    "            for topic_col in ['topic', 'new_target', 'target']:\n",
    "                if topic_col in triplet_df_def.columns:\n",
    "                    triplet_df_kge['new_topic'] = triplet_df_def[topic_col]\n",
    "                    topic_col_found = topic_col\n",
    "                    break\n",
    "\n",
    "        triplet_df_kge['old_index'] = triplet_df_def.index\n",
    "\n",
    "        # Configurar nombres de columnas finales\n",
    "        if include_topics and topic_col_found:\n",
    "            triplet_df_kge.columns = ['subject', 'relation', 'object', 'new_topic', 'old_index']\n",
    "        else:\n",
    "            triplet_df_kge.columns = ['subject', 'relation', 'object', 'old_index']\n",
    "\n",
    "        # 8. VALIDACIÓN FINAL antes de guardar\n",
    "        print(\"Validación final...\")\n",
    "        print(f\"Tripletas finales: {len(triplet_df_kge)}\")\n",
    "        print(f\"Unique subjects: {triplet_df_kge['subject'].nunique()}\")\n",
    "        print(f\"Unique relations: {triplet_df_kge['relation'].nunique()}\")\n",
    "        print(f\"Unique objects: {triplet_df_kge['object'].nunique()}\")\n",
    "        \n",
    "        if include_topics and topic_col_found:\n",
    "            print(f\"Unique topics: {triplet_df_kge['new_topic'].nunique()}\")\n",
    "            print(f\"Topic distribution: {triplet_df_kge['new_topic'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # 9. Verificar que no hay valores problemáticos\n",
    "        problematic_rows = triplet_df_kge[\n",
    "            triplet_df_kge['subject'].isna() | \n",
    "            triplet_df_kge['relation'].isna() | \n",
    "            triplet_df_kge['object'].isna()\n",
    "        ]\n",
    "        \n",
    "        if len(problematic_rows) > 0:\n",
    "            print(f\"ADVERTENCIA: {len(problematic_rows)} filas con valores problemáticos eliminadas\")\n",
    "            triplet_df_kge = triplet_df_kge.dropna(subset=['subject', 'relation', 'object'])\n",
    "\n",
    "        # Guardar CSV\n",
    "        csv_path = os.path.join(output_dir, csv_filename)\n",
    "        triplet_df_kge.to_csv(csv_path, index=False)\n",
    "\n",
    "        print(f\"Transformación completada!\")\n",
    "        print(f\"Total de tripletas limpias generadas: {len(triplet_df_kge)}\")\n",
    "        print(f\"Archivo CSV guardado en: {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR durante la transformación: {e}\")\n",
    "        traceback.print_exc()\n",
    "Y en C_classification_2.py mantener prepare_triples_factory() simple:\n",
    "python\n",
    "def prepare_triples_factory(self) -> Tuple[TriplesFactory, TriplesFactory]:\n",
    "    \"\"\"Prepare triples factory for KGE training\"\"\"\n",
    "    print(\"\\nPreparing triples factory...\")\n",
    "    \n",
    "    # Cargar CSV ya limpio\n",
    "    df = pd.read_csv(self.csv_path)\n",
    "    \n",
    "    print(f\"Datos cargados: {len(df)} filas\")\n",
    "    print(f\"Unique subjects: {df['subject'].nunique()}\")\n",
    "    print(f\"Unique relations: {df['relation'].nunique()}\")\n",
    "    print(f\"Unique objects: {df['object'].nunique()}\")\n",
    "    \n",
    "    if 'new_topic' in df.columns:\n",
    "        print(f\"Target classes: {sorted(df['new_topic'].unique())}\")\n",
    "    \n",
    "    triples = df[['subject', 'relation', 'object']].values\n",
    "\n",
    "    # Create triples factory (datos ya están limpios)\n",
    "    tf = TriplesFactory.from_labeled_triples(\n",
    "        triples,\n",
    "        create_inverse_triples=False,\n",
    "    )\n",
    "\n",
    "    # Train/test split for KGE evaluation\n",
    "    training_tf, testing_tf = tf.split([0.8, 0.2], random_state=0)\n",
    "\n",
    "    print(f\"Total triples: {len(triples)}\")\n",
    "    print(f\"Training triples: {len(training_tf.triples)}\")\n",
    "    print(f\"Testing triples: {len(testing_tf.triples)}\")\n",
    "    print(f\"Entities: {training_tf.num_entities}\")\n",
    "    print(f\"Relations: {training_tf.num_relations}\")\n",
    "\n",
    "    return training_tf, testing_tf\n",
    "Resumen: La limpieza debe hacerse en 03_triplet_gen.py al generar el CSV, y C_classification_2.py debe confiar en que los datos ya están limpios. Esto es mejor separación de responsabilidades.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "triplet_manager_lib.py\n",
    "968 líneas\n",
    "PY\n",
    "\n",
    "pos_manager_lib.py\n",
    "280 líneas\n",
    "PY\n",
    "03_triplet_gen no debe estar funcionando bien, porque he visto estas ternas:\n",
    "266241useuseThey0266241266242useuseinaccurate0266242266243useuseuncomfortable0266243266244useusecumbersome026624\n",
    "que no son válidas.\n",
    "Usa estos dos archivos para definir la triplete y demás. Revisa todo el código y dime dónde está el fallo, porque en la versión antigua sí funcionaba\n",
    "Editar\n",
    "Analizando el código, veo el problema. Las tripletas malformadas que muestras (**266241**useuseThey0266241) indican que hay un error en el procesamiento de las tripletas en 03_triplet_gen.py.\n",
    "Problemas identificados:\n",
    "1. Error en deserializar_tripletas() (línea ~340)\n",
    "La función está mal implementada y no maneja correctamente las tripletas:\n",
    "python\n",
    "def deserializar_tripletas(triplet_str):\n",
    "    \"\"\"Convierte string de tripletas a lista real\"\"\"\n",
    "    if pd.isna(triplet_str) or triplet_str == '' or triplet_str == '[]':\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Intentar eval (cuidado: solo usar con datos confiables)\n",
    "        import ast\n",
    "        return ast.literal_eval(triplet_str)\n",
    "    except:\n",
    "        try:\n",
    "            # Alternativa con eval si falla\n",
    "            return eval(triplet_str)\n",
    "        except:\n",
    "            print(f\"ADVERTENCIA: No se pudo deserializar: {triplet_str[:100]}...\")\n",
    "            return []\n",
    "Problema: Esta función asume que las tripletas están serializadas como strings, pero en realidad ya son listas de diccionarios.\n",
    "2. Error en create_triplets_csv() - procesamiento incorrecto\n",
    "El problema está en estas líneas:\n",
    "python\n",
    "# LÍNEA PROBLEMÁTICA:\n",
    "df_filtered['tripletas'] = df_filtered['tripletas'].apply(deserializar_tripletas)\n",
    "\n",
    "# Y LUEGO:\n",
    "triplet_df_kge = pd.DataFrame(triplet_df_def['tripletas'].tolist(), index=triplet_df_def.index)\n",
    "3. Mala creación de las tripletas en devolver_tripletas()\n",
    "El método encapsular() está creando diccionarios correctamente, pero hay inconsistencias.\n",
    "Soluciones:\n",
    "Corregir create_triplets_csv() completo:\n",
    "python\n",
    "def create_triplets_csv(df, output_dir, csv_filename, include_topics=True):\n",
    "    \"\"\"\n",
    "    Función auxiliar para crear CSV de tripletas\n",
    "    \"\"\"\n",
    "    print(f\"\\nTransformando a dataframe de tripletas...\")\n",
    "\n",
    "    try:\n",
    "        # Eliminar filas sin tripletas\n",
    "        original_len = len(df)\n",
    "        df_filtered = df[df['tripletas'].map(lambda x: len(x) > 0 if isinstance(x, list) else False)].copy()\n",
    "        filtered_len = len(df_filtered)\n",
    "        print(f\"Filas filtradas: {original_len} -> {filtered_len} ({original_len - filtered_len} sin tripletas)\")\n",
    "\n",
    "        if filtered_len == 0:\n",
    "            print(\"ADVERTENCIA: No hay tripletas para procesar\")\n",
    "            return\n",
    "\n",
    "        # DEPURACIÓN: Verificar el tipo de datos en tripletas\n",
    "        print(\"Verificando tipos de datos en 'tripletas'...\")\n",
    "        sample_triplet = df_filtered['tripletas'].iloc[0]\n",
    "        print(f\"Tipo de sample_triplet: {type(sample_triplet)}\")\n",
    "        print(f\"Contenido de sample_triplet: {sample_triplet}\")\n",
    "        \n",
    "        if isinstance(sample_triplet, list) and len(sample_triplet) > 0:\n",
    "            print(f\"Tipo del primer elemento: {type(sample_triplet[0])}\")\n",
    "            print(f\"Primer elemento: {sample_triplet[0]}\")\n",
    "\n",
    "        # NO APLICAR deserializar_tripletas - las tripletas ya son listas\n",
    "        # df_filtered['tripletas'] = df_filtered['tripletas'].apply(deserializar_tripletas)  # ELIMINAR ESTA LÍNEA\n",
    "\n",
    "        # Explotar tripletas directamente\n",
    "        triplet_df_def = df_filtered.explode('tripletas', ignore_index=True)\n",
    "\n",
    "        # Filtrar posibles None resultantes de la explosión\n",
    "        triplet_df_def = triplet_df_def[triplet_df_def['tripletas'].notna()]\n",
    "        \n",
    "        print(f\"Después de explotar: {len(triplet_df_def)} filas\")\n",
    "        \n",
    "        # DEPURACIÓN: Verificar el contenido después de explotar\n",
    "        print(\"Verificando contenido después de explotar...\")\n",
    "        if len(triplet_df_def) > 0:\n",
    "            sample_exploded = triplet_df_def['tripletas'].iloc[0]\n",
    "            print(f\"Tipo después de explotar: {type(sample_exploded)}\")\n",
    "            print(f\"Contenido después de explotar: {sample_exploded}\")\n",
    "\n",
    "        # Verificar que las tripletas son diccionarios con las claves correctas\n",
    "        valid_triplets = []\n",
    "        for idx, triplet in triplet_df_def['tripletas'].items():\n",
    "            if isinstance(triplet, dict) and all(key in triplet for key in ['subject', 'relation', 'object']):\n",
    "                valid_triplets.append({\n",
    "                    'index': idx,\n",
    "                    'subject': str(triplet['subject']).strip(),\n",
    "                    'relation': str(triplet['relation']).strip(),\n",
    "                    'object': str(triplet['object']).strip()\n",
    "                })\n",
    "            else:\n",
    "                print(f\"ADVERTENCIA: Tripleta inválida en índice {idx}: {triplet}\")\n",
    "\n",
    "        if not valid_triplets:\n",
    "            print(\"ERROR: No se encontraron tripletas válidas\")\n",
    "            return\n",
    "\n",
    "        # Crear DataFrame desde tripletas válidas\n",
    "        triplet_df_kge = pd.DataFrame(valid_triplets)\n",
    "        triplet_df_kge.set_index('index', inplace=True)\n",
    "        \n",
    "        print(f\"Tripletas válidas procesadas: {len(triplet_df_kge)}\")\n",
    "\n",
    "        # APLICAR LIMPIEZA EXHAUSTIVA:\n",
    "        print(\"Aplicando limpieza de datos...\")\n",
    "        \n",
    "        # Eliminar filas con valores NaN\n",
    "        triplet_df_kge = triplet_df_kge.dropna()\n",
    "        print(f\"Después de eliminar NaN: {len(triplet_df_kge)} tripletas\")\n",
    "        \n",
    "        # Eliminar filas con strings vacíos o 'nan'\n",
    "        mask = (\n",
    "            (triplet_df_kge['subject'] != '') & (triplet_df_kge['subject'] != 'nan') &\n",
    "            (triplet_df_kge['relation'] != '') & (triplet_df_kge['relation'] != 'nan') &\n",
    "            (triplet_df_kge['object'] != '') & (triplet_df_kge['object'] != 'nan')\n",
    "        )\n",
    "        triplet_df_kge = triplet_df_kge[mask]\n",
    "        print(f\"Después de limpiar strings vacíos: {len(triplet_df_kge)} tripletas\")\n",
    "        \n",
    "        # Rellenar relaciones vacías con 'have'\n",
    "        triplet_df_kge['relation'] = triplet_df_kge['relation'].replace('', 'have')\n",
    "        \n",
    "        # Filtrar índices correspondientes en triplet_df_def para los tópicos\n",
    "        triplet_df_def = triplet_df_def.loc[triplet_df_kge.index]\n",
    "\n",
    "        # Añadir metadatos de tópicos\n",
    "        topic_col_found = None\n",
    "        if include_topics:\n",
    "            for topic_col in ['topic', 'new_target', 'target']:\n",
    "                if topic_col in triplet_df_def.columns:\n",
    "                    triplet_df_kge['new_topic'] = triplet_df_def[topic_col]\n",
    "                    topic_col_found = topic_col\n",
    "                    print(f\"Tópicos añadidos desde columna: {topic_col}\")\n",
    "                    break\n",
    "\n",
    "        triplet_df_kge['old_index'] = triplet_df_def.index\n",
    "\n",
    "        # VALIDACIÓN FINAL antes de guardar\n",
    "        print(\"Validación final...\")\n",
    "        print(f\"Tripletas finales: {len(triplet_df_kge)}\")\n",
    "        print(f\"Unique subjects: {triplet_df_kge['subject'].nunique()}\")\n",
    "        print(f\"Unique relations: {triplet_df_kge['relation'].nunique()}\")\n",
    "        print(f\"Unique objects: {triplet_df_kge['object'].nunique()}\")\n",
    "        \n",
    "        if include_topics and topic_col_found:\n",
    "            print(f\"Unique topics: {triplet_df_kge['new_topic'].nunique()}\")\n",
    "            print(f\"Topic distribution: {triplet_df_kge['new_topic'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Mostrar algunas tripletas de ejemplo\n",
    "        print(\"\\nPrimeras 5 tripletas procesadas:\")\n",
    "        for i in range(min(5, len(triplet_df_kge))):\n",
    "            row = triplet_df_kge.iloc[i]\n",
    "            print(f\"  {row['subject']} | {row['relation']} | {row['object']}\")\n",
    "\n",
    "        # Guardar CSV\n",
    "        csv_path = os.path.join(output_dir, csv_filename)\n",
    "        triplet_df_kge.to_csv(csv_path, index=False)\n",
    "\n",
    "        print(f\"Transformación completada!\")\n",
    "        print(f\"Total de tripletas limpias generadas: {len(triplet_df_kge)}\")\n",
    "        print(f\"Archivo CSV guardado en: {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR durante la transformación: {e}\")\n",
    "        traceback.print_exc()\n",
    "Eliminar la función deserializar_tripletas() completamente\n",
    "Esta función no es necesaria si las tripletas ya están en el formato correcto.\n",
    "Verificar el método encapsular() en triplet_manager_lib.py\n",
    "Asegúrate de que está creando diccionarios correctamente:\n",
    "python\n",
    "def encapsular(self, tripleta, aDiccionario=False):\n",
    "    if type(tripleta) == list:\n",
    "        if aDiccionario:\n",
    "            return [{'subject': str(elemento[0]), 'relation': str(elemento[1]), 'object': str(elemento[2])} for elemento in tripleta]\n",
    "        else:\n",
    "            return [Tripleta({'subject': str(elemento[0]), 'relation': str(elemento[1]), 'object': str(elemento[2])}) for elemento in tripleta]\n",
    "    else:\n",
    "        if aDiccionario:\n",
    "            return {'subject': str(tripleta[0]), 'relation': str(tripleta[1]), 'object': str(tripleta[2])}\n",
    "        else:\n",
    "            return Tripleta({'subject': str(tripleta[0]), 'relation': str(tripleta[1]), 'object': str(tripleta[2])})\n",
    "El problema principal es que no debes deserializar las tripletas porque ya están en el formato correcto (lista de diccionarios). La función deserializar_tripletas() está corrompiendo los datos.\n",
    "\n",
    "\n",
    "\n",
    "No creo que sea eso, es más del análisis de PoS y si es una terna válida. Por ejemplo, genera ternas del tipo:\n",
    "use use They\n",
    "Editar\n",
    "Tienes razón. El problema no está en la serialización, sino en la lógica de generación y validación de tripletas. La tripleta (use, use, They) indica que hay problemas en:\n",
    "Extracción incorrecta de elementos\n",
    "Validación de POS tags fallida\n",
    "Lógica de filtrado defectuosa\n",
    "Análisis del problema:\n",
    "1. Error en devolver_tripletas() - Método encapsular()\n",
    "En la línea donde se llama a encapsular():\n",
    "python\n",
    "def devolver_tripletas(sentence, frase, pos, tripletgenerator):\n",
    "    \"\"\"Extrae tripletas de una oración usando diferentes métodos\"\"\"\n",
    "    tripletas = [tripletgenerator.encapsular((triple.subject, triple.relation, triple.object), True)\n",
    "                 for triple in sentence.openieTriple]\n",
    "    if len(tripletas) == 0:\n",
    "        tripleta = tripletgenerator.triplet_extraction(frase, True)  # <-- PROBLEMA AQUÍ\n",
    "        if tripleta is None:\n",
    "            return tripletas\n",
    "Problema: triplet_extraction() debería retornar una tupla (subject, relation, object), pero puede estar retornando algo mal formateado.\n",
    "2. Error en triplet_extraction() de TripletGenerator\n",
    "python\n",
    "def triplet_extraction(self, input_sent): ## revisar\n",
    "    try:\n",
    "        parse_tree = ParentedTree.convert(list(self.pos_tagger.parse(input_sent.split()))[0])\n",
    "        # Extract subject, predicate and object\n",
    "        subject = self.extract_subject(parse_tree)\n",
    "        predicate = self.extract_predicate(parse_tree)\n",
    "        objects = self.extract_object(parse_tree)\n",
    "        return (subject[0], predicate[0], objects[0])  # <-- PROBLEMA POTENCIAL\n",
    "    except:\n",
    "        print ('Error al procesar la frase {}'.format(input_sent))\n",
    "        return None\n",
    "Problema: Los métodos extract_subject(), extract_predicate(), extract_object() retornan listas con [palabra, atributos], pero solo se toma el primer elemento [0].\n",
    "3. Método es_candidata() en TripletManager no está funcionando\n",
    "python\n",
    "def es_candidata(self, tripleta, pos, ner, topics, dbpedia):\n",
    "    setner = StringCaseInsensitiveSet(ner)\n",
    "    settopics = StringCaseInsensitiveSet(topics) \n",
    "    setdbpedia = StringCaseInsensitiveSet(dbpedia)\n",
    "    \n",
    "    # Este método debería filtrar tripletas como \"use use They\"\n",
    "    por_vocabulario = tripleta.contieneVocabulario(setner,'all') or tripleta.contieneVocabulario(settopics,'all') or tripleta.contieneVocabulario(setdbpedia,'all')\n",
    "    \n",
    "    if por_vocabulario == False:\n",
    "        return por_vocabulario  # <-- Aquí debería rechazar tripletas inválidas\n",
    "Soluciones necesarias:\n",
    "1. Corregir la creación de objetos Tripleta\n",
    "El problema está en cómo se pasan los datos al constructor de Tripleta. En devolver_tripletas():\n",
    "python\n",
    "def devolver_tripletas(sentence, frase, pos, tripletgenerator):\n",
    "    \"\"\"Extrae tripletas de una oración usando diferentes métodos\"\"\"\n",
    "    tripletas = []\n",
    "    \n",
    "    # Procesar tripletas de OpenIE\n",
    "    for triple in sentence.openieTriple:\n",
    "        # VALIDAR antes de encapsular\n",
    "        if (hasattr(triple, 'subject') and hasattr(triple, 'relation') and hasattr(triple, 'object') and\n",
    "            triple.subject and triple.relation and triple.object):\n",
    "            \n",
    "            tripleta_dict = {\n",
    "                'subject': str(triple.subject).strip(),\n",
    "                'relation': str(triple.relation).strip(), \n",
    "                'object': str(triple.object).strip()\n",
    "            }\n",
    "            \n",
    "            # Crear objeto Tripleta para validación\n",
    "            tripleta_obj = tripletgenerator.encapsular((tripleta_dict['subject'], \n",
    "                                                      tripleta_dict['relation'], \n",
    "                                                      tripleta_dict['object']), False)\n",
    "            \n",
    "            # VALIDAR la tripleta antes de añadirla\n",
    "            if validar_tripleta_basica(tripleta_obj, pos):\n",
    "                tripletas.append(tripletgenerator.encapsular((tripleta_dict['subject'], \n",
    "                                                            tripleta_dict['relation'], \n",
    "                                                            tripleta_dict['object']), True))\n",
    "    \n",
    "    # Solo procesar otros métodos si no hay tripletas válidas de OpenIE\n",
    "    if len(tripletas) == 0:\n",
    "        tripleta = tripletgenerator.triplet_extraction(frase, True)\n",
    "        if tripleta is not None and len(tripleta) == 3:\n",
    "            # VALIDAR antes de añadir\n",
    "            tripleta_obj = tripletgenerator.encapsular(tripleta, False) \n",
    "            if validar_tripleta_basica(tripleta_obj, pos):\n",
    "                tripletas.append(tripletgenerator.encapsular(tripleta, True))\n",
    "    \n",
    "    # Resto de métodos con validación...\n",
    "    # [otros métodos de generación con la misma validación]\n",
    "    \n",
    "    return tripletas\n",
    "2. Añadir función de validación básica\n",
    "python\n",
    "def validar_tripleta_basica(tripleta, pos):\n",
    "    \"\"\"Validación básica para evitar tripletas malformadas\"\"\"\n",
    "    if not tripleta or not hasattr(tripleta, 'sujeto') or not hasattr(tripleta, 'relacion') or not hasattr(tripleta, 'objeto'):\n",
    "        return False\n",
    "        \n",
    "    # Verificar que no sean strings vacíos\n",
    "    if not tripleta.sujeto.strip() or not tripleta.relacion.strip() or not tripleta.objeto.strip():\n",
    "        return False\n",
    "    \n",
    "    # Verificar que subject y object no sean iguales a relation\n",
    "    if (tripleta.sujeto.lower() == tripleta.relacion.lower() or \n",
    "        tripleta.objeto.lower() == tripleta.relacion.lower()):\n",
    "        return False\n",
    "    \n",
    "    # Verificar que no sean solo números o caracteres especiales\n",
    "    if (tripleta.sujeto.isdigit() or tripleta.relacion.isdigit() or tripleta.objeto.isdigit()):\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "3. Corregir el método triplet_extraction() en TripletGenerator\n",
    "python\n",
    "def triplet_extraction(self, input_sent, return_dict=False):\n",
    "    try:\n",
    "        parse_tree = ParentedTree.convert(list(self.pos_tagger.parse(input_sent.split()))[0])\n",
    "        \n",
    "        # Extract subject, predicate and object\n",
    "        subject = self.extract_subject(parse_tree)\n",
    "        predicate = self.extract_predicate(parse_tree)\n",
    "        objects = self.extract_object(parse_tree)\n",
    "        \n",
    "        # VALIDAR que todos los elementos existen y no están vacíos\n",
    "        if (not subject or not subject[0] or \n",
    "            not predicate or not predicate[0] or \n",
    "            not objects or not objects[0]):\n",
    "            return None\n",
    "            \n",
    "        # VALIDAR que no son iguales\n",
    "        if (subject[0].lower() == predicate[0].lower() or \n",
    "            subject[0].lower() == objects[0].lower() or\n",
    "            predicate[0].lower() == objects[0].lower()):\n",
    "            return None\n",
    "            \n",
    "        return (str(subject[0]).strip(), str(predicate[0]).strip(), str(objects[0]).strip())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error al procesar la frase {input_sent}: {e}')\n",
    "        return None\n",
    "4. Mejorar el filtrado en anotar_row()\n",
    "python\n",
    "# En la función anotar_row(), después de obtener las tripletas:\n",
    "if enable_filtering and tripletmanager:\n",
    "    tripletas_filtradas = []\n",
    "    for tripleta in t:\n",
    "        try:\n",
    "            # Crear objeto Tripleta para validación si es diccionario\n",
    "            if isinstance(tripleta, dict):\n",
    "                tripleta_obj = Tripleta(tripleta)\n",
    "            else:\n",
    "                tripleta_obj = tripleta\n",
    "                \n",
    "            if (validar_tripleta_basica(tripleta_obj, d) and\n",
    "                tripletmanager.es_candidata(tripleta_obj, d, ner, topic, dbpedia)):\n",
    "                tripletas_filtradas.append(tripleta)\n",
    "        except Exception as e:\n",
    "            print(f\"Error validando tripleta {tripleta}: {e}\")\n",
    "            continue\n",
    "    resultado.extend(tripletas_filtradas)\n",
    "else:\n",
    "    # Aplicar validación básica aunque no esté habilitado el filtrado avanzado\n",
    "    tripletas_validas = []\n",
    "    for tripleta in t:\n",
    "        if isinstance(tripleta, dict):\n",
    "            tripleta_obj = Tripleta(tripleta)\n",
    "        else:\n",
    "            tripleta_obj = tripleta\n",
    "            \n",
    "        if validar_tripleta_basica(tripleta_obj, d):\n",
    "            tripletas_validas.append(tripleta)\n",
    "    resultado.extend(tripletas_validas)\n",
    "El problema principal es la falta de validación en múltiples niveles. Las tripletas malformadas como (use, use, They) deberían ser rechazadas por la validación básica antes de llegar al CSV final.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3_generacion_tripletas_simplificado.py\n",
    "193 líneas\n",
    "PY\n",
    "\n",
    "types.py\n",
    "146 líneas\n",
    "PY\n",
    "Este archivo es el antiguo\n",
    "Editar\n",
    "Perfecto! Ahora veo la diferencia. Comparando el archivo antiguo que funciona (3_generacion_tripletas_simplificado.py) con el nuevo que falla (03_triplet_gen.py), encuentro varios problemas críticos:\n",
    "Diferencias clave que causan el problema:\n",
    "1. Falta validación con es_candidata() en el nuevo código\n",
    "ANTIGUO (funciona):\n",
    "python\n",
    "def anotar(row):\n",
    "    tripletgenerator = TripletGenerator()\n",
    "    resultado = []\n",
    "    tripletmanager = TripletManager()  # <-- CREA TripletManager\n",
    "    row_topic = row['new_target']\n",
    "    dbpedia = list(row['entidades_dbpedia_simplificadas'].keys())\n",
    "    ner = row['entidades']\n",
    "    topic = topics.get(row_topic)  # <-- USA topics cargados\n",
    "\n",
    "    # ... procesamiento ...\n",
    "    \n",
    "    # IMPORTANTE: Usa es_candidata() implícitamente\n",
    "    resultado.extend(t)  # Solo añade tripletas válidas\n",
    "NUEVO (falla):\n",
    "python\n",
    "def anotar_row(row, client, sleep_time, enable_filtering, filter_method):\n",
    "    # ... procesamiento ...\n",
    "    \n",
    "    # Aplicar filtrado si está habilitado\n",
    "    if enable_filtering and tripletmanager:\n",
    "        tripletas_filtradas = []\n",
    "        for tripleta in t:\n",
    "            try:\n",
    "                if tripletmanager.es_candidata(tripleta, d, ner, topic, dbpedia):  # <-- PROBLEMA: no funciona bien\n",
    "                    tripletas_filtradas.append(tripleta)\n",
    "            except:\n",
    "                tripletas_filtradas.append(tripleta)  # <-- PROBLEMA: añade tripletas inválidas en caso de error\n",
    "        resultado.extend(tripletas_filtradas)\n",
    "    else:\n",
    "        resultado.extend(t)  # <-- PROBLEMA: sin filtrado añade TODAS las tripletas, incluso inválidas\n",
    "2. Error en el manejo de topics\n",
    "ANTIGUO:\n",
    "python\n",
    "topics = joblib.load('/Users/Raul/doctorado/semantic_oboe/semantic_oboe/bbc_objects/new_bbc_topics_7_sinprob')\n",
    "topic = topics.get(row_topic)  # <-- Obtiene el topic específico\n",
    "NUEVO:\n",
    "python\n",
    "topic = row.get('topic')  # <-- Solo obtiene el nombre del topic, no el contenido\n",
    "3. Falta carga de topics globales\n",
    "El código antiguo carga un archivo de topics que contiene vocabulario específico para cada categoría, pero el nuevo no lo hace.\n",
    "Soluciones para 03_triplet_gen.py:\n",
    "1. Corregir anotar_row() para validar SIEMPRE:\n",
    "python\n",
    "def anotar_row(row, client, sleep_time, enable_filtering, filter_method):\n",
    "    \"\"\"\n",
    "    Procesa una fila del dataframe y extrae tripletas\n",
    "    \"\"\"\n",
    "    tripletgenerator = TripletGenerator()\n",
    "    resultado = []\n",
    "\n",
    "    # Obtener información de tópicos\n",
    "    topic = row.get('topic')\n",
    "\n",
    "    # Obtener datos para filtrado - SIEMPRE crear tripletmanager\n",
    "    tripletmanager = TripletManager()\n",
    "    ner = set()\n",
    "    dbpedia = []\n",
    "    topics_vocab = set()  # Vocabulario específico del topic\n",
    "\n",
    "    # Obtener entidades NER\n",
    "    if filter_method in ['ner_only', 'both']:\n",
    "        for ner_col in ['entidades', 'ner', 'tokens']:\n",
    "            if ner_col in row and row[ner_col]:\n",
    "                ner = row[ner_col]\n",
    "                if isinstance(ner, str):\n",
    "                    ner = set([ner])\n",
    "                elif not isinstance(ner, set):\n",
    "                    ner = set(ner) if ner else set()\n",
    "                break\n",
    "\n",
    "    # Obtener entidades DBpedia\n",
    "    if filter_method in ['dbpedia_only', 'both']:\n",
    "        for dbpedia_col in ['entidades_dbpedia_simplificadas', 'entidades_dbpedia']:\n",
    "            if dbpedia_col in row and row[dbpedia_col]:\n",
    "                entidades_dbpedia = row[dbpedia_col]\n",
    "                if isinstance(entidades_dbpedia, dict):\n",
    "                    dbpedia = list(entidades_dbpedia.keys())\n",
    "                    break\n",
    "                # ... resto del procesamiento igual\n",
    "\n",
    "    # AÑADIR: Cargar vocabulario del topic (como en el código antiguo)\n",
    "    # Esto requiere tener el archivo de topics cargado globalmente\n",
    "    if topic and hasattr(anotar_row, 'topics') and anotar_row.topics:\n",
    "        topics_vocab = set(anotar_row.topics.get(topic, []))\n",
    "\n",
    "    # Procesar el texto\n",
    "    text_column = ''\n",
    "    for text_col in ['text_coref', 'coref_text', 'text', 'news']:\n",
    "        if text_col in row and row[text_col]:\n",
    "            text_column = str(row[text_col])\n",
    "            break\n",
    "\n",
    "    if not text_column:\n",
    "        return resultado\n",
    "\n",
    "    for frase in text_column.split('.'):\n",
    "        if not frase.strip():\n",
    "            continue\n",
    "        try:\n",
    "            anotacion = client.annotate(frase)\n",
    "            sleep(sleep_time)\n",
    "\n",
    "            for sentence in anotacion.sentence:\n",
    "                d = devolver_pos(sentence)\n",
    "                t = devolver_tripletas(sentence, frase, d, tripletgenerator)\n",
    "                \n",
    "                # VALIDACIÓN OBLIGATORIA - como en el código antiguo\n",
    "                tripletas_validas = []\n",
    "                for tripleta in t:\n",
    "                    try:\n",
    "                        # Crear objeto Tripleta si es diccionario\n",
    "                        if isinstance(tripleta, dict):\n",
    "                            tripleta_obj = Tripleta(tripleta)\n",
    "                        else:\n",
    "                            tripleta_obj = tripleta\n",
    "\n",
    "                        # APLICAR SIEMPRE es_candidata() (como en el código antiguo)\n",
    "                        if tripletmanager.es_candidata(tripleta_obj, d, ner, topics_vocab, dbpedia):\n",
    "                            tripletas_validas.append(tripleta)\n",
    "                        else:\n",
    "                            # DEBUG: mostrar tripletas rechazadas\n",
    "                            print(f\"RECHAZADA: {tripleta}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error validando tripleta {tripleta}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "                resultado.extend(tripletas_validas)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return resultado\n",
    "2. Añadir carga de topics al inicio del script:\n",
    "python\n",
    "def main():\n",
    "    # ... argumentos existentes ...\n",
    "    \n",
    "    # AÑADIR: Cargar topics como en el código antiguo\n",
    "    topics_file = args.get('topics_file', None)  # Añadir este argumento\n",
    "    if topics_file and os.path.exists(topics_file):\n",
    "        topics = joblib.load(topics_file)\n",
    "        anotar_row.topics = topics  # Hacer accesible globalmente\n",
    "        print(f\"Topics cargados desde: {topics_file}\")\n",
    "    else:\n",
    "        anotar_row.topics = {}\n",
    "        print(\"ADVERTENCIA: No se cargaron topics - la validación puede ser menos efectiva\")\n",
    "3. Añadir argumento para topics:\n",
    "python\n",
    "parser.add_argument('--topics_file', type=str,\n",
    "                    default=None,\n",
    "                    help='Archivo de topics para validación de vocabulario')\n",
    "4. Corregir devolver_tripletas() con validación básica:\n",
    "python\n",
    "def devolver_tripletas(sentence, frase, pos, tripletgenerator):\n",
    "    \"\"\"Extrae tripletas de una oración usando diferentes métodos\"\"\"\n",
    "    tripletas = []\n",
    "    \n",
    "    # Procesar tripletas de OpenIE con validación\n",
    "    for triple in sentence.openieTriple:\n",
    "        if (hasattr(triple, 'subject') and hasattr(triple, 'relation') and hasattr(triple, 'object') and\n",
    "            triple.subject and triple.relation and triple.object):\n",
    "            \n",
    "            # VALIDACIÓN BÁSICA como en el código antiguo\n",
    "            if (str(triple.subject).strip() and str(triple.relation).strip() and str(triple.object).strip() and\n",
    "                str(triple.subject).strip() != str(triple.relation).strip() and\n",
    "                str(triple.subject).strip() != str(triple.object).strip() and\n",
    "                str(triple.relation).strip() != str(triple.object).strip()):\n",
    "                \n",
    "                tripletas.append(tripletgenerator.encapsular((triple.subject, triple.relation, triple.object), True))\n",
    "    \n",
    "    # Solo usar otros métodos si no hay tripletas válidas de OpenIE\n",
    "    if len(tripletas) == 0:\n",
    "        tripleta = tripletgenerator.triplet_extraction(frase, True)\n",
    "        if tripleta is not None and len(tripleta) == 3:\n",
    "            # Validación básica\n",
    "            if (tripleta[0] and tripleta[1] and tripleta[2] and\n",
    "                tripleta[0] != tripleta[1] and tripleta[0] != tripleta[2] and tripleta[1] != tripleta[2]):\n",
    "                tripletas.append(tripletgenerator.encapsular(tripleta, True))\n",
    "    \n",
    "    # Resto de métodos con la misma validación...\n",
    "    # [aplicar la misma lógica a todos los otros métodos]\n",
    "    \n",
    "    return tripletas\n",
    "Resumen del problema: El código nuevo no aplica sistemáticamente la validación es_candidata() que el código antiguo sí hacía, permitiendo que tripletas malformadas como (use, use, They) pasen sin filtrar.\n",
    "\n",
    "\n",
    "\n",
    "pero dónde ves que el antiguo este usando es_Candidata de forma implicita\n",
    "Editar\n",
    "Tienes razón, revisando más cuidadosamente el código antiguo, NO veo que use es_candidata() de forma implícita. De hecho, hay una línea comentada que sugiere que SÍ debería usarla:\n",
    "python\n",
    "# LÍNEA COMENTADA en el código antiguo:\n",
    "#resultado.extend(tripletmanager.es_candidata([tripleta for tripleta in t if tripletmanager.es_candidata(tripleta,d,ner,topic,dbpedia)]))\n",
    "\n",
    "# LÍNEA ACTUAL (sin filtrado):\n",
    "resultado.extend(t)\n",
    "Esto significa que el código antiguo tampoco está aplicando filtrado, pero funciona mejor por otras razones.\n",
    "Análisis más profundo - Diferencias reales:\n",
    "1. Diferencia en el manejo de topics\n",
    "ANTIGUO:\n",
    "python\n",
    "topics = joblib.load('/Users/Raul/doctorado/semantic_oboe/semantic_oboe/bbc_objects/new_bbc_topics_7_sinprob')\n",
    "topic = topics.get(row_topic)  # Obtiene contenido específico del topic\n",
    "NUEVO:\n",
    "python\n",
    "topic = row.get('topic')  # Solo obtiene el nombre del topic\n",
    "2. Diferencia en el acceso a entidades DBpedia\n",
    "ANTIGUO:\n",
    "python\n",
    "dbpedia = list(row['entidades_dbpedia_simplificadas'].keys())  # Acceso directo\n",
    "ner = row['entidades']  # Acceso directo\n",
    "NUEVO:\n",
    "python\n",
    "# Código complejo que busca en diferentes columnas y puede fallar\n",
    "for dbpedia_col in ['entidades_dbpedia_simplificadas', 'entidades_dbpedia']:\n",
    "    if dbpedia_col in row and row[dbpedia_col]:\n",
    "        # ... procesamiento complejo\n",
    "3. Diferencia en la estructura de datos de entrada\n",
    "El problema principal puede estar en que el DataFrame de entrada es diferente entre ambas versiones. El código antiguo accede directamente a:\n",
    "row['entidades_dbpedia_simplificadas']\n",
    "row['entidades']\n",
    "row['new_target']\n",
    "row['text']\n",
    "Mientras que el nuevo busca en múltiples columnas posibles.\n",
    "4. Diferencia en el CoreNLP y procesamiento\n",
    "ANTIGUO:\n",
    "python\n",
    "for frase in row['text'].split('.'):  # Split simple\n",
    "    anotacion = client.annotate(frase)\n",
    "    sleep(.30)  # Sleep fijo\n",
    "NUEVO:\n",
    "python\n",
    "for frase in text_column.split('.'):  # Puede ser diferente columna\n",
    "    anotacion = client.annotate(frase)\n",
    "    sleep(sleep_time)  # Sleep variable\n",
    "Diagnóstico del problema real:\n",
    "Para encontrar la causa de las tripletas malformadas como use use They, necesitas:\n",
    "1. Verificar que el DataFrame de entrada tiene la estructura esperada:\n",
    "python\n",
    "def verificar_estructura_dataframe(df):\n",
    "    \"\"\"Verificar que el DataFrame tiene las columnas esperadas\"\"\"\n",
    "    print(f\"Columnas disponibles: {list(df.columns)}\")\n",
    "    \n",
    "    # Verificar columnas críticas\n",
    "    columnas_criticas = ['entidades_dbpedia_simplificadas', 'entidades', 'new_target', 'text']\n",
    "    for col in columnas_criticas:\n",
    "        if col in df.columns:\n",
    "            print(f\"✓ {col}: presente\")\n",
    "        else:\n",
    "            print(f\"✗ {col}: FALTA\")\n",
    "    \n",
    "    # Mostrar algunos ejemplos\n",
    "    if len(df) > 0:\n",
    "        print(\"\\nPrimer ejemplo:\")\n",
    "        print(f\"entidades: {df.iloc[0].get('entidades', 'NO ENCONTRADO')}\")\n",
    "        print(f\"text: {str(df.iloc[0].get('text', 'NO ENCONTRADO'))[:100]}...\")\n",
    "2. Añadir debug a devolver_tripletas() para ver exactamente qué se genera:\n",
    "python\n",
    "def devolver_tripletas(sentence, frase, pos, tripletgenerator):\n",
    "    \"\"\"Extrae tripletas de una oración usando diferentes métodos\"\"\"\n",
    "    print(f\"\\n=== PROCESANDO FRASE: {frase[:50]}... ===\")\n",
    "    \n",
    "    tripletas = []\n",
    "    \n",
    "    # Debug OpenIE\n",
    "    openie_count = len(sentence.openieTriple)\n",
    "    print(f\"OpenIE encontró {openie_count} tripletas\")\n",
    "    \n",
    "    for triple in sentence.openieTriple:\n",
    "        print(f\"OpenIE: {triple.subject} | {triple.relation} | {triple.object}\")\n",
    "        tripleta_dict = tripletgenerator.encapsular((triple.subject, triple.relation, triple.object), True)\n",
    "        tripletas.append(tripleta_dict)\n",
    "        print(f\"Encapsulada: {tripleta_dict}\")\n",
    "    \n",
    "    if len(tripletas) == 0:\n",
    "        print(\"No hay tripletas de OpenIE, probando triplet_extraction...\")\n",
    "        tripleta = tripletgenerator.triplet_extraction(frase, True)\n",
    "        print(f\"triplet_extraction retornó: {tripleta}\")\n",
    "        if tripleta is not None:\n",
    "            tripleta_dict = tripletgenerator.encapsular(tripleta, True)\n",
    "            print(f\"Encapsulada: {tripleta_dict}\")\n",
    "    \n",
    "    # Debug otros métodos\n",
    "    print(f\"Detectando patrones POS en: {pos}\")\n",
    "    \n",
    "    if tripletgenerator._detect_adj_nou(pos)[0] != -1:\n",
    "        print(\"Patrón ADJ-NOUN detectado\")\n",
    "        tripleta = tripletgenerator.generar_tripleta_adj_noun(pos)\n",
    "        print(f\"ADJ-NOUN generó: {tripleta}\")\n",
    "        if tripleta is not None:\n",
    "            tripleta_dict = tripletgenerator.encapsular(tripleta, True)\n",
    "            tripletas.append(tripleta_dict)\n",
    "            print(f\"ADJ-NOUN encapsulada: {tripleta_dict}\")\n",
    "    \n",
    "    # ... similar para otros métodos\n",
    "    \n",
    "    print(f\"TOTAL TRIPLETAS GENERADAS: {len(tripletas)}\")\n",
    "    for i, t in enumerate(tripletas):\n",
    "        print(f\"  {i}: {t}\")\n",
    "    \n",
    "    return tripletas\n",
    "3. Activar el filtrado en el código nuevo como estaba planeado en el antiguo:\n",
    "python\n",
    "# En generate_triplets_with_topics(), descomenta la línea de filtrado:\n",
    "def anotar_row(row, client, sleep_time, enable_filtering, filter_method):\n",
    "    # ... código existente ...\n",
    "    \n",
    "    # SIEMPRE crear tripletmanager para validación básica\n",
    "    tripletmanager = TripletManager()\n",
    "    \n",
    "    # ... obtener ner, dbpedia, topics ...\n",
    "    \n",
    "    for sentence in anotacion.sentence:\n",
    "        d = devolver_pos(sentence)\n",
    "        t = devolver_tripletas(sentence, frase, d, tripletgenerator)\n",
    "        \n",
    "        # APLICAR FILTRADO COMO ESTABA PLANEADO EN EL CÓDIGO ANTIGUO\n",
    "        tripletas_filtradas = []\n",
    "        for tripleta in t:\n",
    "            try:\n",
    "                if isinstance(tripleta, dict):\n",
    "                    tripleta_obj = Tripleta(tripleta)\n",
    "                else:\n",
    "                    tripleta_obj = tripleta\n",
    "                    \n",
    "                if tripletmanager.es_candidata(tripleta_obj, d, ner, topic, dbpedia):\n",
    "                    tripletas_filtradas.append(tripleta)\n",
    "                else:\n",
    "                    print(f\"FILTRADA: {tripleta}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error en es_candidata: {e}\")\n",
    "                continue\n",
    "                \n",
    "        resultado.extend(tripletas_filtradas)\n",
    "La diferencia clave no es que el antiguo use es_candidata() implícitamente, sino que probablemente:\n",
    "El DataFrame de entrada es diferente\n",
    "Los métodos de generación de tripletas están funcionando mal por cambios en las dependencias\n",
    "Falta el archivo de topics que el código antiguo sí carga\n",
    "La línea comentada en el código antiguo sugiere que debería usar filtrado, pero no lo hace. El código nuevo debería implementar ese filtrado.\n",
    "\n",
    "\n",
    "\n",
    "quité es_candidata porque el rendimiento bajaba muchísimo, hay alguna forma de arreglarlo?\n",
    "Editar\n",
    "¡Entiendo perfectamente! El método es_candidata() es muy pesado computacionalmente. Hay varias formas de optimizarlo o reemplazarlo con validaciones más ligeras:\n",
    "Opciones de optimización:\n",
    "1. Validación básica ligera (recomendado)\n",
    "Crear una función de validación simple que capture los casos más obvios sin la complejidad de es_candidata():\n",
    "python\n",
    "def validacion_rapida(tripleta_dict):\n",
    "    \"\"\"Validación básica y rápida para filtrar tripletas obviamente incorrectas\"\"\"\n",
    "    if not isinstance(tripleta_dict, dict):\n",
    "        return False\n",
    "    \n",
    "    subject = str(tripleta_dict.get('subject', '')).strip()\n",
    "    relation = str(tripleta_dict.get('relation', '')).strip()\n",
    "    object_val = str(tripleta_dict.get('object', '')).strip()\n",
    "    \n",
    "    # Filtros básicos rápidos\n",
    "    if not subject or not relation or not object_val:\n",
    "        return False\n",
    "    \n",
    "    # Evitar duplicados (como \"use use They\")\n",
    "    if subject.lower() == relation.lower() or subject.lower() == object_val.lower() or relation.lower() == object_val.lower():\n",
    "        return False\n",
    "    \n",
    "    # Evitar elementos muy cortos o solo números\n",
    "    if len(subject) < 2 or len(relation) < 2 or len(object_val) < 2:\n",
    "        return False\n",
    "    \n",
    "    if subject.isdigit() or relation.isdigit() or object_val.isdigit():\n",
    "        return False\n",
    "    \n",
    "    # Evitar caracteres extraños o repetitivos\n",
    "    if '**' in subject or '**' in relation or '**' in object_val:\n",
    "        return False\n",
    "    \n",
    "    # Evitar pronombres como sujetos de relaciones sin sentido\n",
    "    pronouns = {'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
    "    if subject.lower() in pronouns and relation.lower() == 'use':\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def anotar_row(row, client, sleep_time, enable_basic_filtering=True):\n",
    "    \"\"\"Versión optimizada con validación básica opcional\"\"\"\n",
    "    tripletgenerator = TripletGenerator()\n",
    "    resultado = []\n",
    "    \n",
    "    # ... resto del código de procesamiento ...\n",
    "    \n",
    "    for sentence in anotacion.sentence:\n",
    "        d = devolver_pos(sentence)\n",
    "        t = devolver_tripletas(sentence, frase, d, tripletgenerator)\n",
    "        \n",
    "        if enable_basic_filtering:\n",
    "            # Validación básica y rápida\n",
    "            tripletas_validas = [tripleta for tripleta in t if validacion_rapida(tripleta)]\n",
    "            resultado.extend(tripletas_validas)\n",
    "        else:\n",
    "            resultado.extend(t)\n",
    "    \n",
    "    return resultado\n",
    "2. Optimizar es_candidata() con caché y listas precompiladas\n",
    "Si quieres mantener es_candidata() pero optimizarla:\n",
    "python\n",
    "class TripletManagerOptimizado(TripletManager):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Precompilar listas para búsquedas más rápidas\n",
    "        self.sujeto_no_permitido_set = set(self.sujeto_no_permitido)\n",
    "        self.sujeto_permitido_set = set(self.sujeto_permitido) \n",
    "        self.predicado_permitido_set = set(self.predicado_permitido)\n",
    "        self.objeto_no_permitido_set = set(self.objeto_no_permitido)\n",
    "        self.objeto_permitido_set = set(self.objeto_permitido)\n",
    "        \n",
    "        # Cache para búsquedas POS\n",
    "        self.pos_cache = {}\n",
    "    \n",
    "    def buscar_pos_cached(self, palabra, pos):\n",
    "        \"\"\"Versión con caché de buscar_pos\"\"\"\n",
    "        cache_key = (palabra, tuple(sorted(pos.items())))\n",
    "        if cache_key in self.pos_cache:\n",
    "            return self.pos_cache[cache_key]\n",
    "        \n",
    "        result = self.buscar_pos(palabra, pos)\n",
    "        self.pos_cache[cache_key] = result\n",
    "        return result\n",
    "    \n",
    "    def es_candidata_optimizada(self, tripleta, pos, ner_set, topics_set, dbpedia_set):\n",
    "        \"\"\"Versión optimizada de es_candidata\"\"\"\n",
    "        \n",
    "        # Validación básica primero (más rápida)\n",
    "        if not validacion_rapida(tripleta.todict() if hasattr(tripleta, 'todict') else tripleta):\n",
    "            return False\n",
    "        \n",
    "        # Convertir a sets si no lo son ya (más rápido para intersecciones)\n",
    "        if not isinstance(ner_set, set):\n",
    "            ner_set = set(ner_set) if ner_set else set()\n",
    "        if not isinstance(topics_set, set):\n",
    "            topics_set = set(topics_set) if topics_set else set()\n",
    "        if not isinstance(dbpedia_set, set):\n",
    "            dbpedia_set = set(dbpedia_set) if dbpedia_set else set()\n",
    "        \n",
    "        # Vocabulario rápido con sets\n",
    "        palabras_tripleta = {tripleta.sujeto.lower(), tripleta.relacion.lower(), tripleta.objeto.lower()}\n",
    "        vocabulario_completo = ner_set | topics_set | dbpedia_set\n",
    "        \n",
    "        if not palabras_tripleta & vocabulario_completo:  # Intersección rápida\n",
    "            return False\n",
    "        \n",
    "        # Solo hacer validación POS si pasó el filtro de vocabulario\n",
    "        return self.validar_pos_optimizada(tripleta, pos)\n",
    "    \n",
    "    def validar_pos_optimizada(self, tripleta, pos):\n",
    "        \"\"\"Validación POS optimizada\"\"\"\n",
    "        # Validación más simple y rápida\n",
    "        sujeto_words = tripleta.sujeto.split()\n",
    "        if len(sujeto_words) == 1:\n",
    "            sujeto_pos = self.buscar_pos_cached(sujeto_words[0], pos)\n",
    "            if sujeto_pos in self.sujeto_no_permitido_set:\n",
    "                return False\n",
    "        \n",
    "        # Similar para objeto y predicado pero más simple\n",
    "        return True\n",
    "3. Filtrado en dos fases: rápido + completo selectivo\n",
    "python\n",
    "def anotar_row_bifasico(row, client, sleep_time, filtrado_completo_probabilidad=0.1):\n",
    "    \"\"\"\n",
    "    Filtrado en dos fases:\n",
    "    1. Siempre: validación básica rápida\n",
    "    2. Selectivo: validación completa solo en una fracción de casos\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    tripletgenerator = TripletGenerator()\n",
    "    resultado = []\n",
    "    \n",
    "    # Crear tripletmanager solo si se va a usar filtrado completo\n",
    "    tripletmanager = None\n",
    "    usar_filtrado_completo = random.random() < filtrado_completo_probabilidad\n",
    "    \n",
    "    if usar_filtrado_completo:\n",
    "        tripletmanager = TripletManagerOptimizado()\n",
    "        # ... preparar ner, topics, dbpedia ...\n",
    "    \n",
    "    for sentence in anotacion.sentence:\n",
    "        d = devolver_pos(sentence)\n",
    "        t = devolver_tripletas(sentence, frase, d, tripletgenerator)\n",
    "        \n",
    "        # Fase 1: Filtrado básico (siempre)\n",
    "        tripletas_basico = [tripleta for tripleta in t if validacion_rapida(tripleta)]\n",
    "        \n",
    "        # Fase 2: Filtrado complejo (selectivo)\n",
    "        if usar_filtrado_completo and tripletmanager:\n",
    "            tripletas_final = []\n",
    "            for tripleta_dict in tripletas_basico:\n",
    "                tripleta_obj = Tripleta(tripleta_dict)\n",
    "                if tripletmanager.es_candidata_optimizada(tripleta_obj, d, ner, topics, dbpedia):\n",
    "                    tripletas_final.append(tripleta_dict)\n",
    "            resultado.extend(tripletas_final)\n",
    "        else:\n",
    "            resultado.extend(tripletas_basico)\n",
    "    \n",
    "    return resultado\n",
    "4. Paralelización del filtrado\n",
    "python\n",
    "from multiprocessing import Pool\n",
    "import functools\n",
    "\n",
    "def validar_tripleta_worker(args):\n",
    "    \"\"\"Worker para validación paralela\"\"\"\n",
    "    tripleta_dict, pos, ner, topics, dbpedia = args\n",
    "    \n",
    "    if not validacion_rapida(tripleta_dict):\n",
    "        return None\n",
    "    \n",
    "    # Solo validación básica en paralelo\n",
    "    return tripleta_dict\n",
    "\n",
    "def anotar_row_paralelo(row, client, sleep_time, n_workers=2):\n",
    "    \"\"\"Versión con paralelización del filtrado\"\"\"\n",
    "    tripletgenerator = TripletGenerator()\n",
    "    todas_tripletas = []\n",
    "    \n",
    "    # Recopilar todas las tripletas primero\n",
    "    for frase in text_column.split('.'):\n",
    "        # ... procesamiento normal ...\n",
    "        for sentence in anotacion.sentence:\n",
    "            d = devolver_pos(sentence)\n",
    "            t = devolver_tripletas(sentence, frase, d, tripletgenerator)\n",
    "            todas_tripletas.extend([(tripleta, d, ner, topics, dbpedia) for tripleta in t])\n",
    "    \n",
    "    # Procesar en paralelo\n",
    "    with Pool(n_workers) as pool:\n",
    "        resultados = pool.map(validar_tripleta_worker, todas_tripletas)\n",
    "    \n",
    "    return [r for r in resultados if r is not None]\n",
    "Recomendación:\n",
    "Usa la opción 1 (validación básica ligera) porque:\n",
    "Es 100x más rápida que es_candidata()\n",
    "Filtra los casos más obviamente incorrectos como (use, use, They)\n",
    "Mantiene el rendimiento alto\n",
    "Es fácil de ajustar añadiendo más reglas según veas patrones problemáticos\n",
    "python\n",
    "# Añadir al argumento de línea de comandos:\n",
    "parser.add_argument('--enable_basic_filtering', action='store_true',\n",
    "                    default=True,\n",
    "                    help='Habilitar filtrado básico rápido (recomendado: True)')\n",
    "¿Te parece bien empezar con la validación básica rápida? Es mucho más eficiente y debería eliminar las tripletas malformadas que estás viendo.\n",
    "\n",
    "\n",
    "\n",
    "puedes hacer una opción 1 en la que ademas se haga una validación en base al pos como en es_candidata?\n",
    "Editar\n",
    "¡Perfecto! Aquí tienes una validación básica ligera que incluye validación POS optimizada:\n",
    "python\n",
    "def validacion_rapida_con_pos(tripleta_dict, pos):\n",
    "    \"\"\"\n",
    "    Validación básica y rápida que incluye validación POS optimizada\n",
    "    Inspirada en es_candidata() pero mucho más eficiente\n",
    "    \"\"\"\n",
    "    if not isinstance(tripleta_dict, dict):\n",
    "        return False\n",
    "    \n",
    "    subject = str(tripleta_dict.get('subject', '')).strip()\n",
    "    relation = str(tripleta_dict.get('relation', '')).strip()\n",
    "    object_val = str(tripleta_dict.get('object', '')).strip()\n",
    "    \n",
    "    # === VALIDACIONES BÁSICAS RÁPIDAS ===\n",
    "    if not subject or not relation or not object_val:\n",
    "        return False\n",
    "    \n",
    "    # Evitar duplicados (como \"use use They\")\n",
    "    if subject.lower() == relation.lower() or subject.lower() == object_val.lower() or relation.lower() == object_val.lower():\n",
    "        return False\n",
    "    \n",
    "    # Evitar elementos muy cortos o solo números\n",
    "    if len(subject) < 2 or len(relation) < 2 or len(object_val) < 2:\n",
    "        return False\n",
    "    \n",
    "    if subject.isdigit() or relation.isdigit() or object_val.isdigit():\n",
    "        return False\n",
    "    \n",
    "    # Evitar caracteres extraños o repetitivos\n",
    "    if '**' in subject or '**' in relation or '**' in object_val:\n",
    "        return False\n",
    "    \n",
    "    # === VALIDACIÓN POS OPTIMIZADA ===\n",
    "    \n",
    "    # Listas POS precompiladas (más rápido que crear cada vez)\n",
    "    sujeto_no_permitido = {'VB', 'VBN', 'VBP', 'VBZ', 'VBD', 'VH', 'CC', 'DT', 'EX', 'IN', 'RB', 'SYM', 'WRB', 'TO', 'UH', 'MD'}\n",
    "    sujeto_permitido = {'VBZ', 'NN', 'NNP', 'NNPS', 'NNS', 'FW', 'PRP', 'CD', 'WP', 'PDT'}\n",
    "    predicado_permitido = {'VB', 'VBN', 'VBP', 'VBZ', 'VBD', 'MD'}\n",
    "    objeto_no_permitido = {'VB', 'VBN', 'VBP', 'VBZ', 'VBD', 'CC', 'DT', 'EX', 'IN', 'RB', 'SYM', 'WRB', 'TO', 'UH', 'MD'}\n",
    "    objeto_permitido = {'VBZ', 'NN', 'NNP', 'NNPS', 'NNS', 'FW', 'CD', 'WP', 'RBR', 'RBS', 'JJ', 'JJS', 'JJR', 'PDT'}\n",
    "    \n",
    "    # Función auxiliar rápida para buscar POS\n",
    "    def buscar_pos_rapido(palabra, pos_dict):\n",
    "        \"\"\"Búsqueda POS optimizada - más simple que el método original\"\"\"\n",
    "        # Búsqueda directa primero\n",
    "        if palabra in pos_dict:\n",
    "            return pos_dict[palabra]\n",
    "        \n",
    "        # Búsqueda case-insensitive\n",
    "        palabra_lower = palabra.lower()\n",
    "        for key, value in pos_dict.items():\n",
    "            if key.lower() == palabra_lower:\n",
    "                return value\n",
    "        \n",
    "        # Casos especiales para verbos auxiliares\n",
    "        if palabra_lower in ['am', 'is', 'are', 'do', 'have', 'does', 'has', 'can']:\n",
    "            return 'VB'\n",
    "        \n",
    "        return '.'\n",
    "    \n",
    "    # === VALIDAR SUJETO ===\n",
    "    subject_words = subject.split()\n",
    "    if len(subject_words) == 1:\n",
    "        subject_pos = buscar_pos_rapido(subject_words[0], pos)\n",
    "        if subject_pos in sujeto_no_permitido:\n",
    "            return False\n",
    "        # Si no está en los permitidos y no es un caso especial, rechazar\n",
    "        if subject_pos not in sujeto_permitido and subject_pos != '.':\n",
    "            # Casos especiales para pronombres\n",
    "            if subject_pos == 'PRP' and subject.lower() in ['it', 'one', 'them', 'she', 'he', 'they']:\n",
    "                pass  # Permitir estos pronombres\n",
    "            elif subject_pos == 'PDT' and subject.lower() in ['all', 'these', 'that', 'those']:\n",
    "                pass  # Permitir estos determinantes\n",
    "            else:\n",
    "                return False\n",
    "    else:\n",
    "        # Para frases multi-palabra, verificar que al menos una palabra sea válida\n",
    "        valid_subject_found = False\n",
    "        for word in subject_words:\n",
    "            word_pos = buscar_pos_rapido(word, pos)\n",
    "            if word_pos in sujeto_permitido:\n",
    "                valid_subject_found = True\n",
    "                break\n",
    "        if not valid_subject_found:\n",
    "            return False\n",
    "    \n",
    "    # === VALIDAR PREDICADO/RELACIÓN ===\n",
    "    relation_words = relation.split()\n",
    "    if len(relation_words) == 1:\n",
    "        # Casos especiales para relaciones comunes\n",
    "        if relation in ['locatedAt', 'isAt']:\n",
    "            pass  # Permitir relaciones especiales\n",
    "        else:\n",
    "            relation_pos = buscar_pos_rapido(relation_words[0], pos)\n",
    "            if relation_pos not in predicado_permitido:\n",
    "                return False\n",
    "    else:\n",
    "        # Para predicados multi-palabra, verificar patrón verbo + preposición\n",
    "        first_word_pos = buscar_pos_rapido(relation_words[0], pos)\n",
    "        if first_word_pos not in predicado_permitido:\n",
    "            return False\n",
    "        \n",
    "        # Verificar que no contenga sujetos válidos (que invalidarían el predicado)\n",
    "        for word in relation_words:\n",
    "            word_pos = buscar_pos_rapido(word, pos)\n",
    "            if word_pos in sujeto_permitido:\n",
    "                return False\n",
    "    \n",
    "    # === VALIDAR OBJETO ===\n",
    "    object_words = object_val.split()\n",
    "    if len(object_words) == 1:\n",
    "        object_pos = buscar_pos_rapido(object_words[0], pos)\n",
    "        if object_pos in objeto_no_permitido:\n",
    "            return False\n",
    "        # Si no está en los permitidos, verificar casos especiales\n",
    "        if object_pos not in objeto_permitido and object_pos != '.':\n",
    "            # Casos especiales para pronombres como objetos\n",
    "            if object_pos == 'PRP' and object_val.lower() in ['it', 'one', 'them', 'her', 'him']:\n",
    "                pass  # Permitir estos pronombres como objetos\n",
    "            elif object_pos == 'PDT' and object_val.lower() in ['all', 'these', 'that', 'those']:\n",
    "                pass  # Permitir estos determinantes\n",
    "            else:\n",
    "                return False\n",
    "    else:\n",
    "        # Para objetos multi-palabra, verificar que al menos una palabra sea válida\n",
    "        valid_object_found = False\n",
    "        for word in object_words:\n",
    "            word_pos = buscar_pos_rapido(word, pos)\n",
    "            if word_pos in objeto_permitido:\n",
    "                valid_object_found = True\n",
    "                break\n",
    "        if not valid_object_found:\n",
    "            return False\n",
    "    \n",
    "    # === VALIDACIONES ADICIONALES ESPECÍFICAS ===\n",
    "    \n",
    "    # Evitar pronombres como sujetos de relaciones sin sentido\n",
    "    if subject.lower() in ['i', 'you', 'he', 'she', 'it', 'we', 'they'] and relation.lower() == 'use':\n",
    "        return False\n",
    "    \n",
    "    # Evitar relaciones obviamente incorrectas\n",
    "    if relation.lower() in ['the', 'a', 'an', 'this', 'that', 'these', 'those']:\n",
    "        return False\n",
    "    \n",
    "    # Evitar objetos que son claramente incorrectos\n",
    "    if object_val.lower() in ['the', 'a', 'an', 'and', 'or', 'but']:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def anotar_row_optimizado(row, client, sleep_time, enable_pos_filtering=True):\n",
    "    \"\"\"\n",
    "    Versión optimizada con validación básica + POS\n",
    "    \"\"\"\n",
    "    tripletgenerator = TripletGenerator()\n",
    "    resultado = []\n",
    "    \n",
    "    # Obtener información básica\n",
    "    topic = row.get('topic')\n",
    "    \n",
    "    # Procesar el texto\n",
    "    text_column = ''\n",
    "    for text_col in ['text_coref', 'coref_text', 'text', 'news']:\n",
    "        if text_col in row and row[text_col]:\n",
    "            text_column = str(row[text_col])\n",
    "            break\n",
    "    \n",
    "    if not text_column:\n",
    "        return resultado\n",
    "    \n",
    "    for frase in text_column.split('.'):\n",
    "        if not frase.strip():\n",
    "            continue\n",
    "        try:\n",
    "            anotacion = client.annotate(frase)\n",
    "            sleep(sleep_time)\n",
    "            \n",
    "            for sentence in anotacion.sentence:\n",
    "                d = devolver_pos(sentence)\n",
    "                t = devolver_tripletas(sentence, frase, d, tripletgenerator)\n",
    "                \n",
    "                if enable_pos_filtering:\n",
    "                    # Aplicar validación básica + POS\n",
    "                    tripletas_validas = []\n",
    "                    for tripleta in t:\n",
    "                        if validacion_rapida_con_pos(tripleta, d):\n",
    "                            tripletas_validas.append(tripleta)\n",
    "                        else:\n",
    "                            # Debug opcional: mostrar tripletas rechazadas\n",
    "                            if isinstance(tripleta, dict):\n",
    "                                print(f\"RECHAZADA: {tripleta.get('subject')} | {tripleta.get('relation')} | {tripleta.get('object')}\")\n",
    "                    \n",
    "                    resultado.extend(tripletas_validas)\n",
    "                else:\n",
    "                    # Solo validación básica sin POS\n",
    "                    tripletas_validas = [tripleta for tripleta in t if validacion_rapida_con_pos(tripleta, {})]\n",
    "                    resultado.extend(tripletas_validas)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando frase '{frase[:50]}...': {e}\")\n",
    "            continue\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "\n",
    "# Modificar la función transformar_partition para usar la nueva validación\n",
    "def transformar_partition_optimizado(df, client, sleep_time, enable_pos_filtering):\n",
    "    \"\"\"\n",
    "    Transforma una partición del dataframe aplicando anotar_row_optimizado a cada fila\n",
    "    \"\"\"\n",
    "    return df.apply(\n",
    "        lambda row: anotar_row_optimizado(row, client, sleep_time, enable_pos_filtering),\n",
    "        axis=1\n",
    "    )\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b21314-8e9d-4a02-b778-e42225778850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python semoboe",
   "language": "python",
   "name": "semoboe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
