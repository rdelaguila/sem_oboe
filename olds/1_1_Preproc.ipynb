{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ece765a-b5c5-4b37-86d3-f6f3872eb702",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .jp-icon-warn0 path {fill: var(--jp-warn-color0);} .bp3-button-text path { fill: var(--jp-inverse-layout-color3);} .jp-icon-brand0 path { fill: var(--jp-brand-color0);} text.terms { fill: #616161;} </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import time \n",
    "import joblib\n",
    "from IPython.display import HTML\n",
    "css_str = '<style> \\\n",
    ".jp-icon-warn0 path {fill: var(--jp-warn-color0);} \\\n",
    ".bp3-button-text path { fill: var(--jp-inverse-layout-color3);} \\\n",
    ".jp-icon-brand0 path { fill: var(--jp-brand-color0);} \\\n",
    "text.terms { fill: #616161;} \\\n",
    "</style>'\n",
    "display(HTML(css_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28164b03-bb00-4432-9408-6a2083ca3a52",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13ff1759-07b0-4185-8ef1-3c05f3dcad04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7218de03-6b39-4ae0-9dcb-f17310c535d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b1221a-ae5b-4a6b-ad5f-d4226e3cd17e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997325cb-1954-4b02-8308-d2401c98e25d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca0fa7-7450-47f7-8144-1e68c8472f13",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473dc11b-c7d4-434a-a66b-89155ea74744",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e5557-3042-4e7b-b70a-a15546cda9a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def resolve_coref(text):\n",
    "        return text._.coref_resolved\n",
    "\n",
    "def process_chunk_corefs(docs):\n",
    "    coref_texts = []\n",
    "    for doc in nlp.pipe(docs, batch_size = 20):\n",
    "        coref_texts.append(resolve_coref(doc))\n",
    "    return coref_texts\n",
    "\n",
    "def clean(text, lemma=False):\n",
    "    doc = nlp(text, )\n",
    "    if lemma:\n",
    "        tokens = \" \".join(token.lemma_ for token in doc if token_filter(token))\n",
    "    else:\n",
    "        tokens = \" \".join(token.text for token in doc if token_filter(token))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def to_nlp(text):\n",
    "    return nlp(text)\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"Flatten a list of lists to a combined list\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "\n",
    "def clean(text, lemma=False):\n",
    "    doc = nlp(text, )\n",
    "    if lemma:\n",
    "        tokens = \" \".join(token.lemma_ for token in doc if token_filter(token))\n",
    "    else:\n",
    "        tokens = \" \".join(token.text for token in doc if token_filter(token))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def preprocess_parallel_corefs(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk_corefs)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d39bd-ab4c-4ffe-b021-05ecf6caf599",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BBC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c182818-aa47-4f72-8427-0890b920e337",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc = pd.read_csv('../../corpus/bbc-text.csv')\n",
    "bbc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0c672-22d5-4627-8437-8542e871bec1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b4694-9331-43d5-b07e-0c8acd0f1fe0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2 = bbc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af2a43-f298-45a5-954f-2131d8c20671",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2['coref_text']=preprocess_parallel_corefs(bbc2['text'], chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c322243-e4c8-498a-83a8-c4cdb42b370f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2.iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c067e-fdf4-45c1-960f-7b908e1f14c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(bbc2,'bbc_coref_resolved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c26745-c39b-492a-9fae-70fabfbf0716",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Additional Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b1ead-3145-40ca-81c9-03698424af92",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Remove special characters and stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cdead-6d15-4215-882b-ec330541527d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def token_filter(token):\n",
    "    return not (token.is_punct | token.is_space | token.is_stop)\n",
    "\n",
    "def process_chunk_clean(texts):\n",
    "    preproc_pipe = []\n",
    "\n",
    "    for doc in nlp.pipe(texts, batch_size=200):\n",
    "        tokens = \" \".join(token.lemma_ for token in doc if token_filter(token))\n",
    "        preproc_pipe.append(tokens)\n",
    "    return preproc_pipe\n",
    "    \n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    print(\"Flatten a list of lists to a combined list\")\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def preprocess_parallel_clean(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=-1, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk_clean)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)\n",
    "\n",
    "def process_chunk_ner(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=200):\n",
    "        entidades = set()\n",
    "        if doc.ents:\n",
    "            for ent in doc.ents:\n",
    "                entidades.add(ent.text)\n",
    "        preproc_pipe.append(entidades)\n",
    "       \n",
    "    return preproc_pipe\n",
    "    \n",
    "    \n",
    "def preprocess_parallel_ner (texts, chunksize = 100):\n",
    "    executor = Parallel(n_jobs=-1, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk_ner)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c850b-5052-48e6-b9ce-a31650196f65",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f75fe-f13c-4917-ab1e-5975200614f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2['cleaned'] = preprocess_parallel_clean(bbc2['coref_text'], chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba93bf-9102-4b7a-9700-395db93c7ab8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(bbc2,'bbc_coref_resolved_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5086b4c-8bf5-4d6e-930e-afa89b3e458d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb2b1a-1286-4763-a258-3e8b0daee779",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85395b26-2939-4f2c-85ca-e1e051f33967",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2 = joblib.load('bbc_coref_resolved_cleaned')\n",
    "bbc2['entidades'] = preprocess_parallel_ner(bbc2['coref_text'], chunksize=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161459af-fc8e-4be3-ab3f-0c6efef4de9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6185af56-1f55-4e03-91fd-2a1a493aefab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(bbc2,'bbc_objects/bbc_coref_resolved_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c58d1320-4381-40bd-9573-37e71fe012b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>coref_text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>entidades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "      <td>{nine months to a years  , five years  time, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss leave book worldcom boss bernie ...</td>\n",
       "      <td>{monday, about $180bn, last two months, 20 000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tiger wary farrell gamble leicester tiger wary...</td>\n",
       "      <td>{another three months, five weeks ago}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeade face newcastle fa cup premiership newcas...</td>\n",
       "      <td>{sunday, two, the weekend, second, earlier thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s raid box office ocean s crime caper se...</td>\n",
       "      <td>{$184m, 57.2, 2001, one, five, december, $110m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>car pull retail figure retail sale fall 0.3 ja...</td>\n",
       "      <td>{0.3%, 3.3%, 2004, the first quarter, 2005, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "      <td>kilroy unveil immigration policy ex chatshow h...</td>\n",
       "      <td>{2bn, 14 000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>rem announce new glasgow concert band rem anno...</td>\n",
       "      <td>{four days later, 29, 21, tuesday  14, 10 000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>political squabble snowball s commonplace argu...</td>\n",
       "      <td>{two, 283 000, one, the last couple of decades...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>souness delight euro progress boss graeme soun...</td>\n",
       "      <td>{last 16, this year, the first half, 200, next...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text  \\\n",
       "0              tech  tv future in the hands of viewers with home th...   \n",
       "1          business  worldcom boss  left books alone  former worldc...   \n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3             sport  yeading face newcastle in fa cup premiership s...   \n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "...             ...                                                ...   \n",
       "2220       business  cars pull down us retail figures us retail sal...   \n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...   \n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...   \n",
       "2223       politics  how political squabbles snowball it s become c...   \n",
       "2224          sport  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "                                             coref_text  \\\n",
       "0     tv future in the hands of viewers with home th...   \n",
       "1     worldcom boss  left books alone  former worldc...   \n",
       "2     tigers wary of farrell  gamble  leicester say ...   \n",
       "3     yeading face newcastle in fa cup premiership s...   \n",
       "4     ocean s twelve raids box office ocean s twelve...   \n",
       "...                                                 ...   \n",
       "2220  cars pull down us retail figures us retail sal...   \n",
       "2221  kilroy unveils immigration policy ex-chatshow ...   \n",
       "2222  rem announce new glasgow concert us band rem h...   \n",
       "2223  how political squabbles snowball it s become c...   \n",
       "2224  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0     tv future hand viewer home theatre system plas...   \n",
       "1     worldcom boss leave book worldcom boss bernie ...   \n",
       "2     tiger wary farrell gamble leicester tiger wary...   \n",
       "3     yeade face newcastle fa cup premiership newcas...   \n",
       "4     ocean s raid box office ocean s crime caper se...   \n",
       "...                                                 ...   \n",
       "2220  car pull retail figure retail sale fall 0.3 ja...   \n",
       "2221  kilroy unveil immigration policy ex chatshow h...   \n",
       "2222  rem announce new glasgow concert band rem anno...   \n",
       "2223  political squabble snowball s commonplace argu...   \n",
       "2224  souness delight euro progress boss graeme soun...   \n",
       "\n",
       "                                              entidades  \n",
       "0     {nine months to a years  , five years  time, a...  \n",
       "1     {monday, about $180bn, last two months, 20 000...  \n",
       "2                {another three months, five weeks ago}  \n",
       "3     {sunday, two, the weekend, second, earlier thi...  \n",
       "4     {$184m, 57.2, 2001, one, five, december, $110m...  \n",
       "...                                                 ...  \n",
       "2220  {0.3%, 3.3%, 2004, the first quarter, 2005, 0....  \n",
       "2221                                      {2bn, 14 000}  \n",
       "2222  {four days later, 29, 21, tuesday  14, 10 000 ...  \n",
       "2223  {two, 283 000, one, the last couple of decades...  \n",
       "2224  {last 16, this year, the first half, 200, next...  \n",
       "\n",
       "[2225 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc2=joblib.load('bbc_objects/bbc_coref_resolved_cleaned')\n",
    "bbc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a878b28e-f83e-474a-8454-3b1beae3bb33",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bbc_objects/bbc_dataset_raw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_7029/78156479.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0md2\u001B[0m  \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'bbc_objects/bbc_dataset_raw.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/kge/lib/python3.7/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/kge/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/kge/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    481\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 482\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    483\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    484\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/kge/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 811\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/kge/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1038\u001B[0m             )\n\u001B[1;32m   1039\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/kge/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/kge/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m    227\u001B[0m             \u001B[0mmemory_map\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"memory_map\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m             \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"storage_options\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 229\u001B[0;31m             \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"encoding_errors\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"strict\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    230\u001B[0m         )\n\u001B[1;32m    231\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/kge/lib/python3.7/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    705\u001B[0m                 \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    706\u001B[0m                 \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 707\u001B[0;31m                 \u001B[0mnewline\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    708\u001B[0m             )\n\u001B[1;32m    709\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'bbc_objects/bbc_dataset_raw.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d2  = pd.read_csv('bbc_objects/bbc_dataset_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364d850f-c33b-4d3d-ad63-da4d641581c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_7029/4206307747.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0md2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'd2' is not defined"
     ]
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33bf264-cf2e-4dd3-a715-f693305ece23",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c71a1-20a4-4855-8a4b-540e2c0f830c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c03ac9-9b52-4bd7-bb9b-7209c1c0b427",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4723e79-9a7d-4538-a568-89badcfc5f7a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# bbc_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fef58c91-125f-4d12-8b1e-86c64f31e540",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc = pd.read_csv('../../corpus/bbc_dataset_raw.csv', encoding = \"ISO-8859-1\")\n",
    "bbc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14ca6f1b-1d28-4b14-8aa5-fd551137f9f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news      type\n",
       "0  China had role in Yukos split-up\\n \\n China le...  business"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aeed4b-884b-4e2c-8460-b6a718b31e6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2 = bbc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1eb38-6cd0-45f5-bbff-0f1a467de8fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2['coref_text']=preprocess_parallel_corefs(bbc2['news'], chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f3a45-247c-41b8-9fce-e7e70fe48dfe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2.iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e73c56-8cdb-42f4-8142-44106e9d8df7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(bbc2,'../../corpus/processed/bbc_raw_coref_resolved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03b5a736-5ffd-4215-a92a-387d52d8d88d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2 = joblib.load('../../corpus/processed/bbc_raw_coref_resolved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa1d13d1-a9de-4161-aa46-d362cecd76a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b02522e0-7570-4012-9528-17e4f8657d2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>type</th>\n",
       "      <th>coref_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "      <td>business</td>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "      <td>business</td>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "      <td>business</td>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "      <td>business</td>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
       "      <td>business</td>\n",
       "      <td>The SEC have agreed to pay a total of $515m (Â...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Microsoft launches its own search\\n \\n Microso...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft launches Microsoft own search\\n \\n M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Warnings about junk mail deluge\\n \\n The amoun...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Warnings about junk mail deluge\\n \\n The amoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Microsoft gets the blogging bug\\n \\n Software ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Microsoft gets the blogging bug\\n \\n Software ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Gamers snap up new Sony PSP\\n \\n Gamers have b...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Gamers snap up new Sony PSP\\n \\n Gamers have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n \\n The App...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n \\n The App...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news      type  \\\n",
       "0     China had role in Yukos split-up\\n \\n China le...  business   \n",
       "1     Oil rebounds from weather effect\\n \\n Oil pric...  business   \n",
       "2     Indonesia 'declines debt freeze'\\n \\n Indonesi...  business   \n",
       "3     $1m payoff for former Shell boss\\n \\n Shell is...  business   \n",
       "4     US bank in $515m SEC settlement\\n \\n Five Bank...  business   \n",
       "...                                                 ...       ...   \n",
       "2220  Microsoft launches its own search\\n \\n Microso...      tech   \n",
       "2221  Warnings about junk mail deluge\\n \\n The amoun...      tech   \n",
       "2222  Microsoft gets the blogging bug\\n \\n Software ...      tech   \n",
       "2223  Gamers snap up new Sony PSP\\n \\n Gamers have b...      tech   \n",
       "2224  Apple laptop is 'greatest gadget'\\n \\n The App...      tech   \n",
       "\n",
       "                                             coref_text  \n",
       "0     China had role in Yukos split-up\\n \\n China le...  \n",
       "1     Oil rebounds from weather effect\\n \\n Oil pric...  \n",
       "2     Indonesia 'declines debt freeze'\\n \\n Indonesi...  \n",
       "3     $1m payoff for former Shell boss\\n \\n Shell is...  \n",
       "4     The SEC have agreed to pay a total of $515m (Â...  \n",
       "...                                                 ...  \n",
       "2220  Microsoft launches Microsoft own search\\n \\n M...  \n",
       "2221  Warnings about junk mail deluge\\n \\n The amoun...  \n",
       "2222  Microsoft gets the blogging bug\\n \\n Software ...  \n",
       "2223  Gamers snap up new Sony PSP\\n \\n Gamers have b...  \n",
       "2224  Apple laptop is 'greatest gadget'\\n \\n The App...  \n",
       "\n",
       "[2225 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba560547-8052-492f-8556-48e07d265ba1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Additional Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ebeb3-e860-4d3c-b82d-4a32829617ec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Remove special characters and stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb770d-082b-404a-8858-6b63112f336e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def token_filter(token):\n",
    "    return not (token.is_punct | token.is_space | token.is_stop)\n",
    "\n",
    "def process_chunk_clean(texts):\n",
    "    preproc_pipe = []\n",
    "\n",
    "    for doc in nlp.pipe(texts, batch_size=200):\n",
    "        tokens = \" \".join(token.lemma_.lower() for token in doc if token_filter(token))\n",
    "        preproc_pipe.append(tokens)\n",
    "    return preproc_pipe\n",
    "    \n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    print(\"Flatten a list of lists to a combined list\")\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def preprocess_parallel_clean(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=-1, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk_clean)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)\n",
    "\n",
    "def process_chunk_ner(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=200):\n",
    "        entidades = set()\n",
    "        if doc.ents:\n",
    "            for ent in doc.ents:\n",
    "                print(ent.text)\n",
    "                print(type(ent))\n",
    "                if ent.ent_type_!='ORDINAL' and ent.ent_type_!='CARDINAL' and ent.ent_type_!='TIME':\n",
    "                    entidades.add(ent.text.lower())\n",
    "        preproc_pipe.append(entidades)\n",
    "       \n",
    "    return preproc_pipe\n",
    "    \n",
    "    \n",
    "def preprocess_parallel_ner (texts, chunksize = 100):\n",
    "    executor = Parallel(n_jobs=-1, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk_ner)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655cef15-9925-4924-a424-a07be3dc2a2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e561715-1e8b-47bf-a68b-a4a04906db5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2['cleaned'] = preprocess_parallel_clean(bbc2['coref_text'], chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8063c-fcfb-4862-b46f-1bc2bb153ece",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(bbc2,'../../corpus/processed/bbc_coref_resolved_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660d183-cac0-4557-9f23-961effebe5f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "bbc2 = joblib.load('../../corpus/processed/bbc_coref_resolved_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd793a-39fa-4f46-afb4-53bb4e47d481",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## NER: tenemos que hacerlo sobre el original para evitar que no lo coja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef542f-a22b-41d6-b595-715e6e962907",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c115f9a-50aa-43a5-99b8-65084045b083",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2['entidades'] = preprocess_parallel_ner(bbc2['coref_text'], chunksize=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c438383-b38e-419a-9142-2a66daf07ad7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(bbc2,'../../corpus/processed/bbc_coref_resolved_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309668f8-0f9b-46b9-b227-2b41741f685d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19663685-804b-456b-a717-39783261a924",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "df = joblib.load ('../../corpus/processed/bbc_coref_resolved_cleaned')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07134808-3fa3-45ff-a88a-041dcc8c12d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entidades = df['entidades']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64196193-c473-420a-9caf-a348d7ffe736",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entidad_set = set()\n",
    "for entidad in entidades:\n",
    "    for e in entidad:\n",
    "        entidad_set.add(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac1d70-d5fc-4075-90cc-b535ddf411bc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entidad_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4d5dd-9953-4ab2-9429-c84177c6eb7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(entidad_set,'./bbc_objects/entidades_bbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff144d19-a9e4-40da-8140-aece0cabd327",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "ents = joblib.load('./bbc_objects/entidades_bbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27575f8a-381e-4a4d-9bc1-0e53907ae8a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a2e862-bbd7-4fa7-a3d8-712aa170143d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for ent in doc:\n",
    "    print(ent.label_.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6155e-5239-4778-ac25-427df7492b6c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Crear tabla hash entidades tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f62aa25-d135-494a-806e-4c9839e1cc91",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def token_filter(token):\n",
    "    return not (token.is_punct | token.is_space | token.is_stop)\n",
    "\n",
    "def process_chunk_clean(texts):\n",
    "    preproc_pipe = []\n",
    "\n",
    "    for doc in nlp.pipe(texts, batch_size=200):\n",
    "        tokens = \" \".join(token.lemma_.lower() for token in doc if token_filter(token))\n",
    "        preproc_pipe.append(tokens)\n",
    "    return preproc_pipe\n",
    "    \n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    print(\"Flatten a list of lists to a combined list\")\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def preprocess_parallel_clean(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=-1, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk_clean)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)\n",
    "\n",
    "def process_chunk_hash_ner(texts):\n",
    "    preproc_pipe = []\n",
    "    diccionario = dict()\n",
    "    \n",
    "    for doc in nlp.pipe(texts, batch_size=200):\n",
    "        entidades = set()\n",
    "        if doc.ents:\n",
    "            for ent in doc.ents:\n",
    "#                print(ent.text)\n",
    "#                print(type(ent))\n",
    "#                print(dir(ent))\n",
    "                if ent.label_!='ORDINAL' and ent.label_!='CARDINAL' and ent.label_!='TIME':\n",
    "                    entidades.add(ent.text.lower())\n",
    "                    diccionario[ent.text.lower()] = ent.label_\n",
    "        preproc_pipe.append(diccionario)\n",
    "       \n",
    "    return preproc_pipe\n",
    "    \n",
    "    \n",
    "def preprocess_parallel_hash_ner (texts, chunksize = 100):\n",
    "    executor = Parallel(n_jobs=-1, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk_hash_ner)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e060ebf-073c-49a2-b5e3-a9ff141fd8fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "122dd95e-4d6a-4679-8707-ae9b6a06cdfd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc2 = joblib.load ('../../corpus/processed/bbc_coref_resolved_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d92750ce-4b76-4f6f-ac19-35d103145a99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "L = preprocess_parallel_hash_ner(bbc2['coref_text'], chunksize=250)\n",
    "joblib.dump(L,'./bbc_objects/listado_diccionario_ner')\n",
    "result = {}\n",
    "for d in L:\n",
    "    result.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165f5f8-6ac7-46ed-83fb-e0f3c1bef6de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(result,'./bbc_objects/diccionario_ner')\n",
    "os.remove('./bbc_objects/listado_diccionario_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a3e0c-c4b6-4bfa-a7a3-b9b73f870672",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac0c479d-3e6a-487b-9624-457beaa222b5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spotlight: DBPedia ENtities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b52c2-8264-485e-8337-4a62c0ead3cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import spotlight\n",
    "df = joblib.load('./bbc_objects/bbc_coref_resolved_cleaned')\n",
    "def anotar(text):\n",
    "    #print(text)\n",
    "    return spotlight.annotate(address=\"http://172.17.0.1:2222/rest/annotate\",text=text, confidence=0.5, support=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b52e7-fc28-489e-9c31-213411a82197",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "%time\n",
    "resultados = df.coref_text.apply(anotar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c6baf-ba8d-4cd0-8457-73658994bd01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.coref_text[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c5020-3c5c-4b64-8d53-e8ef541e3d6b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resultados[1000]\n",
    "class hashabledict(dict):\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.items())))\n",
    "\n",
    "res = hashabledict(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11d7df-5d16-4bae-b664-c87a5be9b9a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(res,'./bbc_objects/entidadesdbpedia_bbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411c9965-7e5c-419a-bde2-95cf78c9cc1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module '__main__' has no attribute 'hashabledict'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjoblib\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mjoblib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../bbc_objects/entidadesdbpedia_bbc\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/kge/lib/python3.10/site-packages/joblib/numpy_pickle.py:587\u001B[0m, in \u001B[0;36mload\u001B[0;34m(filename, mmap_mode)\u001B[0m\n\u001B[1;32m    581\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fobj, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    582\u001B[0m                 \u001B[38;5;66;03m# if the returned file object is a string, this means we\u001B[39;00m\n\u001B[1;32m    583\u001B[0m                 \u001B[38;5;66;03m# try to load a pickle file generated with an version of\u001B[39;00m\n\u001B[1;32m    584\u001B[0m                 \u001B[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001B[39;00m\n\u001B[1;32m    585\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m load_compatibility(fobj)\n\u001B[0;32m--> 587\u001B[0m             obj \u001B[38;5;241m=\u001B[39m \u001B[43m_unpickle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmmap_mode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "File \u001B[0;32m~/miniforge3/envs/kge/lib/python3.10/site-packages/joblib/numpy_pickle.py:506\u001B[0m, in \u001B[0;36m_unpickle\u001B[0;34m(fobj, filename, mmap_mode)\u001B[0m\n\u001B[1;32m    504\u001B[0m obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    505\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 506\u001B[0m     obj \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    507\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m unpickler\u001B[38;5;241m.\u001B[39mcompat_mode:\n\u001B[1;32m    508\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe file \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has been generated with a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    509\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoblib version less than 0.10. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    510\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease regenerate this pickle file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    511\u001B[0m                       \u001B[38;5;241m%\u001B[39m filename,\n\u001B[1;32m    512\u001B[0m                       \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/kge/lib/python3.10/pickle.py:1213\u001B[0m, in \u001B[0;36m_Unpickler.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1211\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m\n\u001B[1;32m   1212\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, bytes_types)\n\u001B[0;32m-> 1213\u001B[0m         \u001B[43mdispatch\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _Stop \u001B[38;5;28;01mas\u001B[39;00m stopinst:\n\u001B[1;32m   1215\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m stopinst\u001B[38;5;241m.\u001B[39mvalue\n",
      "File \u001B[0;32m~/miniforge3/envs/kge/lib/python3.10/pickle.py:1529\u001B[0m, in \u001B[0;36m_Unpickler.load_global\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1527\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1528\u001B[0m name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1529\u001B[0m klass \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mappend(klass)\n",
      "File \u001B[0;32m~/miniforge3/envs/kge/lib/python3.10/pickle.py:1584\u001B[0m, in \u001B[0;36m_Unpickler.find_class\u001B[0;34m(self, module, name)\u001B[0m\n\u001B[1;32m   1582\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _getattribute(sys\u001B[38;5;241m.\u001B[39mmodules[module], name)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1583\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1584\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodules\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module '__main__' has no attribute 'hashabledict'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "df = joblib.load('../bbc_objects/entidadesdbpedia_bbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857807f5-ec98-4614-a4b7-3607e84b2779",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759bb3d-94c7-4d72-b1ae-2a0f7934dfb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus = joblib.load('./bbc_objects/bbc_processed_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211bfa5-c4a7-4db0-a5b4-51deca8f8abb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "resultados = joblib.load('./bbc_objects/entidadesdbpedia_bbc')\n",
    "resultados[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00732784-83d2-470a-8a74-92ab3c97ada0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus['entidades_dbpedia'] = resultados\n",
    "joblib.dump(corpus,'./bbc_objects/bbc_processed_final_semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ee15e-771b-409d-932d-2c92a09cf2b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39230a2d-1e7c-4132-a716-bec1151daff4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3856cf-d984-44f8-a368-dadaaf55d3a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a55b4-2d58-44fc-b079-28fe9193ba55",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = joblib.load('./bbc_objects/bbc_coref_resolved_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1371b93-a358-4aa4-b855-a0c81c662a0c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resultados[10][1].get('URI')\n",
    "resultados[10][1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319edf3b-85eb-4d27-8fb9-d8a16d611d39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus.iloc[2,6].remove(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3e3de-2e6e-41ba-bf4b-d7ebed06f7b9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ANalisis entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b12eda-580c-4b4a-9afc-8935873c93cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus1 = joblib.load('./bbc_objects/bbc_processed_final_semantic')\n",
    "corpus1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca154b8-56f9-4f08-9578-ae7a171d3160",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus2 = joblib.load('../../corpus/processed/bbc_coref_resolved_cleaned')\n",
    "corpus2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32530901-a35d-4084-829a-70a11c52e3d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e601bf1-81bc-4c73-9273-03501569ab57",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "listaindices = []\n",
    "for i1,row1 in corpus1.iterrows():\n",
    "    minimo = 100000000\n",
    "    actual = 0\n",
    "    for i2, row2 in corpus2.iterrows():\n",
    "        if i1 == i2:\n",
    "            continue\n",
    "        distancia = editdistance.eval(row1['cleaned'],row2['cleaned'])\n",
    "        if distancia < minimo:\n",
    "            actual = i2\n",
    "            minimo = distancia\n",
    "    listaindices.append(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8376af-9053-4632-b148-ff8c8e350fa0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(listaindices,'correspondencias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82381b31-a489-410f-a9d1-41441d3ef9aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus1.cleaned[1982]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d340e-7690-439e-a94d-4d02ae5e3837",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus2.cleaned[listaindices[1982]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3446d50-e877-47a5-b671-c2c7012d7d87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685fcf0-dd83-4de1-87b6-ce56c2ef3c1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f8693-1975-4d05-8042-d34929da7492",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus1 = joblib.load('./bbc_objects/bbc_processed_final_semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5edd05-9f3b-4f76-8b59-ead165f833c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee710ba-87f1-4599-bfc0-aa093f6321f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "correspondencias = joblib.load('bbc_objects/correspondencias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fb633-5c94-4cad-9400-2aa3804198cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus2 = joblib.load('../../corpus/processed/bbc_coref_resolved_cleaned')\n",
    "corpus2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a55900e-4b01-4249-b803-56d6edcb25ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "listado_entidades_originales = corpus2.loc[:,'entidades'][correspondencias]\n",
    "listado_entidades_originales.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e38b99-54b8-462e-8762-53fcc97aa45b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus1['entidades']=listado_entidades_originales.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8356e-d925-4944-94ad-37950783e1c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(corpus1,'./bbc_objects/bbc_processed_final_semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71eb6d-73d9-47b0-95e6-0eaf76e4e109",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d81a42-1918-4948-853a-b08c534a5b65",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus1.iloc[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a66374-8128-4247-b9c5-c92f55cc12f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DESPUES DE TOPIC MODELING: Añadimos informacion dbpedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5838cf68-4592-4cf8-9cc1-10d7a97604f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resultados = joblib.load('./bbc_objects/entidadesdbpedia_bbc-Copy1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defdb09b-159a-4267-bb79-1c3ac8f7e249",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correspondencias' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_5952/1854300346.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcorpus1\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'entidades_dbpedia'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresultados\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcorrespondencias\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'correspondencias' is not defined"
     ]
    }
   ],
   "source": [
    "corpus1['entidades_dbpedia']=resultados[correspondencias].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f1aa55-f652-452c-b174-babbd2b1c86c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(corpus1,'./bbc_objects/bbc_processed_final_semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71fc69b0-8ee9-4d35-a6e3-aaf57fc8722b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_5952/4182636057.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcorpus1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'corpus1' is not defined"
     ]
    }
   ],
   "source": [
    "corpus1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2eab8-4b18-4c57-a301-0237ce4290c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Nos quedamos con la informacion util de las entidades dbpedia: uri, surfaceForm y Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a3417c9-819d-48d9-a526-7fa0e7a944f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "corpus1 = joblib.load('./bbc_objects/bbc_processed_final_semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53da610-8c13-47c6-9fc3-916aeb38abb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92465dc5-2a27-42b4-a013-ff3f62757a9d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "818b19d8-64f6-477c-97c4-4971407d45a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>coref_text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>entidades</th>\n",
       "      <th>new_target</th>\n",
       "      <th>entidades_dbpedia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "      <td>{Adam Hume, Older, Windows, one, Europe, Tim H...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Televisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss leave book worldcom boss bernie ...</td>\n",
       "      <td>{Monday, about $180bn, Reid Weingarten, last t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/MCI_Inc....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tiger wary farrell gamble leicester tiger wary...</td>\n",
       "      <td>{England, BBC Radio Leicester, five weeks ago,...</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Colin_Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeade face newcastle fa cup premiership newcas...</td>\n",
       "      <td>{Championship, Sheffield United, Sheff Utd, ea...</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Yeading_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s raid box office ocean s crime caper se...</td>\n",
       "      <td>{$184m, Julia Roberts, Blade:, 2001, Twelve, o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Crime_fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>car pull retail figure retail sale fall 0.3 ja...</td>\n",
       "      <td>{January, Popular Securities, just 0.3%, Chris...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Car', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "      <td>kilroy unveil immigration policy ex chatshow h...</td>\n",
       "      <td>{Robert Kilroy-Silk, Â£2bn, Veritas, 14,000, t...</td>\n",
       "      <td>6</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Robert_K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>rem announce new glasgow concert band rem anno...</td>\n",
       "      <td>{Tuesday, Europe, Glasgow, Balloch Castle Coun...</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Glasgow'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>political squabble snowball s commonplace argu...</td>\n",
       "      <td>{Blair, Google, two, Blair and Brown, America,...</td>\n",
       "      <td>6</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Snowball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>souness delight euro progress boss graeme soun...</td>\n",
       "      <td>{Laurent Robert, Euro, Boss Graeme Souness, Ne...</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Graeme_S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text  \\\n",
       "0              tech  tv future in the hands of viewers with home th...   \n",
       "1          business  worldcom boss  left books alone  former worldc...   \n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3             sport  yeading face newcastle in fa cup premiership s...   \n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "...             ...                                                ...   \n",
       "2220       business  cars pull down us retail figures us retail sal...   \n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...   \n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...   \n",
       "2223       politics  how political squabbles snowball it s become c...   \n",
       "2224          sport  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "                                             coref_text  \\\n",
       "0     tv future in the hands of viewers with home th...   \n",
       "1     worldcom boss  left books alone  former worldc...   \n",
       "2     tigers wary of farrell  gamble  leicester say ...   \n",
       "3     yeading face newcastle in fa cup premiership s...   \n",
       "4     ocean s twelve raids box office ocean s twelve...   \n",
       "...                                                 ...   \n",
       "2220  cars pull down us retail figures us retail sal...   \n",
       "2221  kilroy unveils immigration policy ex-chatshow ...   \n",
       "2222  rem announce new glasgow concert us band rem h...   \n",
       "2223  how political squabbles snowball it s become c...   \n",
       "2224  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0     tv future hand viewer home theatre system plas...   \n",
       "1     worldcom boss leave book worldcom boss bernie ...   \n",
       "2     tiger wary farrell gamble leicester tiger wary...   \n",
       "3     yeade face newcastle fa cup premiership newcas...   \n",
       "4     ocean s raid box office ocean s crime caper se...   \n",
       "...                                                 ...   \n",
       "2220  car pull retail figure retail sale fall 0.3 ja...   \n",
       "2221  kilroy unveil immigration policy ex chatshow h...   \n",
       "2222  rem announce new glasgow concert band rem anno...   \n",
       "2223  political squabble snowball s commonplace argu...   \n",
       "2224  souness delight euro progress boss graeme soun...   \n",
       "\n",
       "                                              entidades  new_target  \\\n",
       "0     {Adam Hume, Older, Windows, one, Europe, Tim H...           3   \n",
       "1     {Monday, about $180bn, Reid Weingarten, last t...           1   \n",
       "2     {England, BBC Radio Leicester, five weeks ago,...           5   \n",
       "3     {Championship, Sheffield United, Sheff Utd, ea...           2   \n",
       "4     {$184m, Julia Roberts, Blade:, 2001, Twelve, o...           0   \n",
       "...                                                 ...         ...   \n",
       "2220  {January, Popular Securities, just 0.3%, Chris...           1   \n",
       "2221  {Robert Kilroy-Silk, Â£2bn, Veritas, 14,000, t...           6   \n",
       "2222  {Tuesday, Europe, Glasgow, Balloch Castle Coun...           4   \n",
       "2223  {Blair, Google, two, Blair and Brown, America,...           6   \n",
       "2224  {Laurent Robert, Euro, Boss Graeme Souness, Ne...           2   \n",
       "\n",
       "                                      entidades_dbpedia  \n",
       "0     [{'URI': 'http://dbpedia.org/resource/Televisi...  \n",
       "1     [{'URI': 'http://dbpedia.org/resource/MCI_Inc....  \n",
       "2     [{'URI': 'http://dbpedia.org/resource/Colin_Fa...  \n",
       "3     [{'URI': 'http://dbpedia.org/resource/Yeading_...  \n",
       "4     [{'URI': 'http://dbpedia.org/resource/Crime_fi...  \n",
       "...                                                 ...  \n",
       "2220  [{'URI': 'http://dbpedia.org/resource/Car', 's...  \n",
       "2221  [{'URI': 'http://dbpedia.org/resource/Robert_K...  \n",
       "2222  [{'URI': 'http://dbpedia.org/resource/Glasgow'...  \n",
       "2223  [{'URI': 'http://dbpedia.org/resource/Snowball...  \n",
       "2224  [{'URI': 'http://dbpedia.org/resource/Graeme_S...  \n",
       "\n",
       "[2225 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b49b2-da1a-42b5-8acd-8b7a41fe21af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cont = 0\n",
    "def simplificar_entidades(entidad):\n",
    "    diccionario = dict()\n",
    "    #diccionario['URI'] = entidad.get('URI')\n",
    "    #diccionario['surfaceForm'] = entidad.get('surfaceForm'),  \n",
    "    #diccionario['tipos']: entidad.get('types') if entidad.keys() else None \n",
    "    diccionario[entidad.get('surfaceForm')] = dict({'URI':entidad.get('URI'), 'tipos':entidad.get('types') if entidad.keys() else None })\n",
    "    return diccionario\n",
    "\n",
    "def devolver_entidades_limpias_df (row):\n",
    "    entidades_dbpedia = pd.Series(row['entidades_dbpedia'])\n",
    "   # global cont\n",
    "   # cont = cont + 1 \n",
    "   # print(cont)\n",
    "    dicts = entidades_dbpedia.apply(simplificar_entidades)\n",
    "    print(len(dicts))\n",
    "    import collections\n",
    "    entidades_simplificadas = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():  # d.items() in Python 3+\n",
    "            entidades_simplificadas[k] = v\n",
    "    return entidades_simplificadas\n",
    "\n",
    "corpus1['entidades_dbpedia_simplificadas'] = corpus1.apply(devolver_entidades_limpias_df,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e94d27-1a63-4dcc-9498-5db8cef21396",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus1.head()['entidades_dbpedia_simplificadas'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dac40b-bc2e-4e86-87f6-172576b35bc6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Consolidamos entidades vs entidades_dbpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02e16f-0a0b-4e3c-89a4-49bf2cf2ec8d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#cargamos dbpedia y sumo\n",
    "from spotlight import *\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, CSV\n",
    "\n",
    "class TextAnalyzer(object):\n",
    "    def __init__(self,nlp):\n",
    "        self.nlp = nlp \n",
    "        \n",
    "    # allow the class instance to be called just like\n",
    "    # just like a function and applies the preprocessing and\n",
    "    # tokenize the document\n",
    "    @staticmethod      \n",
    "    def remove_special_lines(texto):\n",
    "        texto = re.sub(\"^upright=.*[\\r|\\n]\", '', texto)\n",
    "        texto = re.sub(\"^upright = .*[\\r|\\n]\", '', texto)\n",
    "        texto = re.sub(\"Category:.*[\\r|\\n]\",'',texto)\n",
    "        texto = re.sub(\"Cat\\D*:.*[\\r|\\n]\",'',texto)\n",
    "        texto = re.sub(\"[[][\\d]+[]]\",'',texto)\n",
    "        texto = re.sub(\"thumb\",'',texto)\n",
    "        texto = re.sub(\"[|]\",'',texto)\n",
    "        texto = re.sub(\"\\d+px\",'',texto)\n",
    "        return (texto)\n",
    "    @staticmethod\n",
    "    def strip_formatting(string):\n",
    "        string = string.lower()\n",
    "        string = re.sub(r\"([.!?,;-_'/|()]=-<>+*`)\", r\"\", string)\n",
    "        string = re.sub(r'https?:\\/\\/.*?[\\s]', '', string) \n",
    "        return string\n",
    "\n",
    "    def get_nlp(self):\n",
    "        return self.nlp\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        tokens = nlp(doc)\n",
    "        lemmatized_tokens = [(token.lemma_.lower()) for token in tokens\n",
    "                                                   if not (token.is_stop or token.is_punct)]\n",
    "            \n",
    "        return(lemmatized_tokens)\n",
    "    \n",
    "    def is_present (self,word,text):\n",
    "        lemmatized_tokens =  lambda text: \" \".join(token.lemma_.lower() for token in nlp(text) if not (token.is_stop or token.is_punct))\n",
    "        normalizado = lemmatized_tokens(text)    \n",
    "        return (word in (normalizado))\n",
    "\n",
    "class SemanticAnalyzer(TextAnalyzer):\n",
    "    def __init__(self,nlp,endpoint=\"http://172.17.0.1:2222/rest/annotate\",soporte=1000,confianza=0.5,umbral=0.1):\n",
    "        super().__init__(nlp)\n",
    "        self.endpoint = endpoint\n",
    "        self.soporte=soporte\n",
    "        self.confianza = confianza\n",
    "        self.alfa = umbral\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        try:\n",
    "            annotations = spotlight.annotate(self.endpoint,\n",
    "                                     doc,\n",
    "                                      confidence=self.confianza, support=self.soporte, spotter='Default')\n",
    "            diccionario =  dict()\n",
    "            for annotation in annotations:\n",
    "                lista = list(annotation.items())\n",
    "                print(lista)\n",
    "                URI = lista[0]\n",
    "                key = lista[3]\n",
    "                score = lista[5]\n",
    "               # if (score[1]>self.alfa):\n",
    "                diccionario[key[1]]=URI[1]\n",
    "        \n",
    "            return(diccionario)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "import spacy\n",
    "import time \n",
    "from owlready2 import *\n",
    "import json \n",
    "from spotlight import *\n",
    "\n",
    "class OntoManager(object):\n",
    "    def __init__(self,nlp,dict_onto,dict_graph):\n",
    "        \n",
    "        self.dict_onto = dict_onto\n",
    "        self.dict_graph = dict_graph\n",
    "        \n",
    "        self.prefijos = \"\"\"  PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>\n",
    "                                PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "                                PREFIX dbr:    <http://dbpedia.org/resource/>\n",
    "                                PREFIX dbo:    <http://dbpedia.org/ontology/>\n",
    "                                PREFIX dct:    <http://purl.org/dc/terms/>\n",
    "                                PREFIX owl:    <http://www.w3.org/2002/07/owl#>\n",
    "                                PREFIX rdf:    <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "                                PREFIX rdfs:   <http://www.w3.org/2000/01/rdf-schema#>\n",
    "                                PREFIX schema: <http://schema.org/>\n",
    "                                PREFIX skos:   <http://www.w3.org/2004/02/skos/core#>\n",
    "                                PREFIX xsd:    <http://www.w3.org/2001/XMLSchema#>\n",
    "                                PREFIX SUMO: <http://www.adampease.org/OP/SUMO.owl#>\n",
    "                            \"\"\"\n",
    "        self.sa = SemanticAnalyzer(nlp)\n",
    "        \n",
    "    def getSemanticsOfTerm(self,term):   \n",
    "        isDbo = self._isDbo(term)\n",
    "        if (isDbo is None):\n",
    "            return None\n",
    "        \n",
    "        concept = self._getBaseConcept(term,isDbo)\n",
    "        if (isDbo):\n",
    "            resources = self.sa(term)\n",
    "        else:\n",
    "            resources = {}\n",
    "        superclasses = self._getHierarchy(str(concept),isDbo,False)\n",
    "        subclasses = self._getHierarchy(str(concept),isDbo,True)\n",
    "        relationships = self._getRelationships(str(concept),isDbo)\n",
    "        types = self._getDBPediaTypes(str(concept))\n",
    "        \n",
    "        termino = dict({'concepto':concept, 'tipos':types,'resources':resources,'padres':superclasses,'hijos':subclasses,'relaciones':relationships})\n",
    "        \n",
    "        return termino\n",
    "\n",
    "    from SPARQLWrapper import SPARQLWrapper, JSON, CSV\n",
    "\n",
    "\n",
    "    def ejecutar_consulta_dbpedia(self,query,tipo=JSON):\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        sparql.setReturnFormat(tipo)\n",
    "\n",
    "        sparql.setQuery(self.prefijos+query)  # the previous query as a literal string\n",
    "        print(query)\n",
    "        return sparql.query().convert()\n",
    "\n",
    "    #def getTypesOfURI (self, uri):\n",
    "        \n",
    "    def _isDbo (self,term):\n",
    "        res = self.dict_onto.get('dbo').search(label=term,_case_sensitive=False)\n",
    "        if (len(res)>0):\n",
    "            return True\n",
    "        res = self.dict_onto.get('SUMO').search(label=term,_case_sensitive=False)\n",
    "        if (len(res)>0):\n",
    "            return False\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _getBaseConcept(self,term,isDbo):\n",
    "        if (isDbo):\n",
    "            res = self.dict_onto.get('dbo').search(label=term,_case_sensitive=False)\n",
    "            if (len(res)>0):\n",
    "                return str(res[0]).replace('.',':')\n",
    "        \n",
    "        res = self.dict_onto.get('SUMO').search(label=term,_case_sensitive=False)\n",
    "        if (len(res)>0):\n",
    "            return str(res[0]).replace('.',':')\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _getDBPediaTypes(self, term):\n",
    "        consulta = self.prefijos + \"\"\"\n",
    "       select ?o where {\"\"\"+term.replace('dbo','dbr')+\"\"\" rdf:type ?o}\n",
    "\n",
    "        \"\"\"\n",
    "        print(consulta)\n",
    "        lista = list(self.dict_graph.get('dbo').query(consulta))\n",
    "        if len(lista)==0:\n",
    "            res = self.ejecutar_consulta_dbpedia(consulta, CSV)\n",
    "            lista = str(res).replace(\"\"\"\\\"\"\"\",'').split('\\\\n')[1:]\n",
    "        return lista\n",
    "        \n",
    "    def _getHierarchy(self,concept,isDbo,isSuperclass):\n",
    "        \n",
    "        consulta = self.prefijos + \"\"\"SELECT ?x\n",
    "            WHERE {\n",
    "                ?x a owl:Class .\n",
    "                ?x rdfs:subClassOf \"\"\"+concept+\"\"\"\n",
    "                }\"\"\"\n",
    "         \n",
    "        if isSuperclass == False:\n",
    "                 consulta = self.prefijos + \"\"\"SELECT ?x\n",
    "            WHERE {\n",
    "                ?x a owl:Class .\n",
    "                \"\"\"+concept+\"\"\" rdfs:subClassOf ?x \n",
    "                }\"\"\"\n",
    "        if (isDbo==False):\n",
    "            return (list(self.dict_graph.get('SUMO').query(consulta)))  \n",
    "        else:\n",
    "            return (list(self.dict_graph.get('dbo').query(consulta)))\n",
    "    \n",
    "    def _getRelationships(self,concept,isDbo):\n",
    "        consulta = self.prefijos + \"\"\"select distinct * where {\n",
    "                  \"\"\"+concept+\"\"\" ?property ?value .\n",
    "                  filter ( ?property not in ( rdf:type ) )\n",
    "                   filter ( ?property not in ( rdfs:label ) )\n",
    "                optional {?property rdfs:comment ?comment}\n",
    "                  optional {?property rdfs:label ?label}\n",
    "                  optional {?property rdfs:range ?range} \n",
    "                  optional {?property rdfs:domain ?domain} \n",
    "                }\n",
    "        \"\"\"\n",
    "        \n",
    "        if (isDbo==False):\n",
    "            resultado =  (list(self.dict_graph.get('SUMO').query(consulta)))  \n",
    "        else:\n",
    "            resultado = (list(self.dict_graph.get('dbo').query(consulta)))\n",
    "        \n",
    "        res = pd.DataFrame(data = resultado, columns = ['term','property','comment','label','range','domain'])\n",
    "        res.set_index(res.term)\n",
    "        return (res.to_json())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b060ea4d-b21d-45f4-b373-55d22dc253a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1º buscar tipos vacíos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6d52a-6126-44eb-bfd2-d704b6e189bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "myworld1 = World()\n",
    "sumo =myworld1.get_ontology(\"file:///home/raul/doctorado/ontologias/SUMO.owl\").load()\n",
    "graphsumo = myworld1.as_rdflib_graph()\n",
    "myworld2 = World()\n",
    "dbpedia = myworld2.get_ontology(\"file:///home/raul/doctorado/ontologias/dbpedia_3.9.owl/\").load()\n",
    "graphdbo = myworld2.as_rdflib_graph()\n",
    "#dbpedia.base_iri = \"http://dbpedia.org/ontology/\"\n",
    "dbpedia.name='dbo'\n",
    "dict_onto = dict([('dbo',dbpedia),('SUMO',sumo)])\n",
    "dict_graph = dict([('dbo',graphdbo),('SUMO',graphsumo)])\n",
    "alfred = OntoManager(nlp,dict_onto,dict_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3d375-4a98-41d5-ba57-3806dd5404cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc27817-dd95-4c9c-a1f1-6b3d14c13405",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#corpus1 = joblib.load('./bbc_objects/bbc_processed_final_semantic')\n",
    "cont = 0\n",
    "\n",
    "def devolver_nuevos_tipos (row):\n",
    "    \n",
    "    cluster_dict = defaultdict(list)\n",
    "    for termino, propiedades in row['entidades_dbpedia_simplificadas'].items():\n",
    "        \n",
    "        new_props = propiedades\n",
    "        if new_props['tipos']==[] or len(new_props['tipos'])==0:\n",
    "            dbr = '<'+new_props['URI']+'>'#.replace(\"http://dbpedia.org/resource/\",\"dbr:\")\n",
    "            #dbr = dbr[0:len(dbr)-1] if dbr.endswith('.') else dbr\n",
    "            #dbr = dbr.replace(\"'\",\"\"\"\\\\'\"\"\") if dbr.find(\"'\")>-1 else dbr\n",
    "            new_props['tipos']=alfred._getDBPediaTypes(dbr)\n",
    "        cluster_dict[termino].append(new_props)\n",
    "        \n",
    "    return cluster_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92231c88-ec15-40e2-bc49-b89fc7bb4f44",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb352c-ac8e-4af9-acbd-caeded897748",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minicorupus = corpus1.iloc[0:3,:]\n",
    "corpus1['entidades_dbpedia_simplificadas'] = corpus1.apply(devolver_nuevos_tipos,axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12f505b3-27ae-40ed-b669-5e46ebd180e3",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "minicorupus.loc[0,'entidades_dbpedia_simplificadas']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a8bda-20d4-4a0d-a5ec-09a8041cd1d0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ejecutamos `!python3 dbpedia_tipos.py > salida_tipos.out&`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7cac97ee-a32c-43bb-a430-205569747f37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "newcorpus = joblib.load('./bbc_objects/bbc_processed_final_semantic_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cacc732-c465-4851-9c85-1f5abc3751c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newcorpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_7029/692195482.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnewcorpus\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'newcorpus' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a032be3-3a94-4cca-9e07-6bda8beb27e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Guardamos diccionario entidadesdbpedia vs topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fa6b59-d4c2-465b-ab2f-cbe5248e3f26",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "newcorpus = joblib.load('../bbc_objects/bbc_processed_final_semantic_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e244ce33-50ed-4cbc-ac5b-987c11c83a2e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>coref_text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>entidades</th>\n",
       "      <th>new_target</th>\n",
       "      <th>entidades_dbpedia</th>\n",
       "      <th>entidades_dbpedia_simplificadas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "      <td>{Adam Hume, TiVo DVR, One, today, Bill Gates, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Televisi...</td>\n",
       "      <td>{'TV': [{'URI': 'http://dbpedia.org/resource/T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss leave book worldcom boss bernie ...</td>\n",
       "      <td>{2002, 11bn, Reid Weingarten, PE, the late 199...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/MCI_Inc....</td>\n",
       "      <td>{'Worldcom': [{'URI': 'http://dbpedia.org/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tiger wary farrell gamble leicester tiger wary...</td>\n",
       "      <td>{Saracens, another three months, Great Britain...</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Colin_Fa...</td>\n",
       "      <td>{'Farrell': [{'URI': 'http://dbpedia.org/resou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeade face newcastle fa cup premiership newcas...</td>\n",
       "      <td>{Milton Keynes Dons, Gillingham, Third, Presto...</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Yeading_...</td>\n",
       "      <td>{'Yeading': [{'URI': 'http://dbpedia.org/resou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s raid box office ocean s crime caper se...</td>\n",
       "      <td>{Ocean, the New York Times, the 1960s, Andy Ga...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'URI': 'http://dbpedia.org/resource/Crime_fi...</td>\n",
       "      <td>{'crime': [{'URI': 'http://dbpedia.org/resourc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                          coref_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0  tv future hand viewer home theatre system plas...   \n",
       "1  worldcom boss leave book worldcom boss bernie ...   \n",
       "2  tiger wary farrell gamble leicester tiger wary...   \n",
       "3  yeade face newcastle fa cup premiership newcas...   \n",
       "4  ocean s raid box office ocean s crime caper se...   \n",
       "\n",
       "                                           entidades  new_target  \\\n",
       "0  {Adam Hume, TiVo DVR, One, today, Bill Gates, ...           3   \n",
       "1  {2002, 11bn, Reid Weingarten, PE, the late 199...           1   \n",
       "2  {Saracens, another three months, Great Britain...           5   \n",
       "3  {Milton Keynes Dons, Gillingham, Third, Presto...           2   \n",
       "4  {Ocean, the New York Times, the 1960s, Andy Ga...           0   \n",
       "\n",
       "                                   entidades_dbpedia  \\\n",
       "0  [{'URI': 'http://dbpedia.org/resource/Televisi...   \n",
       "1  [{'URI': 'http://dbpedia.org/resource/MCI_Inc....   \n",
       "2  [{'URI': 'http://dbpedia.org/resource/Colin_Fa...   \n",
       "3  [{'URI': 'http://dbpedia.org/resource/Yeading_...   \n",
       "4  [{'URI': 'http://dbpedia.org/resource/Crime_fi...   \n",
       "\n",
       "                     entidades_dbpedia_simplificadas  \n",
       "0  {'TV': [{'URI': 'http://dbpedia.org/resource/T...  \n",
       "1  {'Worldcom': [{'URI': 'http://dbpedia.org/reso...  \n",
       "2  {'Farrell': [{'URI': 'http://dbpedia.org/resou...  \n",
       "3  {'Yeading': [{'URI': 'http://dbpedia.org/resou...  \n",
       "4  {'crime': [{'URI': 'http://dbpedia.org/resourc...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newcorpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55add34-07c9-419d-b7fb-4994bf6c7ade",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n",
      "508\n",
      "289\n",
      "112\n",
      "167\n",
      "439\n",
      "355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../bbc_objects/diccionario_topic_entidades_dbpedia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dbpedia_ent = {}\n",
    "for target in newcorpus.new_target.unique():\n",
    "    entidades = newcorpus.loc[newcorpus.new_target==target,'entidades_dbpedia_simplificadas']\n",
    "    print(len(entidades))\n",
    "    result = {}\n",
    "    L = entidades\n",
    "    for d in L:\n",
    "        result.update(d)\n",
    "    topic_dbpedia_ent [target]=result\n",
    "\n",
    "joblib.dump(topic_dbpedia_ent,'../bbc_objects/diccionario_topic_entidades_dbpedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3ddd6-1691-462c-af0f-6352397f460e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a7281456-0ef5-42e8-a321-36a98114c9c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#entidades=joblib.load('./bbc_objects/entidades_bbc')\n",
    "#topics =joblib.load('./bbc_objects/new_bbc_topics_7')\n",
    "#corpus = joblib.load('./bbc_objects/bbc_processed_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97d5444b-ea55-4d0a-8e51-cd7145393b89",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus3 = joblib.load('./bbc_objects/entidades_bbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e50f3b12-c243-4e6d-8ecc-34e5963c413f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3\n",
       "1       1\n",
       "2       5\n",
       "3       2\n",
       "4       0\n",
       "       ..\n",
       "2220    1\n",
       "2221    6\n",
       "2222    4\n",
       "2223    6\n",
       "2224    2\n",
       "Name: new_target, Length: 2225, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5d6d9ee2-348c-4059-b3b0-5a579eb916e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newcorups' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_43030/2227984649.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnewcorups\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnew_target\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'newcorups' is not defined"
     ]
    }
   ],
   "source": [
    "newcorups.new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5397c5-9100-4d53-be5f-51c4a0df472e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}